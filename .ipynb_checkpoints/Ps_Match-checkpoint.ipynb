{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from Utils import Utils\n",
    "from Constants import Constants\n",
    "from PS_Matching import PS_Matching\n",
    "from Propensity_socre_network import Propensity_socre_network\n",
    "from Utils import Utils\n",
    "\n",
    "from GAN import Generator, Discriminator\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from GAN_Manager import GAN_Manager\n",
    "from Utils import Utils\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from torch.autograd.variable import Variable\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propensity_scores(ps_train_set, iter_id, input_nodes, device):\n",
    "    prop_score_NN_model_path = Constants.PROP_SCORE_NN_MODEL_PATH \\\n",
    "            .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR)\n",
    "\n",
    "    train_parameters_NN = {\n",
    "            \"epochs\": Constants.PROP_SCORE_NN_EPOCHS,\n",
    "            \"lr\": Constants.PROP_SCORE_NN_LR,\n",
    "            \"batch_size\": Constants.PROP_SCORE_NN_BATCH_SIZE,\n",
    "            \"shuffle\": True,\n",
    "            \"train_set\": ps_train_set,\n",
    "            \"model_save_path\": prop_score_NN_model_path,\n",
    "            \"input_nodes\": input_nodes\n",
    "    }\n",
    "\n",
    "        # ps using NN\n",
    "    ps_net_NN = Propensity_socre_network()\n",
    "    print(\"############### Propensity Score neural net Training ###############\")\n",
    "    ps_net_NN.train(train_parameters_NN, device, phase=\"train\")\n",
    "\n",
    "        # eval\n",
    "    eval_parameters_train_NN = {\n",
    "            \"eval_set\": ps_train_set,\n",
    "            \"model_path\": prop_score_NN_model_path,\n",
    "            \"input_nodes\": input_nodes\n",
    "    }\n",
    "\n",
    "    ps_score_list_train_NN = ps_net_NN.eval(eval_parameters_train_NN, device, phase=\"eval\")\n",
    "\n",
    "    return ps_score_list_train_NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from GAN_Manager import GAN_Manager\n",
    "from Utils import Utils\n",
    "\n",
    "def get_matched_and_unmatched_control_indices(ps_treated, ps_control):\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(ps_control)\n",
    "    distance, matched_control = nn.kneighbors(ps_treated)\n",
    "    matched_control_indices = np.array(matched_control).ravel()\n",
    "\n",
    "        # # remove duplicates\n",
    "    #matched_control_indices = list(dict.fromkeys(matched_control_indices))\n",
    "    set_matched_control_indices = set(matched_control_indices)\n",
    "    total_indices = list(range(len(ps_control)))\n",
    "    unmatched_control_indices = list(filter(lambda x: x not in set_matched_control_indices,\n",
    "                                                total_indices))\n",
    "\n",
    "    return matched_control_indices, unmatched_control_indices\n",
    "\n",
    "def filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                              np_control_df_Y_f,\n",
    "                              np_control_df_Y_cf, indices):\n",
    "    np_filter_control_df_X = np.take(np_control_df_X, indices, axis=0)\n",
    "    np_filter_control_ps_score = np.take(np_control_ps_score, indices, axis=0)\n",
    "    np_filter_control_df_Y_f = np.take(np_control_df_Y_f, indices, axis=0)\n",
    "    np_filter_control_df_Y_cf = np.take(np_control_df_Y_cf, indices, axis=0)\n",
    "    tuple_matched_control = (np_filter_control_df_X, np_filter_control_ps_score,\n",
    "                                 np_filter_control_df_Y_f, np_filter_control_df_Y_cf)\n",
    "\n",
    "    return tuple_matched_control\n",
    "\n",
    "def filter_matched_and_unmatched_control_samples(np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f,\n",
    "                                                     np_control_df_Y_cf, matched_control_indices,\n",
    "                                                     unmatched_control_indices):\n",
    "    tuple_matched_control = filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                                                           np_control_df_Y_f,\n",
    "                                                           np_control_df_Y_cf,\n",
    "                                                           matched_control_indices)\n",
    "\n",
    "    tuple_unmatched_control = filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                                                             np_control_df_Y_f,\n",
    "                                                             np_control_df_Y_cf,\n",
    "                                                             unmatched_control_indices)\n",
    "\n",
    "    return tuple_matched_control, tuple_unmatched_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Propensity Score neural net Training ###############\n",
      ".. Training started ..\n",
      "Saved model path: ./Propensity_Model/NN_PS_model_iter_id_1_epoch_50_lr_0.001.pth\n",
      "Epoch: 25, loss: 7.324844643473625, correct: 486/597, accuracy: 0.8140703517587939\n",
      "Epoch: 50, loss: 5.5640725791454315, correct: 521/597, accuracy: 0.8726968174204355\n",
      "Saved model..\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"Dataset/ihdp_sample.csv\"\n",
    "from dataloader import DataLoader\n",
    "split_size = 0.8\n",
    "dL = DataLoader()\n",
    "iter_id = 1\n",
    "input_nodes = 25\n",
    "device = Utils.get_device()\n",
    "\n",
    "np_covariates_X_train, np_covariates_X_test, np_covariates_Y_train, np_covariates_Y_test \\\n",
    "                = dL.preprocess_data_from_csv(csv_path, split_size)\n",
    "\n",
    "ps_train_set = dL.convert_to_tensor(np_covariates_X_train, np_covariates_Y_train)\n",
    "ps_score_list_train_NN = get_propensity_scores(ps_train_set, iter_id, input_nodes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_score_list_train_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Treated Statistics ==>\n",
      "(112, 25)\n",
      " Control Statistics ==>\n",
      "(485, 25)\n"
     ]
    }
   ],
   "source": [
    "data_loader_dict_train = dL.prepare_tensor_for_DCN(np_covariates_X_train,\n",
    "                                                              np_covariates_Y_train,\n",
    "                                                              ps_score_list_train_NN,\n",
    "                                                              False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Control: 112\n",
      "Unmatched Control: 434\n",
      "Matched Control: (112, 25)\n",
      "Matched Treated: (112, 25)\n"
     ]
    }
   ],
   "source": [
    "tuple_treated = data_loader_dict_train[\"treated_data\"]\n",
    "tuple_control = data_loader_dict_train[\"control_data\"]\n",
    "np_treated_df_X, np_treated_ps_score, np_treated_df_Y_f, np_treated_df_Y_cf = tuple_treated\n",
    "np_control_df_X, np_control_ps_score, np_control_df_Y_f, np_control_df_Y_cf = tuple_control\n",
    "\n",
    "        # get unmatched controls\n",
    "matched_control_indices, unmatched_control_indices = get_matched_and_unmatched_control_indices(\n",
    "Utils.convert_to_col_vector(np_treated_ps_score),\n",
    "Utils.convert_to_col_vector(np_control_ps_score))\n",
    "\n",
    "print(\"Matched Control: {0}\".format(len(matched_control_indices)))\n",
    "print(\"Unmatched Control: {0}\".format(len(unmatched_control_indices)))\n",
    "\n",
    "tuple_matched_control, tuple_unmatched_control = filter_matched_and_unmatched_control_samples(\n",
    "            np_control_df_X, np_control_ps_score,\n",
    "            np_control_df_Y_f,\n",
    "            np_control_df_Y_cf, matched_control_indices,\n",
    "            unmatched_control_indices)\n",
    "\n",
    "        # generate matched treated for unmatched controls using variable\n",
    "        # tuple_unmatched_control\n",
    "        # create GAN code here\n",
    "print(\"Matched Control: {0}\".format(tuple_matched_control[0].shape))\n",
    "print(\"Matched Treated: {0}\".format(tuple_treated[0].shape))\n",
    "tensor_treated = \\\n",
    "            Utils.create_tensors_to_train_DCN(tuple_treated, dL)\n",
    "\n",
    "        # need to change for unmatched\n",
    "tensor_matched_control = \\\n",
    "            Utils.create_tensors_to_train_DCN(tuple_matched_control, dL)\n",
    "\n",
    "tensor_unmatched_control = \\\n",
    "            Utils.create_tensors_to_train_DCN(tuple_unmatched_control, dL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(treated_ps_list, control_ps_list, bins1):\n",
    "    pyplot.hist(treated_ps_list, bins1, alpha=0.5, label='treated')\n",
    "    pyplot.hist(control_ps_list, bins1, alpha=0.5, label='control')\n",
    "\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 112\n"
     ]
    }
   ],
   "source": [
    "ps_matched_control_list = tuple_matched_control[1].tolist()\n",
    "ps_un_matched_control_list = tuple_unmatched_control[1].tolist()\n",
    "ps_treated_list = tuple_treated[1].tolist()\n",
    "\n",
    "print(len(ps_matched_control_list), len(ps_treated_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins1 = np.linspace(0, 1, 100)\n",
    "bins2 = np.linspace(0, 0.2, 100)\n",
    "bins3 = np.linspace(0.2, 0.5, 100)\n",
    "bins4 = np.linspace(0.5, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matched control and treated\n",
    "draw(ps_treated_list, ps_matched_control_list, bins1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unmatched control and treated\n",
    "draw(ps_treated_list, ps_un_matched_control_list, bins1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_Module:\n",
    "    def __init__(self, discriminator_in_nodes, generator_out_nodes, device):\n",
    "        self.discriminator = Discriminator(in_nodes=discriminator_in_nodes).to(device)\n",
    "        self.discriminator.apply(self.weights_init)\n",
    "\n",
    "        self.generator = Generator(out_nodes=generator_out_nodes).to(device)\n",
    "        self.generator.apply(self.weights_init)\n",
    "\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def get_generator(self):\n",
    "        return self.generator\n",
    "\n",
    "    def __cal_propensity_loss(self, ps_score_control, prop_score_NN_model_path, gen_treated, device):\n",
    "        # Assign treated\n",
    "        Y = np.ones(gen_treated.size(0))\n",
    "        eval_set = Utils.convert_to_tensor(gen_treated.detach().numpy(), Y)\n",
    "\n",
    "        eval_parameters_ps_net = {\n",
    "            \"eval_set\": eval_set,\n",
    "            \"model_path\": prop_score_NN_model_path,\n",
    "            \"input_nodes\": 25\n",
    "        }\n",
    "        ps_net_NN = Propensity_socre_network()\n",
    "        ps_score_list_treated = ps_net_NN.eval(eval_parameters_ps_net, device,\n",
    "                                               phase=\"eval\",\n",
    "                                               eval_from_GAN=True)\n",
    "\n",
    "        Tensor = torch.cuda.DoubleTensor if torch.cuda.is_available() else torch.DoubleTensor\n",
    "        ps_score_treated = Tensor(ps_score_list_treated)\n",
    "\n",
    "        prop_loss = torch.sum((torch.sub(ps_score_treated, ps_score_control)) ** 2)\n",
    "        return prop_loss\n",
    "\n",
    "    def noise(self, _size):\n",
    "        n = Variable(torch.normal(mean=0, std=1, size=(_size, 25)))\n",
    "        # print(n.size())\n",
    "        if torch.cuda.is_available(): return n.cuda()\n",
    "        return n\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def real_data_target(self, size):\n",
    "        '''\n",
    "        Tensor containing ones, with shape = size\n",
    "        '''\n",
    "        data = Variable(torch.ones(size, 1))\n",
    "        if torch.cuda.is_available(): return data.cuda()\n",
    "        return data\n",
    "\n",
    "    def fake_data_target(self, size):\n",
    "        '''\n",
    "        Tensor containing zeros, with shape = size\n",
    "        '''\n",
    "        data = Variable(torch.zeros(size, 1))\n",
    "        if torch.cuda.is_available(): return data.cuda()\n",
    "        return data\n",
    "\n",
    "    def train_discriminator(self, optimizer, real_data, fake_data):\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1.1 Train on Real Data\n",
    "        prediction_real = self.discriminator(real_data)\n",
    "        real_score = torch.mean(prediction_real).item()\n",
    "        \n",
    "        # Calculate error and backpropagate\n",
    "        error_real = self.loss(prediction_real, self.real_data_target(real_data.size(0)))\n",
    "        error_real.backward()\n",
    "\n",
    "        # 1.2 Train on Fake Data\n",
    "        prediction_fake = self.discriminator(fake_data)\n",
    "        fake_score = torch.mean(prediction_fake).item()\n",
    "        # Calculate error and backpropagate\n",
    "        error_fake = self.loss(prediction_fake, self.fake_data_target(real_data.size(0)))\n",
    "        error_fake.backward()\n",
    "\n",
    "        # 1.3 Update weights with gradients\n",
    "        optimizer.step()\n",
    "        loss_D = error_real + error_fake\n",
    "        # Return error\n",
    "        return loss_D.item(), real_score, fake_score\n",
    "\n",
    "    def train_generator(self, optimizer, fake_data, BETA, ps_score_control,\n",
    "                        prop_score_NN_model_path, device):\n",
    "        # 2. Train Generator\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Sample noise and generate fake data\n",
    "        predicted_D = self.discriminator(fake_data)\n",
    "        # Calculate error and back propagate\n",
    "        error_g = self.loss(predicted_D, self.real_data_target(predicted_D.size(0)))\n",
    "        prop_loss = self.__cal_propensity_loss(ps_score_control, prop_score_NN_model_path,\n",
    "                                               fake_data, device)\n",
    "        error = error_g + (BETA * prop_loss)\n",
    "        error.backward()\n",
    "        # Update weights with gradients\n",
    "        optimizer.step()\n",
    "        # Return error\n",
    "        return error_g.item(), prop_loss.item()\n",
    "\n",
    "    def train_GAN(self, train_parameters, device):\n",
    "        epochs = train_parameters[\"epochs\"]\n",
    "        train_set = train_parameters[\"train_set\"]\n",
    "        lr = train_parameters[\"lr\"]\n",
    "        shuffle = train_parameters[\"shuffle\"]\n",
    "        batch_size = train_parameters[\"batch_size\"]\n",
    "        prop_score_NN_model_path = train_parameters[\"prop_score_NN_model_path\"]\n",
    "        BETA = train_parameters[\"BETA\"]\n",
    "\n",
    "        data_loader_train = torch.utils.data.DataLoader(train_set,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=shuffle,\n",
    "                                                        num_workers=1)\n",
    "\n",
    "        #         generator = Generator(out_nodes=generator_out_nodes).to(device)\n",
    "        #         discriminator = Discriminator(in_nodes=discriminator_in_nodes).to(device)\n",
    "\n",
    "        g_optimizer = optim.Adam(self.generator.parameters(), lr=lr)\n",
    "        d_optimizer = optim.Adam(self.discriminator.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch += 1\n",
    "#             self.generator.train()\n",
    "#             self.discriminator.train()\n",
    "            \n",
    "            total_G_loss = 0\n",
    "            total_D_loss = 0\n",
    "            total_prop_loss = 0\n",
    "            total_d_pred_real = 0\n",
    "            total_d_pred_fake = 0\n",
    "\n",
    "            for batch in data_loader_train:\n",
    "                covariates_X_control, ps_score_control, y_f, y_cf = batch\n",
    "                covariates_X_control = covariates_X_control.to(device)\n",
    "                covariates_X_control_size = covariates_X_control.size(0)\n",
    "                ps_score_control = ps_score_control.squeeze().to(device)\n",
    "\n",
    "                # 1. Train Discriminator\n",
    "                real_data = covariates_X_control\n",
    "                # Generate fake data\n",
    "                fake_data = self.generator(self.noise(covariates_X_control_size)).detach()\n",
    "                # Train D\n",
    "                d_error, d_pred_real, d_pred_fake = self.train_discriminator(d_optimizer,\n",
    "                                                                             real_data, fake_data)\n",
    "                total_D_loss += d_error\n",
    "                total_d_pred_real += d_pred_real\n",
    "                total_d_pred_fake += d_pred_fake\n",
    "\n",
    "                # 2. Train Generator\n",
    "                # Generate fake data\n",
    "                fake_data = self.generator(self.noise(covariates_X_control_size))\n",
    "                # Train G\n",
    "                error_g, prop_loss = self.train_generator(g_optimizer, fake_data, BETA, ps_score_control,\n",
    "                                                          prop_score_NN_model_path, device)\n",
    "                total_G_loss += error_g\n",
    "                total_prop_loss += prop_loss\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(\"Epoch: {0}, D_loss: {1}, D_score_real: {2}, D_score_Fake: {3}, G_loss: {4}, \"\n",
    "                      \"Prop_loss: {5}\"\n",
    "                      .format(epoch,\n",
    "                              total_D_loss, total_d_pred_real, total_d_pred_fake, total_G_loss, total_prop_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000, D_loss: 7.005464971065521, D_score_real: 4.566374897956848, D_score_Fake: 2.350002497434616, G_loss: 8.115492343902588, Prop_loss: 17.87253822464598\n",
      "Epoch: 2000, D_loss: 6.096078157424927, D_score_real: 4.907849550247192, D_score_Fake: 1.830512285232544, G_loss: 9.785092115402222, Prop_loss: 17.20625606435692\n",
      "Epoch: 3000, D_loss: 4.927120804786682, D_score_real: 5.403427720069885, D_score_Fake: 1.7144108712673187, G_loss: 9.950222134590149, Prop_loss: 13.284923444306845\n",
      "Epoch: 4000, D_loss: 4.920042872428894, D_score_real: 5.391724228858948, D_score_Fake: 1.612783968448639, G_loss: 10.675748705863953, Prop_loss: 34.6739152552783\n",
      "Epoch: 5000, D_loss: 4.507252812385559, D_score_real: 5.6173980832099915, D_score_Fake: 1.6250247210264206, G_loss: 10.661932945251465, Prop_loss: 11.065842683489873\n",
      "Epoch: 6000, D_loss: 4.5048282742500305, D_score_real: 5.527669668197632, D_score_Fake: 1.4677288383245468, G_loss: 11.256579756736755, Prop_loss: 16.121738219936702\n",
      "Epoch: 7000, D_loss: 4.570451527833939, D_score_real: 5.537724316120148, D_score_Fake: 1.5130939334630966, G_loss: 10.923591494560242, Prop_loss: 37.45273518556544\n",
      "Epoch: 8000, D_loss: 4.331517547369003, D_score_real: 5.676365256309509, D_score_Fake: 1.4370487481355667, G_loss: 11.704237341880798, Prop_loss: 10.927793963476839\n",
      "Epoch: 9000, D_loss: 4.04997780919075, D_score_real: 5.722727060317993, D_score_Fake: 1.3119520843029022, G_loss: 12.299439072608948, Prop_loss: 19.35571688753607\n",
      "Epoch: 10000, D_loss: 4.118287086486816, D_score_real: 5.7484853863716125, D_score_Fake: 1.2968441545963287, G_loss: 12.555970788002014, Prop_loss: 11.214004958978363\n"
     ]
    }
   ],
   "source": [
    "# GAN Part from here\n",
    "prop_score_NN_model_path = Constants.PROP_SCORE_NN_MODEL_PATH \\\n",
    "            .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR)\n",
    "gan = GAN_Module(discriminator_in_nodes=25, generator_out_nodes=25, device=device)\n",
    "GAN_train_parameters = {\n",
    "            \"epochs\": 10000,\n",
    "            \"lr\": 0.0002,\n",
    "            \"shuffle\": True,\n",
    "            \"train_set\": tensor_unmatched_control,\n",
    "            \"batch_size\": 64,\n",
    "            \"prop_score_NN_model_path\": prop_score_NN_model_path,\n",
    "            \"BETA\": 1\n",
    "}\n",
    "\n",
    "gan.train_GAN(GAN_train_parameters, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(_size):\n",
    "        n = Variable(torch.normal(mean=0, std=1, size=(_size, 25)))\n",
    "        # print(n.size())\n",
    "        if torch.cuda.is_available(): return n.cuda()\n",
    "        return n\n",
    "    \n",
    "def eval_GAN(eval_size, generator, device):\n",
    "    treated_g = generator(noise(eval_size))\n",
    "    return treated_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_size: 434\n",
      "Treated G size\n",
      "torch.Size([434, 25])\n"
     ]
    }
   ],
   "source": [
    "eval_size = len(unmatched_control_indices)\n",
    "gen_net = gan.get_generator()\n",
    "treated_generated = eval_GAN(eval_size, gen_net, device)\n",
    "print(\"eval_size: \" + str(eval_size))\n",
    "print(\"Treated G size\")\n",
    "print(treated_generated.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.ones(treated_generated.size(0))\n",
    "eval_set = Utils.convert_to_tensor(treated_generated.detach().numpy(), Y)\n",
    "eval_parameters_ps_net = {\n",
    "            \"eval_set\": eval_set,\n",
    "            \"model_path\": prop_score_NN_model_path,\n",
    "            \"input_nodes\": 25\n",
    "}\n",
    "ps_net_NN = Propensity_socre_network()\n",
    "ps_score_list_treated = ps_net_NN.eval(eval_parameters_ps_net, device,\n",
    "                                        phase=\"eval\", eval_from_GAN=True)\n",
    "\n",
    "len(ps_score_list_treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAUOElEQVR4nO3df5DU9Z3n8edbIJK4XAIMEiK6QApPQQGtgQWlVIrFU5OoqYOUWCokXsia5NjL7abOnH+Y2oOSikRcqkxuZy8GvfgDzmQTkkpyMQSjEIhglhAFYzAimYUAQjSmCEbws3/MF9JgD9Mz3T0985nno2pqvv35/uj3Z7p59ac//e0vkVJCkpSX0xpdgCSp9gx3ScqQ4S5JGTLcJSlDhrskZah/owsAaGpqSqNGjWp0GZLUqzzzzDOvpJSGlVvXI8J91KhRbN68udFlSFKvEhEvt7fOaRlJypDhLkkZMtwlKUM9Ys5dUt/25ptv0trayuHDhxtdSo80cOBARo4cyYABAyrex3CX1HCtra0MGjSIUaNGERGNLqdHSSlx4MABWltbGT16dMX7OS0jqeEOHz7M0KFDDfYyIoKhQ4d2+l2N4S6pRzDY29eVv43hLkkZcs5dUo+z7PEXanq8z8w6t8NtXn31VR5++GE++clP1uQ+7733XhYsWMC73vWuivd54oknWLp0Kd/5zneqvv9eP3Jf9vgLx38kqateffVVvvSlL72t/ejRo1063r333suhQ4eqLavLen24S1It3H777bz44otMmjSJyZMnM2PGDG688UYuvPBCAL72ta8xZcoUJk2axCc+8YnjoX/bbbfR3NzM+PHjufPOOwFYvnw5u3fvZsaMGcyYMQOAH/zgB0ybNo2LL76YOXPm8Ic//AGA73//+5x33nlMnz6db3zjGzXrj+EuScCSJUt4//vfz5YtW7j77rt5+umnWbx4Mdu2bWP79u2sXLmS9evXs2XLFvr168dDDz0EwOLFi9m8eTNbt27lxz/+MVu3bmXhwoW8733vY+3ataxdu5ZXXnmFRYsW8cMf/pCf/exnNDc3c88993D48GE+/vGP8+1vf5unnnqK3/72tzXrj3PuklTGlClTjp9XvmbNGp555hkmT54MwB//+EfOPPNMAFatWkVLSwtHjhxhz549bNu2jQkTJpxwrI0bN7Jt2zYuvfRSAP70pz8xbdo0nn/+eUaPHs3YsWMBuOmmm2hpaalJ/Ya7JJVxxhlnHF9OKTFv3jzuuuuuE7Z56aWXWLp0KZs2bWLw4MHMnz+/7PnoKSVmzZrFI488ckL7li1b6nYKqNMykgQMGjSI119/vey6mTNn8thjj7Fv3z4ADh48yMsvv8zvf/97zjjjDN797nezd+9evve975U93tSpU1m/fj07duwA4NChQ7zwwgucd955vPTSS7z44osAbwv/ajhyl9TjVHLqYq0NHTqUSy+9lAsuuIB3vvOdDB8+/Pi6cePGsWjRIq688kreeustBgwYwH333cfUqVO56KKLGD9+PGPGjDk+7QKwYMECrr76akaMGMHatWtZsWIFc+fO5Y033gBg0aJFnHvuubS0tPCBD3yApqYmpk+fzrPPPluT/kRKqSYHqkZzc3Pq6n/WUXoKZCOeEJKqt337ds4///xGl9GjlfsbRcQzKaXmcts7LSNJGTLcJSlDhrskZchwl6QMGe6SlCHDXZIy5HnuknqetXd1vE1nzPhcbY/Xjp07d/KTn/yEG2+8sdP7ffCDH6zZOe7gyF2Sambnzp08/PDDZdcdOXKkW2sx3CWp8OCDDzJhwgQmTpzIzTffzMsvv8zMmTOZMGECM2fOZNeuXQDMnz+fhQsXcskllzBmzBgee+wxoO2ywU899RSTJk1i2bJlrFixgjlz5vChD32IK6+8kpQSn/3sZ7ngggu48MILWblyZd364rSMJAHPPfccixcvZv369TQ1NXHw4EHmzZvHLbfcwrx587j//vtZuHAh3/zmNwHYs2cP69at4/nnn+faa69l9uzZLFmy5IT/SWnFihVs2LCBrVu3MmTIEL7+9a+zZcsWfv7zn/PKK68wefJkLrvssrr0x5G7JAE/+tGPmD17Nk1NTQAMGTKEDRs2HJ8/v/nmm1m3bt3x7a+//npOO+00xo0bx969e9s97qxZsxgyZAgA69atY+7cufTr14/hw4dz+eWXs2nTprr0x3CXJNouy9vR5XdL159++ukn7Nueky8d3F0Md0mi7bK+q1at4sCBA0DbZX0vueQSHn30UQAeeughpk+ffspjnOqywQCXXXYZK1eu5OjRo+zfv58nn3ySKVOm1K4TJZxzl9TzdNOpi6XGjx/PHXfcweWXX06/fv246KKLWL58OR/72Me4++67GTZsGF/96ldPeYwJEybQv39/Jk6cyPz58xk8ePAJ6z/84Q+zYcMGJk6cSETwhS98gfe+973s3Lmz5v3xkr+SGs5L/nbMS/5Kkgx3ScpRh+EeEWdHxNqI2B4Rz0XE3xbtQyLi8Yj4VfF7cNEeEbE8InZExNaIuLjenZDU+/WEKeKeqit/m0pG7keAv0spnQ9MBT4VEeOA24E1KaWxwJriNsDVwNjiZwHw5U5XJalPGThwIAcOHDDgy0gpceDAAQYOHNip/To8WyaltAfYUyy/HhHbgbOA64Aris0eAJ4A/kfR/mBqe5Q2RsR7ImJEcRxJepuRI0fS2trK/v37G11KjzRw4EBGjhzZqX06dSpkRIwCLgJ+Cgw/FtgppT0RcWax2VnAb0p2ay3aTgj3iFhA28iec845p1NFS8rLgAEDGD16dKPLyErFH6hGxF8AXwf+W0rp96fatEzb295rpZRaUkrNKaXmYcOGVVqGJKkCFYV7RAygLdgfSil9o2jeGxEjivUjgH1FeytwdsnuI4HdtSlXklSJSs6WCeArwPaU0j0lq1YD84rlecC3StpvKc6amQq85ny7JHWvSubcLwVuBn4REVuKtv8JLAFWRcStwC5gTrHuu8A1wA7gEPDRmlYsSepQJWfLrKP8PDrAzDLbJ+BTVdYlSaqC31CVpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDhrskZchwl6QMGe6SlCHDXZIyZLhLUoYMd0nKkOEuSRky3CUpQ4a7JGXIcJekDBnukpQhw12SMmS4S1KGDHdJypDhLkkZMtwlKUOGuyRlyHCXpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDhrskZchwl6QMGe6SlCHDXZIyZLhLUoY6DPeIuD8i9kXEsyVtn4+If4uILcXPNSXrPhcROyLilxHxn+pVuCSpfZWM3FcAV5VpX5ZSmlT8fBcgIsYBNwDji32+FBH9alWsJKkyHYZ7SulJ4GCFx7sOeDSl9EZK6SVgBzClivokSV1QzZz7pyNiazFtM7hoOwv4Tck2rUXb20TEgojYHBGb9+/fX0UZkqSTdTXcvwy8H5gE7AG+WLRHmW1TuQOklFpSSs0ppeZhw4Z1sQxJUjldCveU0t6U0tGU0lvAP/PnqZdW4OySTUcCu6srUZLUWV0K94gYUXLzw8CxM2lWAzdExOkRMRoYCzxdXYmSpM7q39EGEfEIcAXQFBGtwJ3AFRExibYpl53AJwBSSs9FxCpgG3AE+FRK6Wh9SpcktafDcE8pzS3T/JVTbL8YWFxNUZKk6vgNVUnKkOEuSRky3CUpQ4a7JGXIcJekDBnukpQhw12SMmS4S1KGDHdJypDhLkkZMtwlKUOGuyRlyHCXpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDhrskZchwl6QMGe6SlCHDXZIyZLhLUoYMd0nKkOEuSRnq3+gCqjV1V0vJraUNq0OSehJH7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDhrskZchwl6QMdRjuEXF/ROyLiGdL2oZExOMR8avi9+CiPSJieUTsiIitEXFxPYuXJJVXych9BXDVSW23A2tSSmOBNcVtgKuBscXPAuDLtSlTktQZHYZ7SulJ4OBJzdcBDxTLDwDXl7Q/mNpsBN4TESNqVawkqTJdnXMfnlLaA1D8PrNoPwv4Tcl2rUWbJKkb1foD1SjTlspuGLEgIjZHxOb9+/fXuAxJ6tu6Gu57j023FL/3Fe2twNkl240Edpc7QEqpJaXUnFJqHjZsWBfLkCSV09VwXw3MK5bnAd8qab+lOGtmKvDasekbSVL36fB67hHxCHAF0BQRrcCdwBJgVUTcCuwC5hSbfxe4BtgBHAI+WoeaJUkd6DDcU0pz21k1s8y2CfhUtUVJkqrjN1QlKUOGuyRlyHCXpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDhrskZchwl6QMGe6SlCHDXZIyZLhLUoYMd0nKkOEuSRky3CUpQ4a7JGXIcJekDBnukpQhw12SMmS4S1KGDHdJypDhLkkZMtwlKUOGuyRlyHCXpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDhrskZchwl6QMGe6SlKH+1ewcETuB14GjwJGUUnNEDAFWAqOAncBHUkq/q65MSVJn1GLkPiOlNCml1Fzcvh1Yk1IaC6wpbkuSulE9pmWuAx4olh8Arq/DfUiSTqHacE/ADyLimYhYULQNTyntASh+n1lux4hYEBGbI2Lz/v37qyxDklSqqjl34NKU0u6IOBN4PCKer3THlFIL0ALQ3NycqqxDklSiqpF7Sml38Xsf8C/AFGBvRIwAKH7vq7ZISVLndDncI+KMiBh0bBm4EngWWA3MKzabB3yr2iIlSZ1TzbTMcOBfIuLYcR5OKX0/IjYBqyLiVmAXMKf6MiVJndHlcE8p/RqYWKb9ADCzmqIkSdXxG6qSlCHDXZIyZLhLUoaqPc+9R1n2+Asn3P7MrHMbVIkkNZYjd0nKkOEuSRnKalpm6q6Wk1qWNqQOSWo0R+6SlCHDXZIyZLhLUoYMd0nKkOEuSRky3CUpQ4a7JGUoq/Pc32btXX9envG5xtUhSd3MkbskZSjvkXspR/GS+hBH7pKUIcNdkjJkuEtShgx3ScpQ1h+obvj1gbLt02Z0cyGS1M0cuUtShrIeuVfFUycl9WKO3CUpQ31z5N7ZUbmjeEm9TN8M91KlwS1JmXBaRpIy5Mi93pzSkdQAhntufDGRhOHeWAaxpDrpk+Fe+s3VaWOG1uagJUG97Mh/Pr78mT75F5bUaEZPT9HJUfyyx184vvyZWefWoyJJvVifD/dOj+JLQrgu7wBOcX+1mropfWEoVfGLhNNJUo/X58O9Eu1dgKw9U3e1/PlGSehv+Mrfn7Bd3V4Qjqni3QBUGPYGvdQjGe4N1N7Iv/RFoLS9vRF3Jdrb94QXordZ+ufFSr7s1V7Qd7ZdUtXqFu4RcRXwj0A/4P+klJbU677qobOj9c5uX+3+J4Ty2s69Azh1oHddaR82Hin5TKD0WdbZbwRX8sJw8jpJ9Qn3iOgH3AfMAlqBTRGxOqW0rR73VyvVBnS91SuU23PyNNIxJ7zLqOBvVs1nEyfsW+11+H2noD6kXiP3KcCOlNKvASLiUeA6oEeHez10xwtGRfdxTjffXyePM43yp5JOLdn+hDOETn7m9rRrBPnOQg0WKaXaHzRiNnBVSum/FLdvBv4qpfTpkm0WAAuKm/8R+GUX764JeKWKcnsj+9w32Oe+oZo+/2VKaVi5FfUauUeZthNeRVJKLUDV8wwRsTml1FztcXoT+9w32Oe+oV59rtdVIVuBs0tujwR21+m+JEknqVe4bwLGRsToiHgHcAOwuk73JUk6SV2mZVJKRyLi08D/p+1UyPtTSs/V476owdROL2Sf+wb73DfUpc91+UBVktRY/k9MkpQhw12SMtRrwj0iroqIX0bEjoi4vcz60yNiZbH+pxExqvurrK0K+vzfI2JbRGyNiDUR8ZeNqLOWOupzyXazIyJFRK8/ba6SPkfER4rH+rmIeLi7a6y1Cp7b50TE2oj41+L5fU0j6qyViLg/IvZFxLPtrI+IWF78PbZGxMVV32lKqcf/0Pah7IvAGOAdwM+BcSdt80ngfxfLNwArG113N/R5BvCuYvm2vtDnYrtBwJPARqC50XV3w+M8FvhXYHBx+8xG190NfW4BbiuWxwE7G113lX2+DLgYeLad9dcA36PtO0JTgZ9We5+9ZeR+/HIGKaU/AccuZ1DqOuCBYvkxYGZElPsyVW/RYZ9TSmtTSoeKmxtp+z5Bb1bJ4wzwv4AvAIe7s7g6qaTPHwfuSyn9DiCltK+ba6y1SvqcgP9QLL+bXv49mZTSk8DBU2xyHfBgarMReE9EjKjmPntLuJ8F/KbkdmvRVnablNIR4DWgzhdMr6tK+lzqVtpe+XuzDvscERcBZ6eUvtOdhdVRJY/zucC5EbE+IjYWV1ztzSrp8+eBmyKiFfgu8F+7p7SG6ey/9w71luu5d3g5gwq36U0q7k9E3AQ0A5fXtaL6O2WfI+I0YBkwv7sK6gaVPM79aZuauYK2d2dPRcQFKaVX61xbvVTS57nAipTSFyNiGvB/iz6/Vf/yGqLm+dVbRu6VXM7g+DYR0Z+2t3KnehvU01V0CYeI+GvgDuDalNIb3VRbvXTU50HABcATEbGTtrnJ1b38Q9VKn9vfSim9mVJ6ibaL7I3tpvrqoZI+3wqsAkgpbQAG0naBrVzV/JItvSXcK7mcwWpgXrE8G/hRKj6p6KU67HMxRfFPtAV7b5+HhQ76nFJ6LaXUlFIalVIaRdvnDNemlDY3ptyaqOS5/U3aPjwnIppom6b5dbdWWVuV9HkXMBMgIs6nLdz3d2uV3Ws1cEtx1sxU4LWU0p6qjtjoT5E78WnzNcALtH3KfkfR9g+0/eOGtgf//wE7gKeBMY2uuRv6/ENgL7Cl+Fnd6Jrr3eeTtn2CXn62TIWPcwD30Pb/IfwCuKHRNXdDn8cB62k7k2YLcGWja66yv48Ae4A3aRul3wr8DfA3JY/xfcXf4xe1eF57+QFJylBvmZaRJHWC4S5JGTLcJSlDhrskZchwl6QMGe6SlCHDXZIy9O9PKTcypSKWKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# treated by GAN vs unmactched control\n",
    "draw(ps_score_list_treated, ps_un_matched_control_list, bins1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TARNetPhi(nn.Module):\n",
    "    def __init__(self, input_nodes, shared_nodes=200):\n",
    "        super(TARNetPhi, self).__init__()\n",
    "\n",
    "        # shared layer\n",
    "        self.shared1 = nn.Linear(in_features=input_nodes, out_features=shared_nodes)\n",
    "        nn.init.xavier_uniform_(self.shared1.weight)\n",
    "        nn.init.zeros_(self.shared1.bias)\n",
    "\n",
    "        self.shared2 = nn.Linear(in_features=shared_nodes, out_features=shared_nodes)\n",
    "        nn.init.xavier_uniform_(self.shared2.weight)\n",
    "        nn.init.zeros_(self.shared2.bias)\n",
    "\n",
    "        self.shared3 = nn.Linear(in_features=shared_nodes, out_features=shared_nodes)\n",
    "        nn.init.xavier_uniform_(self.shared3.weight)\n",
    "        nn.init.zeros_(self.shared3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.float().cuda()\n",
    "        else:\n",
    "            x = x.float()\n",
    "        # shared layers\n",
    "        x = F.relu(self.shared1(x))\n",
    "        x = F.relu(self.shared2(x))\n",
    "        x = F.relu(self.shared3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TARNetH_Y1(nn.Module):\n",
    "    def __init__(self, input_nodes=200, outcome_nodes=100):\n",
    "        super(TARNetH_Y1, self).__init__()\n",
    "\n",
    "        # potential outcome1 Y(1)\n",
    "        self.hidden1_Y1 = nn.Linear(in_features=input_nodes, out_features=outcome_nodes)\n",
    "        nn.init.xavier_uniform_(self.hidden1_Y1.weight)\n",
    "        nn.init.zeros_(self.hidden1_Y1.bias)\n",
    "\n",
    "        self.hidden2_Y1 = nn.Linear(in_features=outcome_nodes, out_features=outcome_nodes)\n",
    "        nn.init.xavier_uniform_(self.hidden2_Y1.weight)\n",
    "        nn.init.zeros_(self.hidden2_Y1.bias)\n",
    "\n",
    "        self.out_Y1 = nn.Linear(in_features=outcome_nodes, out_features=1)\n",
    "        nn.init.xavier_uniform_(self.out_Y1.weight)\n",
    "        nn.init.zeros_(self.out_Y1.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.float().cuda()\n",
    "        else:\n",
    "            x = x.float()\n",
    "\n",
    "        # potential outcome1 Y(1)\n",
    "        y1 = F.relu(self.hidden1_Y1(x))\n",
    "        y1 = F.relu(self.hidden2_Y1(y1))\n",
    "        y1 = self.out_Y1(y1)\n",
    "\n",
    "        return y1\n",
    "\n",
    "\n",
    "class TARNetH_Y0(nn.Module):\n",
    "    def __init__(self, input_nodes=200, outcome_nodes=100):\n",
    "        super(TARNetH_Y0, self).__init__()\n",
    "\n",
    "        # potential outcome1 Y(0)\n",
    "        self.hidden1_Y0 = nn.Linear(in_features=input_nodes, out_features=outcome_nodes)\n",
    "        nn.init.xavier_uniform_(self.hidden1_Y0.weight)\n",
    "        nn.init.zeros_(self.hidden1_Y0.bias)\n",
    "\n",
    "        self.hidden2_Y0 = nn.Linear(in_features=outcome_nodes, out_features=outcome_nodes)\n",
    "        nn.init.xavier_uniform_(self.hidden2_Y0.weight)\n",
    "        nn.init.zeros_(self.hidden2_Y0.bias)\n",
    "\n",
    "        self.out_Y0 = nn.Linear(in_features=outcome_nodes, out_features=1)\n",
    "        nn.init.xavier_uniform_(self.out_Y0.weight)\n",
    "        nn.init.zeros_(self.out_Y0.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.float().cuda()\n",
    "        else:\n",
    "            x = x.float()\n",
    "\n",
    "        # potential outcome1 Y(0)\n",
    "        y0 = F.relu(self.hidden1_Y0(x))\n",
    "        y0 = F.relu(self.hidden2_Y0(y0))\n",
    "        y0 = self.out_Y0(y0)\n",
    "\n",
    "        return y0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PS_Matching:\n",
    "    def match_using_prop_score(self, tuple_treated, tuple_control):\n",
    "        matched_controls = []\n",
    "\n",
    "        # do ps match\n",
    "        np_treated_df_X, np_treated_ps_score, np_treated_df_Y_f, np_treated_df_Y_cf = tuple_treated\n",
    "        np_control_df_X, np_control_ps_score, np_control_df_Y_f, np_control_df_Y_cf = tuple_control\n",
    "\n",
    "        # get unmatched controls\n",
    "        matched_control_indices, unmatched_control_indices = self.get_matched_and_unmatched_control_indices(\n",
    "            Utils.convert_to_col_vector(np_treated_ps_score),\n",
    "            Utils.convert_to_col_vector(np_control_ps_score))\n",
    "\n",
    "        tuple_matched_control, tuple_unmatched_control = self.filter_matched_and_unmatched_control_samples(\n",
    "            np_control_df_X, np_control_ps_score,\n",
    "            np_control_df_Y_f,\n",
    "            np_control_df_Y_cf, matched_control_indices,\n",
    "            unmatched_control_indices)\n",
    "\n",
    "        return tuple_matched_control\n",
    "\n",
    "    def filter_matched_and_unmatched_control_samples(self, np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f,\n",
    "                                                     np_control_df_Y_cf, matched_control_indices,\n",
    "                                                     unmatched_control_indices):\n",
    "        tuple_matched_control = self.filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                                                           np_control_df_Y_f,\n",
    "                                                           np_control_df_Y_cf,\n",
    "                                                           matched_control_indices)\n",
    "\n",
    "        tuple_unmatched_control = self.filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                                                             np_control_df_Y_f,\n",
    "                                                             np_control_df_Y_cf,\n",
    "                                                             unmatched_control_indices)\n",
    "\n",
    "        return tuple_matched_control, tuple_unmatched_control\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                              np_control_df_Y_f,\n",
    "                              np_control_df_Y_cf, indices):\n",
    "        np_filter_control_df_X = np.take(np_control_df_X, indices, axis=0)\n",
    "        np_filter_control_ps_score = np.take(np_control_ps_score, indices, axis=0)\n",
    "        np_filter_control_df_Y_f = np.take(np_control_df_Y_f, indices, axis=0)\n",
    "        np_filter_control_df_Y_cf = np.take(np_control_df_Y_cf, indices, axis=0)\n",
    "        tuple_matched_control = (np_filter_control_df_X, np_filter_control_ps_score,\n",
    "                                 np_filter_control_df_Y_f, np_filter_control_df_Y_cf)\n",
    "\n",
    "        return tuple_matched_control\n",
    "\n",
    "    @staticmethod\n",
    "    def get_matched_and_unmatched_control_indices(ps_treated, ps_control):\n",
    "        nn = NearestNeighbors(n_neighbors=1)\n",
    "        nn.fit(ps_control)\n",
    "        distance, matched_control = nn.kneighbors(ps_treated)\n",
    "        matched_control_indices = np.array(matched_control).ravel()\n",
    "\n",
    "        # remove duplicates\n",
    "        # matched_control_indices = list(dict.fromkeys(matched_control_indices))\n",
    "        set_matched_control_indices = set(matched_control_indices)\n",
    "        total_indices = list(range(len(ps_control)))\n",
    "        unmatched_control_indices = list(filter(lambda x: x not in set_matched_control_indices,\n",
    "                                                total_indices))\n",
    "\n",
    "        return matched_control_indices, unmatched_control_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def get_unmatched_prop_list(tensor_unmatched_control):\n",
    "        control_data_loader_train = torch.utils.data.DataLoader(tensor_unmatched_control,\n",
    "                                                                batch_size=1,\n",
    "                                                                shuffle=False,\n",
    "                                                                num_workers=1)\n",
    "        ps_unmatched_control_list = []\n",
    "        for batch in control_data_loader_train:\n",
    "            covariates_X, ps_score, y_f, y_cf = batch\n",
    "            ps_unmatched_control_list.append(ps_score.item())\n",
    "\n",
    "        return ps_unmatched_control_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceNet:\n",
    "    def __init__(self, input_nodes, shared_nodes, outcome_nodes, device):\n",
    "        self.tarnet_phi = TARNetPhi(input_nodes, shared_nodes=shared_nodes).to(device)\n",
    "\n",
    "        self.tarnet_h_y1 = TARNetH_Y1(input_nodes=shared_nodes,\n",
    "                                      outcome_nodes=outcome_nodes).to(device)\n",
    "\n",
    "        self.tarnet_h_y0 = TARNetH_Y0(input_nodes=shared_nodes,\n",
    "                                      outcome_nodes=outcome_nodes).to(device)\n",
    "\n",
    "    def get_tarnet_phi(self):\n",
    "        return self.tarnet_phi\n",
    "\n",
    "    def get_tarnet_h_y1(self):\n",
    "        return self.tarnet_h_y1\n",
    "\n",
    "    def get_tarnet_h_y0_model(self):\n",
    "        return self.tarnet_h_y0\n",
    "\n",
    "    def train_semi_supervised(self, train_parameters, n_total, n_treated, device):\n",
    "        epochs = train_parameters[\"epochs\"]\n",
    "        batch_size = train_parameters[\"batch_size\"]\n",
    "        lr = train_parameters[\"lr\"]\n",
    "        weight_decay = train_parameters[\"lambda\"]\n",
    "        shuffle = train_parameters[\"shuffle\"]\n",
    "        tensor_dataset = train_parameters[\"tensor_dataset\"]\n",
    "        u = n_treated / n_total\n",
    "        weight_t = 1 / (2 * u)\n",
    "        weight_c = 1 / (2 * (1 - u))\n",
    "\n",
    "        treated_data_loader_train = torch.utils.data.DataLoader(tensor_dataset,\n",
    "                                                                batch_size=batch_size,\n",
    "                                                                shuffle=shuffle,\n",
    "                                                                num_workers=1)\n",
    "\n",
    "        optimizer_W = optim.Adam(self.tarnet_phi.parameters(), lr=lr)\n",
    "        optimizer_V1 = optim.Adam(self.tarnet_h_y1.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        optimizer_V2 = optim.Adam(self.tarnet_h_y0.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        lossF = nn.MSELoss()\n",
    "        print(\".. Training started ..\")\n",
    "        print(device)\n",
    "        for epoch in range(epochs):\n",
    "            epoch += 1\n",
    "            total_loss_T = 0\n",
    "            total_loss_C = 0\n",
    "            for batch in treated_data_loader_train:\n",
    "                covariates_X, ps_score, T, y_f, y_cf = batch\n",
    "                covariates_X = covariates_X.to(device)\n",
    "                ps_score = ps_score.squeeze().to(device)\n",
    "\n",
    "                idx = (T == 1)\n",
    "                covariates_X_treated = covariates_X[idx]\n",
    "                y_f_treated = y_f[idx]\n",
    "                covariates_X_control = covariates_X[~idx]\n",
    "                y_f_control = y_f[~idx]\n",
    "\n",
    "                treated_size = covariates_X_treated.size(0)\n",
    "                control_size = covariates_X_control.size(0)\n",
    "                \n",
    "                optimizer_W.zero_grad()\n",
    "                optimizer_V1.zero_grad()\n",
    "                optimizer_V2.zero_grad()\n",
    "\n",
    "                if treated_size > 0:\n",
    "                    y1_hat = self.tarnet_h_y1(self.tarnet_phi(covariates_X_treated))\n",
    "                    if torch.cuda.is_available():\n",
    "                        loss_T = weight_t * lossF(y1_hat.float().cuda(),\n",
    "                                                  y_f_treated.float().cuda()).to(device)\n",
    "                    else:\n",
    "                        loss_T = weight_t * lossF(y1_hat.float(),\n",
    "                                                  y_f_treated.float()).to(device)\n",
    "                    loss_T.backward()\n",
    "                    total_loss_T += loss_T.item()\n",
    "\n",
    "                if control_size > 0:\n",
    "                    y0_hat = self.tarnet_h_y0(self.tarnet_phi(covariates_X_control))\n",
    "                    if torch.cuda.is_available():\n",
    "                        loss_C = weight_c * lossF(y0_hat.float().cuda(),\n",
    "                                                  y_f_control.float().cuda()).to(device)\n",
    "                    else:\n",
    "                        loss_C = weight_c * lossF(y0_hat.float(),\n",
    "                                                  y_f_control.float()).to(device)\n",
    "                    loss_C.backward()\n",
    "                    total_loss_C += loss_C.item()\n",
    "\n",
    "\n",
    "\n",
    "                optimizer_W.step()\n",
    "\n",
    "                if treated_size > 0:\n",
    "                    optimizer_V1.step()\n",
    "                if control_size > 0:\n",
    "                    optimizer_V2.step()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"epoch: {0}, Treated + Control loss: {1}\".format(epoch, total_loss_T + total_loss_C))\n",
    "\n",
    "    def eval_semi_supervised(self, eval_parameters, device, treated_flag):\n",
    "        eval_set = eval_parameters[\"tensor_dataset\"]\n",
    "\n",
    "        _data_loader = torch.utils.data.DataLoader(eval_set,\n",
    "                                                   shuffle=False, num_workers=1)\n",
    "\n",
    "        y_f_list = []\n",
    "        y_cf_list = []\n",
    "\n",
    "        for batch in _data_loader:\n",
    "            covariates_X, ps_score = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "            ps_score = ps_score.squeeze().to(device)\n",
    "            y1_hat = self.tarnet_h_y1(self.tarnet_phi(covariates_X))\n",
    "            y0_hat = self.tarnet_h_y0(self.tarnet_phi(covariates_X))\n",
    "            if treated_flag:\n",
    "                y_f_list.append(y1_hat.item())\n",
    "                y_cf_list.append(y0_hat.item())\n",
    "            else:\n",
    "                y_f_list.append(y0_hat.item())\n",
    "                y_cf_list.append(y1_hat.item())\n",
    "\n",
    "        return {\n",
    "            \"y_f_list\": np.array(y_f_list),\n",
    "            \"y_cf_list\": np.array(y_cf_list)\n",
    "        }\n",
    "\n",
    "    def train(self, train_parameters, device):\n",
    "        epochs = train_parameters[\"epochs\"]\n",
    "        batch_size = train_parameters[\"batch_size\"]\n",
    "        lr = train_parameters[\"lr\"]\n",
    "        weight_decay = train_parameters[\"lambda\"]\n",
    "        shuffle = train_parameters[\"shuffle\"]\n",
    "        treated_tensor_dataset = train_parameters[\"treated_tensor_dataset\"]\n",
    "        tuple_control = train_parameters[\"tuple_control_train\"]\n",
    "\n",
    "        treated_data_loader_train = torch.utils.data.DataLoader(treated_tensor_dataset,\n",
    "                                                                batch_size=batch_size,\n",
    "                                                                shuffle=shuffle,\n",
    "                                                                num_workers=1)\n",
    "\n",
    "        optimizer_W = optim.Adam(self.tarnet_phi.parameters(), lr=lr)\n",
    "        optimizer_V1 = optim.Adam(self.tarnet_h_y1.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        optimizer_V2 = optim.Adam(self.tarnet_h_y0.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        lossF = nn.MSELoss()\n",
    "        print(\".. Training started ..\")\n",
    "        print(device)\n",
    "        for epoch in range(epochs):\n",
    "            epoch += 1\n",
    "            total_loss_T = 0\n",
    "            total_loss_C = 0\n",
    "            for batch in treated_data_loader_train:\n",
    "                covariates_X_treated, ps_score_treated, y_f_treated, y_cf_treated = batch\n",
    "                covariates_X_treated = covariates_X_treated.to(device)\n",
    "                ps_score_treated = ps_score_treated.squeeze().to(device)\n",
    "\n",
    "                _tuple_treated = self.get_np_tuple_from_tensor(covariates_X_treated, ps_score_treated,\n",
    "                                                               y_f_treated, y_cf_treated)\n",
    "                psm = PS_Matching()\n",
    "                tuple_matched_control = psm.match_using_prop_score(_tuple_treated, tuple_control)\n",
    "\n",
    "                covariates_X_control, ps_score_control, y_f_control, y_cf_control = \\\n",
    "                    self.get_tensor_from_np_tuple(tuple_matched_control)\n",
    "\n",
    "                y1_hat = self.tarnet_h_y1(self.tarnet_phi(covariates_X_treated))\n",
    "                y0_hat = self.tarnet_h_y0(self.tarnet_phi(covariates_X_control))\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    loss_T = lossF(y1_hat.float().cuda(),\n",
    "                                   y_f_treated.float().cuda()).to(device)\n",
    "                    loss_C = lossF(y0_hat.float().cuda(),\n",
    "                                   y_f_control.float().cuda()).to(device)\n",
    "                else:\n",
    "                    loss_T = lossF(y1_hat.float(),\n",
    "                                   y_f_treated.float()).to(device)\n",
    "                    loss_C = lossF(y0_hat.float(),\n",
    "                                   y_f_control.float()).to(device)\n",
    "\n",
    "                optimizer_W.zero_grad()\n",
    "                optimizer_V1.zero_grad()\n",
    "                optimizer_V2.zero_grad()\n",
    "                loss_T.backward()\n",
    "                loss_C.backward()\n",
    "                optimizer_W.step()\n",
    "                optimizer_V1.step()\n",
    "                optimizer_V2.step()\n",
    "                total_loss_T += loss_T.item()\n",
    "                total_loss_C += loss_C.item()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"epoch: {0}, Treated + Control loss: {1}\".format(epoch, total_loss_T + total_loss_C))\n",
    "\n",
    "    def eval(self, eval_parameters, device):\n",
    "        treated_set = eval_parameters[\"treated_set\"]\n",
    "        control_set = eval_parameters[\"control_set\"]\n",
    "        treated_data_loader = torch.utils.data.DataLoader(treated_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "        control_data_loader = torch.utils.data.DataLoader(control_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "\n",
    "        err_treated_list = []\n",
    "        err_control_list = []\n",
    "        true_ITE_list = []\n",
    "        predicted_ITE_list = []\n",
    "\n",
    "        ITE_dict_list = []\n",
    "\n",
    "        for batch in treated_data_loader:\n",
    "            covariates_X, ps_score, y_f, y_cf = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "            y1_hat = self.tarnet_h_y1(self.tarnet_phi(covariates_X))\n",
    "            y0_hat = self.tarnet_h_y0(self.tarnet_phi(covariates_X))\n",
    "\n",
    "            predicted_ITE = y1_hat - y0_hat\n",
    "            true_ITE = y_f - y_cf\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                diff = true_ITE.float().cuda() - predicted_ITE.float().cuda()\n",
    "            else:\n",
    "                diff = true_ITE.float() - predicted_ITE.float()\n",
    "\n",
    "            # ITE_dict_list.append(self.create_ITE_Dict(covariates_X,\n",
    "            #                                           ps_score.item(), y_f.item(),\n",
    "            #                                           y_cf.item(),\n",
    "            #                                           true_ITE.item(),\n",
    "            #                                           predicted_ITE.item(),\n",
    "            #                                           diff.item()))\n",
    "            err_treated_list.append(diff.item())\n",
    "            true_ITE_list.append(true_ITE.item())\n",
    "            predicted_ITE_list.append(predicted_ITE.item())\n",
    "\n",
    "        for batch in control_data_loader:\n",
    "            covariates_X, ps_score, y_f, y_cf = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "\n",
    "            y1_hat = self.tarnet_h_y1(self.tarnet_phi(covariates_X))\n",
    "            y0_hat = self.tarnet_h_y0(self.tarnet_phi(covariates_X))\n",
    "\n",
    "            predicted_ITE = y1_hat - y0_hat\n",
    "            true_ITE = y_cf - y_f\n",
    "            if torch.cuda.is_available():\n",
    "                diff = true_ITE.float().cuda() - predicted_ITE.float().cuda()\n",
    "            else:\n",
    "                diff = true_ITE.float() - predicted_ITE.float()\n",
    "\n",
    "            # ITE_dict_list.append(self.create_ITE_Dict(covariates_X,\n",
    "            #                                           ps_score.item(), y_f.item(),\n",
    "            #                                           y_cf.item(),\n",
    "            #                                           true_ITE.item(),\n",
    "            #                                           predicted_ITE.item(),\n",
    "            #                                           diff.item()))\n",
    "            err_control_list.append(diff.item())\n",
    "            true_ITE_list.append(true_ITE.item())\n",
    "            predicted_ITE_list.append(predicted_ITE.item())\n",
    "\n",
    "        # print(err_treated_list)\n",
    "        # print(err_control_list)\n",
    "        return {\n",
    "            \"treated_err\": err_treated_list,\n",
    "            \"control_err\": err_control_list,\n",
    "            \"true_ITE\": true_ITE_list,\n",
    "            \"predicted_ITE\": predicted_ITE_list,\n",
    "            \"ITE_dict_list\": ITE_dict_list\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_np_tuple_from_tensor(covariates_X, ps_score, y_f, y_cf):\n",
    "        np_covariates_X = covariates_X.numpy()\n",
    "        ps_score = ps_score.numpy()\n",
    "        y_f = y_f.numpy()\n",
    "        y_cf = y_cf.numpy()\n",
    "        _tuple = (np_covariates_X, ps_score, y_f, y_cf)\n",
    "\n",
    "        return _tuple\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tensor_from_np_tuple(_tuple):\n",
    "        np_df_X, np_ps_score, np_df_Y_f, np_df_Y_cf = _tuple\n",
    "        return torch.from_numpy(np_df_X), torch.from_numpy(np_ps_score), \\\n",
    "               torch.from_numpy(np_df_Y_f), torch.from_numpy(np_df_Y_cf),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor_DCN_PS(tensor_x, ps_score):\n",
    "    tensor_ps_score = torch.from_numpy(ps_score)\n",
    "    processed_dataset = torch.utils.data.TensorDataset(tensor_x, tensor_ps_score)\n",
    "    return processed_dataset\n",
    "\n",
    "def convert_to_tensor_DCN_semi_supervised(X, ps_score, T, Y_f, Y_cf):\n",
    "    tensor_x = torch.stack([torch.Tensor(i) for i in X])\n",
    "    tensor_ps_score = torch.from_numpy(ps_score)\n",
    "    tensor_T = torch.from_numpy(T)\n",
    "    tensor_y_f = torch.from_numpy(Y_f)\n",
    "    tensor_y_cf = torch.from_numpy(Y_cf)\n",
    "    processed_dataset = torch.utils.data.TensorDataset(tensor_x, tensor_ps_score, tensor_T,\n",
    "                                                           tensor_y_f, tensor_y_cf)\n",
    "    return processed_dataset    \n",
    "\n",
    "def create_tensors_to_train_DCN_semi_supervised(group):\n",
    "    np_df_X = group[0]\n",
    "    np_ps_score = group[1]\n",
    "    T = group[2]\n",
    "    np_df_Y_f = group[3]\n",
    "    np_df_Y_cf = group[4]\n",
    "    tensor = convert_to_tensor_DCN_semi_supervised(np_df_X, np_ps_score, T,\n",
    "                                             np_df_Y_f, np_df_Y_cf)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi Supervised Unbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DCN semi supervised training Unbalanced ###\n",
      "112\n",
      "485\n",
      "(597, 25)\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 100, Treated + Control loss: 0.1383688165806234\n",
      "epoch: 200, Treated + Control loss: 0.02089365356368944\n",
      "epoch: 300, Treated + Control loss: 0.0036189987149555236\n",
      "epoch: 400, Treated + Control loss: 0.0724185430444777\n"
     ]
    }
   ],
   "source": [
    "print(\"### DCN semi supervised training Unbalanced ###\")\n",
    "np_treated_x, np_treated_ps, np_treated_f, np_treated_cf = data_loader_dict_train[\"treated_data\"]\n",
    "np_control_x, np_control_ps, np_control_f, np_control_cf = data_loader_dict_train[\"control_data\"]\n",
    "\n",
    "t_1 = np.ones(np_treated_x.shape[0])\n",
    "t_0 = np.zeros(np_control_x.shape[0])\n",
    "\n",
    "n_treated = np_treated_x.shape[0]\n",
    "n_control = np_control_x.shape[0]\n",
    "n_total = n_treated + n_control\n",
    "\n",
    "np_train_ss_X = np.concatenate((np_treated_x, np_control_x), axis=0)\n",
    "np_train_ss_ps = np.concatenate((np_treated_ps, np_control_ps), axis=0)\n",
    "np_train_ss_T = np.concatenate((t_1, t_0), axis=0)\n",
    "np_train_ss_f = np.concatenate((np_treated_f, np_control_f), axis=0)\n",
    "np_train_ss_cf = np.concatenate((np_treated_cf, np_control_cf), axis=0)\n",
    "\n",
    "\n",
    "print(np_treated_x.shape[0])\n",
    "print(np_control_x.shape[0])\n",
    "print(np_train_ss_X.shape)\n",
    "\n",
    "train_set = create_tensors_to_train_DCN_semi_supervised((np_train_ss_X, np_train_ss_ps, \n",
    "                                                         np_train_ss_T, np_train_ss_f, \n",
    "                                                         np_train_ss_cf))\n",
    "train_parameters = {\n",
    "                \"epochs\": 400,\n",
    "                \"lr\": 1e-3,\n",
    "                \"lambda\":1e-4,\n",
    "                \"batch_size\": 100,\n",
    "                \"shuffle\": True,\n",
    "                \"tensor_dataset\": train_set\n",
    "    }\n",
    "\n",
    "inference_semi_supervised = InferenceNet(input_nodes=25, shared_nodes=200, \n",
    "                         outcome_nodes=100,\n",
    "                         device=device)\n",
    "\n",
    "inference_semi_supervised.train_semi_supervised(train_parameters, n_total, n_treated, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_score_list_treated_np = np.array(ps_score_list_treated)\n",
    "eval_set = convert_to_tensor_DCN_PS(treated_generated.detach(), ps_score_list_treated_np)\n",
    "\n",
    "DCN_test_parameters = {\n",
    "            \"tensor_dataset\": eval_set\n",
    "}\n",
    "treated_gen_y = inference_semi_supervised.eval_semi_supervised(DCN_test_parameters, device,\n",
    "                                                               treated_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi supervised balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Sizes\n",
      "(112, 25)\n",
      "(112, 25)\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 10, Treated + Control loss: 15.01821744441986\n",
      "epoch: 20, Treated + Control loss: 3.1733776330947876\n",
      "epoch: 30, Treated + Control loss: 2.484353870153427\n",
      "epoch: 40, Treated + Control loss: 2.1613616943359375\n",
      "epoch: 50, Treated + Control loss: 1.2551585957407951\n",
      "epoch: 60, Treated + Control loss: 0.6676626466214657\n",
      "epoch: 70, Treated + Control loss: 0.2919698655605316\n",
      "epoch: 80, Treated + Control loss: 0.15808430314064026\n",
      "epoch: 90, Treated + Control loss: 0.06831961078569293\n",
      "epoch: 100, Treated + Control loss: 0.027002389542758465\n",
      "epoch: 110, Treated + Control loss: 0.01916440762579441\n",
      "epoch: 120, Treated + Control loss: 0.009962025738786906\n",
      "epoch: 130, Treated + Control loss: 0.006586541305296123\n",
      "epoch: 140, Treated + Control loss: 0.0034101191558875144\n",
      "epoch: 150, Treated + Control loss: 0.005120418456499465\n",
      "epoch: 160, Treated + Control loss: 0.001959626628377009\n",
      "epoch: 170, Treated + Control loss: 0.0011676537833409384\n",
      "epoch: 180, Treated + Control loss: 0.0006760719224985223\n",
      "epoch: 190, Treated + Control loss: 0.0005583822894550394\n",
      "epoch: 200, Treated + Control loss: 0.00043607041152426973\n",
      "epoch: 210, Treated + Control loss: 0.00016597564172116108\n",
      "epoch: 220, Treated + Control loss: 0.00010840660252142698\n",
      "epoch: 230, Treated + Control loss: 0.00015115041242097504\n",
      "epoch: 240, Treated + Control loss: 5.6547828990005655e-05\n",
      "epoch: 250, Treated + Control loss: 3.060320705117192e-05\n",
      "epoch: 260, Treated + Control loss: 7.325295428017853e-05\n",
      "epoch: 270, Treated + Control loss: 6.707428872232413e-05\n",
      "epoch: 280, Treated + Control loss: 8.639057159598451e-05\n",
      "epoch: 290, Treated + Control loss: 5.081938854800683e-05\n",
      "epoch: 300, Treated + Control loss: 4.1869905658131756e-05\n",
      "epoch: 310, Treated + Control loss: 3.70205868875928e-05\n",
      "epoch: 320, Treated + Control loss: 3.421545864057407e-05\n",
      "epoch: 330, Treated + Control loss: 3.8734487134206574e-05\n",
      "epoch: 340, Treated + Control loss: 9.516578393231612e-05\n",
      "epoch: 350, Treated + Control loss: 0.0001726934920043277\n",
      "epoch: 360, Treated + Control loss: 8.26207301543036e-05\n",
      "epoch: 370, Treated + Control loss: 0.0002471184111527691\n",
      "epoch: 380, Treated + Control loss: 0.0010604236776998732\n",
      "epoch: 390, Treated + Control loss: 0.009837483550654724\n",
      "epoch: 400, Treated + Control loss: 0.01458994671702385\n",
      "epoch: 410, Treated + Control loss: 0.028167213080450892\n",
      "epoch: 420, Treated + Control loss: 0.022969836834818125\n",
      "epoch: 430, Treated + Control loss: 0.022524136817082763\n",
      "epoch: 440, Treated + Control loss: 0.041840005200356245\n",
      "epoch: 450, Treated + Control loss: 0.01211799104930833\n",
      "epoch: 460, Treated + Control loss: 0.022354104556143284\n",
      "epoch: 470, Treated + Control loss: 0.023815542925149202\n",
      "epoch: 480, Treated + Control loss: 0.028435131069272757\n",
      "epoch: 490, Treated + Control loss: 0.011612510541453958\n",
      "epoch: 500, Treated + Control loss: 0.005090342077892274\n",
      "epoch: 510, Treated + Control loss: 0.0015698055067332461\n",
      "epoch: 520, Treated + Control loss: 0.0030160926398821175\n",
      "epoch: 530, Treated + Control loss: 0.003989935416029766\n",
      "epoch: 540, Treated + Control loss: 0.0025044592330232263\n",
      "epoch: 550, Treated + Control loss: 0.0018435009551467374\n",
      "epoch: 560, Treated + Control loss: 0.0008962414940469898\n",
      "epoch: 570, Treated + Control loss: 0.0006168220788822509\n",
      "epoch: 580, Treated + Control loss: 0.0008014236991584767\n",
      "epoch: 590, Treated + Control loss: 0.0005340339230315294\n",
      "epoch: 600, Treated + Control loss: 0.001120180488214828\n",
      "epoch: 610, Treated + Control loss: 0.001033694985380862\n",
      "epoch: 620, Treated + Control loss: 0.001215680553286802\n",
      "epoch: 630, Treated + Control loss: 0.011723910007276572\n",
      "epoch: 640, Treated + Control loss: 0.004501037212321535\n",
      "epoch: 650, Treated + Control loss: 0.003860220604110509\n",
      "epoch: 660, Treated + Control loss: 0.005017004790715873\n",
      "epoch: 670, Treated + Control loss: 0.07780705421464518\n",
      "epoch: 680, Treated + Control loss: 0.07022426230832934\n",
      "epoch: 690, Treated + Control loss: 0.047041723039001226\n",
      "epoch: 700, Treated + Control loss: 0.026620080694556236\n",
      "epoch: 710, Treated + Control loss: 0.01609951109276153\n",
      "epoch: 720, Treated + Control loss: 0.04405014822259545\n",
      "epoch: 730, Treated + Control loss: 0.018461674568243325\n",
      "epoch: 740, Treated + Control loss: 0.03226913092657924\n",
      "epoch: 750, Treated + Control loss: 0.01283423462882638\n",
      "epoch: 760, Treated + Control loss: 0.014476602198556066\n",
      "epoch: 770, Treated + Control loss: 0.005430997931398451\n",
      "epoch: 780, Treated + Control loss: 0.0028993796440772712\n",
      "epoch: 790, Treated + Control loss: 0.004843186266953126\n",
      "epoch: 800, Treated + Control loss: 0.00252192429616116\n",
      "epoch: 810, Treated + Control loss: 0.005232186056673527\n",
      "epoch: 820, Treated + Control loss: 0.0048381182787125\n",
      "epoch: 830, Treated + Control loss: 0.007929640996735543\n",
      "epoch: 840, Treated + Control loss: 0.020537549862638116\n",
      "epoch: 850, Treated + Control loss: 0.010961776075419039\n",
      "epoch: 860, Treated + Control loss: 0.008462766534648836\n",
      "epoch: 870, Treated + Control loss: 0.019601817417424172\n",
      "epoch: 880, Treated + Control loss: 0.010529340477660298\n",
      "epoch: 890, Treated + Control loss: 0.01713901897892356\n",
      "epoch: 900, Treated + Control loss: 0.01270381931681186\n",
      "epoch: 910, Treated + Control loss: 0.01939802523702383\n",
      "epoch: 920, Treated + Control loss: 0.059303788002580404\n",
      "epoch: 930, Treated + Control loss: 0.024861060082912445\n",
      "epoch: 940, Treated + Control loss: 0.010789180640131235\n",
      "epoch: 950, Treated + Control loss: 0.008336938684806228\n",
      "epoch: 960, Treated + Control loss: 0.008375536446692422\n",
      "epoch: 970, Treated + Control loss: 0.023631855845451355\n",
      "epoch: 980, Treated + Control loss: 0.010722370934672654\n",
      "epoch: 990, Treated + Control loss: 0.002596869016997516\n",
      "epoch: 1000, Treated + Control loss: 0.0026481519889784977\n"
     ]
    }
   ],
   "source": [
    "_np_control_matched_X = tuple_matched_control[0]\n",
    "_np_ps_score_list_control_matched = tuple_matched_control[1]\n",
    "_np_control_matched_f = tuple_matched_control[2]\n",
    "_np_control_matched_cf = tuple_matched_control[3]\n",
    "\n",
    "_tuple_control_train = (_np_control_matched_X, _np_ps_score_list_control_matched, _np_control_matched_f, \n",
    "                           _np_control_matched_cf)\n",
    "\n",
    "_np_original_X = tuple_treated[0]\n",
    "_np_original_ps_score = tuple_treated[1]\n",
    "_np_original_Y_f = tuple_treated[2]\n",
    "_np_original_Y_cf = tuple_treated[3]\n",
    "\n",
    "print(\"Actual Sizes\")\n",
    "print(_np_control_matched_X.shape)\n",
    "print(_np_original_X.shape)\n",
    "\n",
    "_tensor_treated = Utils.convert_to_tensor_DCN(_np_original_X, _np_original_ps_score,\n",
    "                                            _np_original_Y_cf, _np_original_Y_f)\n",
    "\n",
    "train_parameters = {\n",
    "                \"epochs\": 400,\n",
    "                \"lr\": 1e-3,\n",
    "                \"lambda\":1e-4,\n",
    "                \"batch_size\": 100,\n",
    "                \"shuffle\": True,\n",
    "                \"treated_tensor_dataset\": _tensor_treated,\n",
    "                \"tuple_control_train\": _tuple_control_train\n",
    "    }\n",
    "\n",
    "inference_ss_balanced = InferenceNet(input_nodes=25, shared_nodes=200, \n",
    "                         outcome_nodes=100,\n",
    "                         device=device)\n",
    "\n",
    "inference_ss_balanced.train(train_parameters, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_score_list_treated_np = np.array(ps_score_list_treated)\n",
    "eval_set = convert_to_tensor_DCN_PS(treated_generated.detach(), ps_score_list_treated_np)\n",
    "\n",
    "_test_parameters = {\n",
    "            \"tensor_dataset\": eval_set\n",
    "}\n",
    "treated_gen_y = inference_semi_supervised.eval_semi_supervised(_test_parameters, device,\n",
    "                                                               treated_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434, 1)\n",
      "(112, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(546, 25)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_treated_generated = treated_generated.detach().numpy()\n",
    "np_ps_score_list_gen_treated = ps_score_list_treated_np\n",
    "\n",
    "# fliped = True\n",
    "np_treated_gen_f = Utils.convert_to_col_vector(treated_gen_y[\"y_f_list\"])\n",
    "np_treated_gen_cf = Utils.convert_to_col_vector(treated_gen_y[\"y_cf_list\"])\n",
    "\n",
    "print(np_treated_gen_f.shape)\n",
    "\n",
    "np_original_X = tuple_treated[0]\n",
    "np_original_ps_score = tuple_treated[1]\n",
    "np_original_Y_f = tuple_treated[2]\n",
    "np_original_Y_cf = tuple_treated[3]\n",
    "\n",
    "print(np_original_Y_f.shape)\n",
    "\n",
    "np_treated_x = np.concatenate((np_treated_generated, np_original_X), axis=0)\n",
    "np_treated_ps = np.concatenate((np_ps_score_list_gen_treated, np_original_ps_score), axis=0)\n",
    "np_treated_f = np.concatenate((np_treated_gen_f, np_original_Y_f), axis=0)\n",
    "np_treated_cf = np.concatenate((np_treated_gen_cf, np_original_Y_cf), axis=0)\n",
    "\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_x, np_treated_ps,\n",
    "                                            np_treated_f, np_treated_cf)\n",
    "\n",
    "np_treated_x.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434, 1)\n",
      "(112, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(546, 25)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_control_unmatched_X = tuple_unmatched_control[0]\n",
    "np_ps_score_list_control_unmatched = tuple_unmatched_control[1]\n",
    "np_control_unmatched_f = tuple_unmatched_control[2]\n",
    "np_control_unmatched_cf = tuple_unmatched_control[3]\n",
    "\n",
    "np_control_matched_X = tuple_matched_control[0]\n",
    "np_ps_score_list_control_matched = tuple_matched_control[1]\n",
    "np_control_matched_f = tuple_matched_control[2]\n",
    "np_control_matched_cf = tuple_matched_control[3]\n",
    "\n",
    "print(np_control_unmatched_cf.shape)\n",
    "print(np_control_matched_cf.shape)\n",
    "\n",
    "np_control_x = np.concatenate((np_control_unmatched_X, np_control_matched_X), axis=0)\n",
    "np_control_ps = np.concatenate((np_ps_score_list_control_unmatched, np_ps_score_list_control_matched), axis=0)\n",
    "np_control_f = np.concatenate((np_control_unmatched_f, np_control_matched_f), axis=0)\n",
    "np_control_cf = np.concatenate((np_control_unmatched_cf, np_control_matched_cf), axis=0)\n",
    "\n",
    "tensor_control = Utils.convert_to_tensor_DCN(np_control_x, np_control_ps,\n",
    "                                            np_control_f, np_control_cf)\n",
    "\n",
    "\n",
    "np_control_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Sizes\n",
      "(546, 25)\n",
      "(546, 25)\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 100, Treated + Control loss: 0.014005993405589834\n",
      "epoch: 200, Treated + Control loss: 0.003415948733163532\n",
      "epoch: 300, Treated + Control loss: 0.009761141962371767\n",
      "epoch: 400, Treated + Control loss: 0.0016710447234800085\n"
     ]
    }
   ],
   "source": [
    "tuple_control_train = (np_control_x, np_control_ps, np_control_f, np_control_cf)\n",
    "tuple_treated_train = (np_treated_x, np_treated_ps, np_treated_f, np_treated_cf)\n",
    "\n",
    "print(\"Actual Sizes\")\n",
    "print(np_treated_x.shape)\n",
    "print(np_control_x.shape)\n",
    "\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_x, np_treated_ps,\n",
    "                                            np_treated_f, np_treated_cf)\n",
    "\n",
    "train_parameters = {\n",
    "                \"epochs\": 400,\n",
    "                \"lr\": 1e-3,\n",
    "                \"lambda\":1e-4,\n",
    "                \"batch_size\": 100,\n",
    "                \"shuffle\": True,\n",
    "                \"treated_tensor_dataset\": tensor_treated,\n",
    "                \"tuple_control_train\": tuple_control_train\n",
    "    }\n",
    "\n",
    "inference = InferenceNet(input_nodes=25, shared_nodes=200, \n",
    "                         outcome_nodes=100,\n",
    "                         device=device)\n",
    "\n",
    "inference.train(train_parameters, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Testing phase ------------\n",
      " Treated Statistics ==>\n",
      "(27, 25)\n",
      " Control Statistics ==>\n",
      "(123, 25)\n",
      "(27, 25)\n",
      "(123, 25)\n",
      "Testing using Inference\n",
      "2.8993636993889904\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"----------- Testing phase ------------\")\n",
    "dL = DataLoader()\n",
    "ps_test_set = dL.convert_to_tensor(np_covariates_X_test,\n",
    "                                           np_covariates_Y_test)\n",
    "\n",
    "is_synthetic = False\n",
    "input_nodes = 25\n",
    "\n",
    "        # get propensity scores using NN\n",
    "ps_net_NN = Propensity_socre_network()\n",
    "ps_eval_parameters_NN = {\n",
    "            \"eval_set\": ps_test_set,\n",
    "            \"model_path\": Constants.PROP_SCORE_NN_MODEL_PATH\n",
    "                .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR),\n",
    "            \"input_nodes\": input_nodes\n",
    "}\n",
    "ps_score_list_NN = ps_net_NN.eval(ps_eval_parameters_NN, device, phase=\"eval\")\n",
    "\n",
    "data_loader_dict = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                        np_covariates_Y_test,\n",
    "                                                        ps_score_list_NN,\n",
    "                                                        is_synthetic)\n",
    "\n",
    "treated_group = data_loader_dict[\"treated_data\"]\n",
    "np_treated_df_X = treated_group[0]\n",
    "np_treated_ps_score = treated_group[1]\n",
    "np_treated_df_Y_f = treated_group[2]\n",
    "np_treated_df_Y_cf = treated_group[3]\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_df_X, np_treated_ps_score,\n",
    "                                                     np_treated_df_Y_f, np_treated_df_Y_cf)\n",
    "\n",
    "control_group = data_loader_dict[\"control_data\"]\n",
    "np_control_df_X = control_group[0]\n",
    "np_control_ps_score = control_group[1]\n",
    "np_control_df_Y_f = control_group[2]\n",
    "np_control_df_Y_cf = control_group[3]\n",
    "tensor_control = Utils.convert_to_tensor_DCN(np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f, np_control_df_Y_cf)\n",
    "\n",
    "print(np_treated_df_X.shape)\n",
    "print(np_control_df_X.shape)\n",
    "\n",
    "test_parameters = {\n",
    "            \"treated_set\": tensor_treated,\n",
    "            \"control_set\": tensor_control\n",
    "}\n",
    "\n",
    "response_dict = inference.eval(test_parameters, device)\n",
    "err_treated = [ele ** 2 for ele in response_dict[\"treated_err\"]]\n",
    "err_control = [ele ** 2 for ele in response_dict[\"control_err\"]]\n",
    "\n",
    "total_sum = sum(err_treated) + sum(err_control)\n",
    "total_item = len(err_treated) + len(err_control)\n",
    "MSE = total_sum / total_item\n",
    "\n",
    "print(\"Testing using Inference\")\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional TARNET training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TARNET semi supervised training Unbalanced ###\n",
      "112\n",
      "485\n",
      "(597, 25)\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 100, Treated + Control loss: 0.05631272913888097\n",
      "epoch: 200, Treated + Control loss: 0.005538105266168714\n",
      "epoch: 300, Treated + Control loss: 0.003056409077544231\n",
      "epoch: 400, Treated + Control loss: 0.00014060529156267876\n"
     ]
    }
   ],
   "source": [
    "print(\"### TARNET semi supervised training Unbalanced ###\")\n",
    "np_treated_x, np_treated_ps, np_treated_f, np_treated_cf = data_loader_dict_train[\"treated_data\"]\n",
    "np_control_x, np_control_ps, np_control_f, np_control_cf = data_loader_dict_train[\"control_data\"]\n",
    "\n",
    "t_1 = np.ones(np_treated_x.shape[0])\n",
    "t_0 = np.zeros(np_control_x.shape[0])\n",
    "\n",
    "n_treated = np_treated_x.shape[0]\n",
    "n_control = np_control_x.shape[0]\n",
    "n_total = n_treated + n_control\n",
    "\n",
    "np_train_ss_X = np.concatenate((np_treated_x, np_control_x), axis=0)\n",
    "np_train_ss_ps = np.concatenate((np_treated_ps, np_control_ps), axis=0)\n",
    "np_train_ss_T = np.concatenate((t_1, t_0), axis=0)\n",
    "np_train_ss_f = np.concatenate((np_treated_f, np_control_f), axis=0)\n",
    "np_train_ss_cf = np.concatenate((np_treated_cf, np_control_cf), axis=0)\n",
    "\n",
    "\n",
    "print(np_treated_x.shape[0])\n",
    "print(np_control_x.shape[0])\n",
    "print(np_train_ss_X.shape)\n",
    "\n",
    "train_set = create_tensors_to_train_DCN_semi_supervised((np_train_ss_X, np_train_ss_ps, \n",
    "                                                         np_train_ss_T, np_train_ss_f, \n",
    "                                                         np_train_ss_cf))\n",
    "train_parameters = {\n",
    "                \"epochs\": 400,\n",
    "                \"lr\": 1e-3,\n",
    "                \"lambda\":1e-4,\n",
    "                \"batch_size\": 100,\n",
    "                \"shuffle\": True,\n",
    "                \"tensor_dataset\": train_set\n",
    "    }\n",
    "\n",
    "tarnet = InferenceNet(input_nodes=25, shared_nodes=200, \n",
    "                         outcome_nodes=100,\n",
    "                         device=device)\n",
    "\n",
    "tarnet.train_semi_supervised(train_parameters, n_total, n_treated, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Testing phase ------------\n",
      " Treated Statistics ==>\n",
      "(27, 25)\n",
      " Control Statistics ==>\n",
      "(123, 25)\n",
      "(27, 25)\n",
      "(123, 25)\n",
      "Testing using Inference\n",
      "3.657445152818628\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"----------- Testing phase ------------\")\n",
    "dL = DataLoader()\n",
    "ps_test_set = dL.convert_to_tensor(np_covariates_X_test,\n",
    "                                           np_covariates_Y_test)\n",
    "\n",
    "is_synthetic = False\n",
    "input_nodes = 25\n",
    "\n",
    "        # get propensity scores using NN\n",
    "ps_net_NN = Propensity_socre_network()\n",
    "ps_eval_parameters_NN = {\n",
    "            \"eval_set\": ps_test_set,\n",
    "            \"model_path\": Constants.PROP_SCORE_NN_MODEL_PATH\n",
    "                .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR),\n",
    "            \"input_nodes\": input_nodes\n",
    "}\n",
    "ps_score_list_NN = ps_net_NN.eval(ps_eval_parameters_NN, device, phase=\"eval\")\n",
    "\n",
    "data_loader_dict = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                        np_covariates_Y_test,\n",
    "                                                        ps_score_list_NN,\n",
    "                                                        is_synthetic)\n",
    "\n",
    "treated_group = data_loader_dict[\"treated_data\"]\n",
    "np_treated_df_X = treated_group[0]\n",
    "np_treated_ps_score = treated_group[1]\n",
    "np_treated_df_Y_f = treated_group[2]\n",
    "np_treated_df_Y_cf = treated_group[3]\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_df_X, np_treated_ps_score,\n",
    "                                                     np_treated_df_Y_f, np_treated_df_Y_cf)\n",
    "\n",
    "control_group = data_loader_dict[\"control_data\"]\n",
    "np_control_df_X = control_group[0]\n",
    "np_control_ps_score = control_group[1]\n",
    "np_control_df_Y_f = control_group[2]\n",
    "np_control_df_Y_cf = control_group[3]\n",
    "tensor_control = Utils.convert_to_tensor_DCN(np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f, np_control_df_Y_cf)\n",
    "\n",
    "print(np_treated_df_X.shape)\n",
    "print(np_control_df_X.shape)\n",
    "\n",
    "test_parameters = {\n",
    "            \"treated_set\": tensor_treated,\n",
    "            \"control_set\": tensor_control\n",
    "}\n",
    "\n",
    "response_dict = tarnet.eval(test_parameters, device)\n",
    "err_treated = [ele ** 2 for ele in response_dict[\"treated_err\"]]\n",
    "err_control = [ele ** 2 for ele in response_dict[\"control_err\"]]\n",
    "\n",
    "total_sum = sum(err_treated) + sum(err_control)\n",
    "total_item = len(err_treated) + len(err_control)\n",
    "MSE = total_sum / total_item\n",
    "\n",
    "print(\"Testing using Inference\")\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCN Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Constants import Constants\n",
    "\n",
    "\n",
    "class DCN_shared(nn.Module):\n",
    "    def __init__(self, input_nodes):\n",
    "        super(DCN_shared, self).__init__()\n",
    "\n",
    "        # shared layer\n",
    "        self.shared1 = nn.Linear(in_features=input_nodes, out_features=200)\n",
    "        nn.init.xavier_uniform_(self.shared1.weight)\n",
    "\n",
    "        self.shared2 = nn.Linear(in_features=200, out_features=200)\n",
    "        nn.init.xavier_uniform_(self.shared2.weight)\n",
    "\n",
    "        self.dropout_2 = nn.Dropout(p=0.2)\n",
    "        self.dropout_5 = nn.Dropout(p=0.5)\n",
    "        self.training_mode = None\n",
    "\n",
    "    def set_train_mode(self, training_mode):\n",
    "        self.training_mode = training_mode\n",
    "\n",
    "    def forward(self, x, ps_score):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.float().cuda()\n",
    "        else:\n",
    "            x = x.float()\n",
    "\n",
    "        if self.training_mode == Constants.DCN_EVALUATION:\n",
    "            x = self.__eval_net(x)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_PD:\n",
    "            x = self.__train_net_PD(x, ps_score=ps_score)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_CONSTANT_DROPOUT_5:\n",
    "            x = self.__train_net_constant_dropout(x, ps_score=0.5)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_CONSTANT_DROPOUT_2:\n",
    "            x = self.__train_net_constant_dropout(x, ps_score=0.2)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_NO_DROPOUT:\n",
    "            x = self.__train_net_no_droput(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __train_net_constant_dropout(self, x, ps_score):\n",
    "        if ps_score == 0.2:\n",
    "            drop_out = self.dropout_2\n",
    "        elif ps_score == 0.5:\n",
    "            drop_out = self.dropout_5\n",
    "\n",
    "        # shared layers\n",
    "        x = F.relu(drop_out(self.shared1(x)))\n",
    "        x = F.relu(drop_out(self.shared2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __train_net_PD(self, x, ps_score):\n",
    "        entropy = Utils.get_shanon_entropy(ps_score.item())\n",
    "        dropout_prob = Utils.get_dropout_probability(entropy, gama=1)\n",
    "\n",
    "        # shared layers\n",
    "        shared_mask = Utils.get_dropout_mask(dropout_prob, self.shared1(x))\n",
    "        x = F.relu(shared_mask * self.shared1(x))\n",
    "        x = F.relu(shared_mask * self.shared2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __train_net_no_droput(self, x):\n",
    "        # shared layers\n",
    "        x = F.relu(self.shared1(x))\n",
    "        x = F.relu(self.shared2(x))\n",
    "        return x\n",
    "\n",
    "    def __eval_net(self, x):\n",
    "        # shared layers\n",
    "        x = F.relu(self.shared1(x))\n",
    "        x = F.relu(self.shared2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DCN_Y1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCN_Y1, self).__init__()\n",
    "\n",
    "        # potential outcome1 Y(1)\n",
    "        self.hidden1_Y1 = nn.Linear(in_features=200, out_features=200)\n",
    "        nn.init.xavier_uniform_(self.hidden1_Y1.weight)\n",
    "\n",
    "        self.hidden2_Y1 = nn.Linear(in_features=200, out_features=200)\n",
    "        nn.init.xavier_uniform_(self.hidden2_Y1.weight)\n",
    "\n",
    "        self.out_Y1 = nn.Linear(in_features=200, out_features=1)\n",
    "        nn.init.xavier_uniform_(self.out_Y1.weight)\n",
    "\n",
    "        self.dropout_2 = nn.Dropout(p=0.2)\n",
    "        self.dropout_5 = nn.Dropout(p=0.5)\n",
    "        self.training_mode = None\n",
    "\n",
    "    def set_train_mode(self, training_mode):\n",
    "        self.training_mode = training_mode\n",
    "\n",
    "    def forward(self, x, ps_score):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.float().cuda()\n",
    "        else:\n",
    "            x = x.float()\n",
    "\n",
    "        if self.training_mode == Constants.DCN_EVALUATION:\n",
    "            y1 = self.__eval_net(x)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_PD:\n",
    "            y1 = self.__train_net_PD(x, ps_score=ps_score)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_CONSTANT_DROPOUT_5:\n",
    "            y1 = self.__train_net_constant_dropout(x, ps_score=0.5)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_CONSTANT_DROPOUT_2:\n",
    "            y1 = self.__train_net_constant_dropout(x, ps_score=0.2)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_NO_DROPOUT:\n",
    "            y1 = self.__train_net_no_droput(x)\n",
    "\n",
    "        return y1\n",
    "\n",
    "    def __train_net_constant_dropout(self, x, ps_score):\n",
    "        if ps_score == 0.2:\n",
    "            drop_out = self.dropout_2\n",
    "        elif ps_score == 0.5:\n",
    "            drop_out = self.dropout_5\n",
    "\n",
    "        # potential outcome1 Y(1)\n",
    "        y1 = F.relu(drop_out(self.hidden1_Y1(x)))\n",
    "        y1 = F.relu(drop_out(self.hidden2_Y1(y1)))\n",
    "        y1 = self.out_Y1(y1)\n",
    "\n",
    "        return y1\n",
    "\n",
    "    def __train_net_PD(self, x, ps_score):\n",
    "        entropy = Utils.get_shanon_entropy(ps_score.item())\n",
    "        dropout_prob = Utils.get_dropout_probability(entropy, gama=1)\n",
    "\n",
    "        # potential outcome1 Y(1)\n",
    "        y1_mask = Utils.get_dropout_mask(dropout_prob, self.hidden1_Y1(x))\n",
    "        y1 = F.relu(y1_mask * self.hidden1_Y1(x))\n",
    "        y1 = F.relu(y1_mask * self.hidden2_Y1(y1))\n",
    "        y1 = self.out_Y1(y1)\n",
    "\n",
    "        return y1\n",
    "\n",
    "    def __train_net_no_droput(self, x):\n",
    "        # potential outcome1 Y(1)\n",
    "        y1 = F.relu(self.hidden1_Y1(x))\n",
    "        y1 = F.relu(self.hidden2_Y1(y1))\n",
    "        y1 = self.out_Y1(y1)\n",
    "\n",
    "        return y1\n",
    "\n",
    "    def __eval_net(self, x):\n",
    "        # potential outcome1 Y(1)\n",
    "        y1 = F.relu(self.hidden1_Y1(x))\n",
    "        y1 = F.relu(self.hidden2_Y1(y1))\n",
    "        y1 = self.out_Y1(y1)\n",
    "\n",
    "        return y1\n",
    "\n",
    "\n",
    "class DCN_Y0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCN_Y0, self).__init__()\n",
    "\n",
    "        # potential outcome1 Y(0)\n",
    "        self.hidden1_Y0 = nn.Linear(in_features=200, out_features=200)\n",
    "        nn.init.xavier_uniform_(self.hidden1_Y0.weight)\n",
    "\n",
    "        self.hidden2_Y0 = nn.Linear(in_features=200, out_features=200)\n",
    "        nn.init.xavier_uniform_(self.hidden2_Y0.weight)\n",
    "\n",
    "        self.out_Y0 = nn.Linear(in_features=200, out_features=1)\n",
    "        nn.init.xavier_uniform_(self.out_Y0.weight)\n",
    "\n",
    "        self.dropout_2 = nn.Dropout(p=0.2)\n",
    "        self.dropout_5 = nn.Dropout(p=0.5)\n",
    "        self.training_mode = None\n",
    "\n",
    "    def set_train_mode(self, training_mode):\n",
    "        self.training_mode = training_mode\n",
    "        \n",
    "\n",
    "    def forward(self, x, ps_score):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.float().cuda()\n",
    "        else:\n",
    "            x = x.float()\n",
    "\n",
    "        if self.training_mode == Constants.DCN_EVALUATION:\n",
    "            y0 = self.__eval_net(x)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_PD:\n",
    "            y0 = self.__train_net_PD(x, ps_score=ps_score)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_CONSTANT_DROPOUT_5:\n",
    "            y0 = self.__train_net_constant_dropout(x, ps_score=0.5)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_CONSTANT_DROPOUT_2:\n",
    "            y0 = self.__train_net_constant_dropout(x, ps_score=0.2)\n",
    "        elif self.training_mode == Constants.DCN_TRAIN_NO_DROPOUT:\n",
    "            y0 = self.__train_net_no_droput(x)\n",
    "\n",
    "        return y0\n",
    "\n",
    "    def __train_net_constant_dropout(self, x, ps_score):\n",
    "        if ps_score == 0.2:\n",
    "            drop_out = self.dropout_2\n",
    "        elif ps_score == 0.5:\n",
    "            drop_out = self.dropout_5\n",
    "\n",
    "        # potential outcome1 Y(0)\n",
    "        y0 = F.relu(drop_out(self.hidden1_Y0(x)))\n",
    "        y0 = F.relu(drop_out(self.hidden2_Y0(y0)))\n",
    "        y0 = self.out_Y0(y0)\n",
    "\n",
    "        return y0\n",
    "\n",
    "    def __train_net_PD(self, x, ps_score):\n",
    "        entropy = Utils.get_shanon_entropy(ps_score.item())\n",
    "        dropout_prob = Utils.get_dropout_probability(entropy, gama=1)\n",
    "\n",
    "        # potential outcome1 Y(0)\n",
    "        y0_mask = Utils.get_dropout_mask(dropout_prob, self.hidden1_Y0(x))\n",
    "        y0 = F.relu(y0_mask * self.hidden1_Y0(x))\n",
    "        y0 = F.relu(y0_mask * self.hidden2_Y0(y0))\n",
    "        y0 = self.out_Y0(y0)\n",
    "\n",
    "        return y0\n",
    "\n",
    "    def __train_net_no_droput(self, x):\n",
    "        # potential outcome1 Y(0)\n",
    "        y0 = F.relu(self.hidden1_Y0(x))\n",
    "        y0 = F.relu(self.hidden2_Y0(y0))\n",
    "        y0 = self.out_Y0(y0)\n",
    "\n",
    "        return y0\n",
    "\n",
    "    def __eval_net(self, x):\n",
    "        # potential outcome1 Y(0)\n",
    "        y0 = F.relu(self.hidden1_Y0(x))\n",
    "        y0 = F.relu(self.hidden2_Y0(y0))\n",
    "        y0 = self.out_Y0(y0)\n",
    "\n",
    "        return y0\n",
    "\n",
    "\n",
    "####################################\n",
    "\n",
    "class DCN_network_2:\n",
    "    def __init__(self, input_nodes, training_mode, device):\n",
    "        self.dcn_shared = DCN_shared(input_nodes=input_nodes, ).to(device)\n",
    "        self.dcn_y1 = DCN_Y1().to(device)\n",
    "        self.dcn_y0 = DCN_Y0().to(device)\n",
    "\n",
    "    def train(self, train_parameters, device, train_mode=Constants.DCN_TRAIN_PD):\n",
    "        epochs = train_parameters[\"epochs\"]\n",
    "        treated_batch_size = train_parameters[\"treated_batch_size\"]\n",
    "        control_batch_size = train_parameters[\"control_batch_size\"]\n",
    "        lr = train_parameters[\"lr\"]\n",
    "        shuffle = train_parameters[\"shuffle\"]\n",
    "        treated_set_train = train_parameters[\"treated_set_train\"]\n",
    "        control_set_train = train_parameters[\"control_set_train\"]\n",
    "\n",
    "        self.dcn_shared.set_train_mode(training_mode=train_mode)\n",
    "        self.dcn_y1.set_train_mode(training_mode=train_mode)\n",
    "        self.dcn_y0.set_train_mode(training_mode=train_mode)\n",
    "\n",
    "        treated_data_loader_train = torch.utils.data.DataLoader(treated_set_train,\n",
    "                                                                batch_size=treated_batch_size,\n",
    "                                                                shuffle=shuffle,\n",
    "                                                                num_workers=1)\n",
    "\n",
    "        control_data_loader_train = torch.utils.data.DataLoader(control_set_train,\n",
    "                                                                batch_size=control_batch_size,\n",
    "                                                                shuffle=shuffle,\n",
    "                                                                num_workers=1)\n",
    "\n",
    "        optimizer_shared = optim.Adam(self.dcn_shared.parameters(), lr=lr)\n",
    "        optimizer_y1 = optim.Adam(self.dcn_y1.parameters(), lr=lr)\n",
    "        optimizer_y0 = optim.Adam(self.dcn_y0.parameters(), lr=lr)\n",
    "        lossF = nn.MSELoss()\n",
    "\n",
    "        min_loss = 100000.0\n",
    "        dataset_loss = 0.0\n",
    "        print(\".. Training started ..\")\n",
    "        print(device)\n",
    "        for epoch in range(epochs):\n",
    "            self.dcn_shared.train()\n",
    "            self.dcn_y1.train()\n",
    "            self.dcn_y0.train()\n",
    "            total_loss = 0\n",
    "            train_set_size = 0\n",
    "\n",
    "            if epoch % 2 == 0:\n",
    "                dataset_loss = 0\n",
    "                # train treated\n",
    "                for batch in treated_data_loader_train:\n",
    "                    covariates_X, ps_score, y_f, y_cf = batch\n",
    "                    covariates_X = covariates_X.to(device)\n",
    "                    ps_score = ps_score.squeeze().to(device)\n",
    "                    train_set_size += covariates_X.size(0)\n",
    "                    y1_hat = self.dcn_y1(self.dcn_shared(covariates_X, ps_score), ps_score)\n",
    "                    if torch.cuda.is_available():\n",
    "                        loss = lossF(y1_hat.float().cuda(),\n",
    "                                     y_f.float().cuda()).to(device)\n",
    "                    else:\n",
    "                        loss = lossF(y1_hat.float(),\n",
    "                                     y_f.float()).to(device)\n",
    "\n",
    "                    optimizer_shared.zero_grad()\n",
    "                    optimizer_y1.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer_shared.step()\n",
    "                    optimizer_y1.step()\n",
    "                    total_loss += loss.item()\n",
    "                dataset_loss = total_loss\n",
    "\n",
    "            elif epoch % 2 == 1:\n",
    "                # train control\n",
    "\n",
    "                for batch in control_data_loader_train:\n",
    "                    covariates_X, ps_score, y_f, y_cf = batch\n",
    "                    covariates_X = covariates_X.to(device)\n",
    "                    ps_score = ps_score.squeeze().to(device)\n",
    "\n",
    "                    train_set_size += covariates_X.size(0)\n",
    "                    y0_hat = self.dcn_y0(self.dcn_shared(covariates_X, ps_score), ps_score)\n",
    "                    if torch.cuda.is_available():\n",
    "                        loss = lossF(y0_hat.float().cuda(),\n",
    "                                     y_f.float().cuda()).to(device)\n",
    "                    else:\n",
    "                        loss = lossF(y0_hat.float(),\n",
    "                                     y_f.float()).to(device)\n",
    "                    optimizer_shared.zero_grad()\n",
    "                    optimizer_y0.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer_shared.step()\n",
    "                    optimizer_y0.step()\n",
    "                    total_loss += loss.item()\n",
    "                    total_loss += loss.item()\n",
    "                dataset_loss = dataset_loss + total_loss\n",
    "\n",
    "            if epoch % 10 == 9:\n",
    "                print(\"epoch: {0}, Treated + Control loss: {1}\".format(epoch, dataset_loss))\n",
    "\n",
    "    def eval(self, eval_parameters, device, input_nodes, train_mode):\n",
    "        treated_set = eval_parameters[\"treated_set\"]\n",
    "        control_set = eval_parameters[\"control_set\"]\n",
    "\n",
    "        self.dcn_shared.eval()\n",
    "        self.dcn_y1.eval()\n",
    "        self.dcn_y0.eval()\n",
    "        treated_data_loader = torch.utils.data.DataLoader(treated_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "        control_data_loader = torch.utils.data.DataLoader(control_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "\n",
    "        err_treated_list = []\n",
    "        err_control_list = []\n",
    "        true_ITE_list = []\n",
    "        predicted_ITE_list = []\n",
    "\n",
    "        ITE_dict_list = []\n",
    "\n",
    "        for batch in treated_data_loader:\n",
    "            covariates_X, ps_score, y_f, y_cf = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "            ps_score = ps_score.squeeze().to(device)\n",
    "            y1_hat = self.dcn_y1(self.dcn_shared(covariates_X, ps_score), ps_score)\n",
    "            y0_hat = self.dcn_y0(self.dcn_shared(covariates_X, ps_score), ps_score)\n",
    "            predicted_ITE = y1_hat - y0_hat\n",
    "\n",
    "            true_ITE = y_f - y_cf\n",
    "            if torch.cuda.is_available():\n",
    "                diff = true_ITE.float().cuda() - predicted_ITE.float().cuda()\n",
    "            else:\n",
    "                diff = true_ITE.float() - predicted_ITE.float()\n",
    "\n",
    "            ITE_dict_list.append(self.create_ITE_Dict(covariates_X,\n",
    "                                                      ps_score.item(), y_f.item(),\n",
    "                                                      y_cf.item(),\n",
    "                                                      true_ITE.item(),\n",
    "                                                      predicted_ITE.item(),\n",
    "                                                      diff.item()))\n",
    "            err_treated_list.append(diff.item())\n",
    "            true_ITE_list.append(true_ITE.item())\n",
    "            predicted_ITE_list.append(predicted_ITE.item())\n",
    "\n",
    "        for batch in control_data_loader:\n",
    "            covariates_X, ps_score, y_f, y_cf = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "            ps_score = ps_score.squeeze().to(device)\n",
    "            y1_hat = self.dcn_y1(self.dcn_shared(covariates_X, ps_score), ps_score)\n",
    "            y0_hat = self.dcn_y0(self.dcn_shared(covariates_X, ps_score), ps_score)\n",
    "            predicted_ITE = y1_hat - y0_hat\n",
    "            true_ITE = y_cf - y_f\n",
    "            if torch.cuda.is_available():\n",
    "                diff = true_ITE.float().cuda() - predicted_ITE.float().cuda()\n",
    "            else:\n",
    "                diff = true_ITE.float() - predicted_ITE.float()\n",
    "\n",
    "            ITE_dict_list.append(self.create_ITE_Dict(covariates_X,\n",
    "                                                      ps_score.item(), y_f.item(),\n",
    "                                                      y_cf.item(),\n",
    "                                                      true_ITE.item(),\n",
    "                                                      predicted_ITE.item(),\n",
    "                                                      diff.item()))\n",
    "            err_control_list.append(diff.item())\n",
    "            true_ITE_list.append(true_ITE.item())\n",
    "            predicted_ITE_list.append(predicted_ITE.item())\n",
    "\n",
    "        # print(err_treated_list)\n",
    "        # print(err_control_list)\n",
    "        return {\n",
    "            \"treated_err\": err_treated_list,\n",
    "            \"control_err\": err_control_list,\n",
    "            \"true_ITE\": true_ITE_list,\n",
    "            \"predicted_ITE\": predicted_ITE_list,\n",
    "            \"ITE_dict_list\": ITE_dict_list\n",
    "        }\n",
    "\n",
    "    def eval_semi_supervised(self, eval_parameters, device, input_nodes, train_mode, treated_flag):\n",
    "        eval_set = eval_parameters[\"eval_set\"]\n",
    "\n",
    "        self.dcn_shared.eval()\n",
    "        self.dcn_y1.eval()\n",
    "        self.dcn_y0.eval()\n",
    "        treated_data_loader = torch.utils.data.DataLoader(eval_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "\n",
    "        y_f_list = []\n",
    "        y_cf_list = []\n",
    "\n",
    "        for batch in treated_data_loader:\n",
    "            covariates_X, ps_score = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "            ps_score = ps_score.squeeze().to(device)\n",
    "            y1_hat = self.dcn_y1(self.dcn_shared(covariates_X, ps_score), ps_score)\n",
    "            y0_hat = self.dcn_y0(self.dcn_shared(covariates_X, ps_score), ps_score)\n",
    "            if treated_flag:\n",
    "                y_f_list.append(y1_hat.item())\n",
    "                y_cf_list.append(y0_hat.item())\n",
    "            else:\n",
    "                y_f_list.append(y0_hat.item())\n",
    "                y_cf_list.append(y1_hat.item())\n",
    "\n",
    "        return {\n",
    "            \"y_f_list\": np.array(y_f_list),\n",
    "            \"y_cf_list\": np.array(y_cf_list)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def create_ITE_Dict(covariates_X, ps_score, y_f, y_cf, true_ITE,\n",
    "                        predicted_ITE, diff):\n",
    "        result_dict = OrderedDict()\n",
    "        covariate_list = [element.item() for element in covariates_X.flatten()]\n",
    "        idx = 0\n",
    "        for item in covariate_list:\n",
    "            idx += 1\n",
    "            result_dict[\"X\" + str(idx)] = item\n",
    "\n",
    "        result_dict[\"ps_score\"] = ps_score\n",
    "        result_dict[\"factual\"] = y_f\n",
    "        result_dict[\"counter_factual\"] = y_cf\n",
    "        result_dict[\"true_ITE\"] = true_ITE\n",
    "        result_dict[\"predicted_ITE\"] = predicted_ITE\n",
    "        result_dict[\"diff\"] = diff\n",
    "\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi supervised PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 25)\n",
      "(485, 25)\n",
      "train_PD\n",
      "train_PD\n",
      "train_PD\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 9, Treated + Control loss: 1961.2638130664895\n",
      "epoch: 19, Treated + Control loss: 1472.9067256047892\n",
      "epoch: 29, Treated + Control loss: 1296.8970066994016\n",
      "epoch: 39, Treated + Control loss: 1189.1118177572896\n",
      "epoch: 49, Treated + Control loss: 1072.3577394380163\n",
      "epoch: 59, Treated + Control loss: 927.0307484128989\n",
      "epoch: 69, Treated + Control loss: 860.0475168312505\n",
      "epoch: 79, Treated + Control loss: 837.9197884738633\n",
      "epoch: 89, Treated + Control loss: 838.1204476781782\n",
      "epoch: 99, Treated + Control loss: 873.7809079849094\n",
      "epoch: 109, Treated + Control loss: 701.6926627746261\n",
      "epoch: 119, Treated + Control loss: 817.5942120288128\n",
      "epoch: 129, Treated + Control loss: 637.4452150745719\n",
      "epoch: 139, Treated + Control loss: 807.6539895935048\n",
      "epoch: 149, Treated + Control loss: 872.7669742318268\n",
      "epoch: 159, Treated + Control loss: 597.418499731079\n",
      "epoch: 169, Treated + Control loss: 633.2379789996214\n",
      "epoch: 179, Treated + Control loss: 814.7799564074135\n",
      "epoch: 189, Treated + Control loss: 506.64254109853573\n",
      "epoch: 199, Treated + Control loss: 563.9724262212194\n",
      "epoch: 209, Treated + Control loss: 523.9502972891332\n",
      "epoch: 219, Treated + Control loss: 574.2868552388983\n",
      "epoch: 229, Treated + Control loss: 489.6136347226875\n",
      "epoch: 239, Treated + Control loss: 524.8997180352076\n",
      "epoch: 249, Treated + Control loss: 482.9490391765212\n",
      "epoch: 259, Treated + Control loss: 506.13976799413376\n",
      "epoch: 269, Treated + Control loss: 422.6057811375895\n",
      "epoch: 279, Treated + Control loss: 576.8243094300342\n",
      "epoch: 289, Treated + Control loss: 479.6360132184319\n",
      "epoch: 299, Treated + Control loss: 525.6934483182354\n",
      "epoch: 309, Treated + Control loss: 473.75131998463166\n",
      "epoch: 319, Treated + Control loss: 492.946890192905\n",
      "epoch: 329, Treated + Control loss: 393.22630885751664\n",
      "epoch: 339, Treated + Control loss: 513.8903165908289\n",
      "epoch: 349, Treated + Control loss: 492.040101271622\n",
      "epoch: 359, Treated + Control loss: 426.6132380111453\n",
      "epoch: 369, Treated + Control loss: 501.02835552853685\n",
      "epoch: 379, Treated + Control loss: 437.0111624186809\n",
      "epoch: 389, Treated + Control loss: 466.16892170085407\n",
      "epoch: 399, Treated + Control loss: 556.6964420835538\n"
     ]
    }
   ],
   "source": [
    "t_treated = Utils.create_tensors_to_train_DCN(data_loader_dict_train[\"treated_data\"], dL)\n",
    "t_control =  Utils.create_tensors_to_train_DCN(data_loader_dict_train[\"control_data\"], dL)\n",
    "\n",
    "print(data_loader_dict_train[\"treated_data\"][0].shape)\n",
    "print(data_loader_dict_train[\"control_data\"][0].shape)\n",
    "\n",
    "train_mode = Constants.DCN_TRAIN_PD\n",
    "DCN_train_parameters = {\n",
    "                \"epochs\": epochs,\n",
    "                \"lr\": lr,\n",
    "                \"treated_batch_size\": 1,\n",
    "                \"control_batch_size\": 1,\n",
    "                \"shuffle\": True,\n",
    "                \"treated_set_train\": t_treated,\n",
    "                \"control_set_train\": t_control,\n",
    "                \"input_nodes\": input_nodes\n",
    "    }\n",
    "\n",
    "            # train DCN network\n",
    "dcn_ss_pd = DCN_network_2(input_nodes, train_mode, device )\n",
    "dcn_ss_pd.train(DCN_train_parameters, device, train_mode=train_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_score_list_treated_np = np.array(ps_score_list_treated)\n",
    "eval_set = convert_to_tensor_DCN_PS(treated_generated.detach(), ps_score_list_treated_np)\n",
    "\n",
    "DCN_test_parameters = {\n",
    "            \"eval_set\": eval_set\n",
    "}\n",
    "treated_gen_y = dcn_ss_pd.eval_semi_supervised(DCN_test_parameters, device, input_nodes,\n",
    "                                 Constants.DCN_EVALUATION, treated_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi Supervised No PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Sizes\n",
      "(112, 25)\n",
      "(112, 25)\n",
      "train_with_no_dropout\n",
      "train_with_no_dropout\n",
      "train_with_no_dropout\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 9, Treated + Control loss: 127.02410712396886\n",
      "epoch: 19, Treated + Control loss: 139.83041609955637\n",
      "epoch: 29, Treated + Control loss: 34.86894857322841\n",
      "epoch: 39, Treated + Control loss: 218.43963904438533\n",
      "epoch: 49, Treated + Control loss: 19.482150484427734\n",
      "epoch: 59, Treated + Control loss: 60.232615848216255\n",
      "epoch: 69, Treated + Control loss: 31.350609298416714\n",
      "epoch: 79, Treated + Control loss: 38.55644314837173\n",
      "epoch: 89, Treated + Control loss: 10.355409078315915\n",
      "epoch: 99, Treated + Control loss: 40.42866590691176\n"
     ]
    }
   ],
   "source": [
    "_np_control_matched_X = tuple_matched_control[0]\n",
    "_np_ps_score_list_control_matched = tuple_matched_control[1]\n",
    "_np_control_matched_f = tuple_matched_control[2]\n",
    "_np_control_matched_cf = tuple_matched_control[3]\n",
    "\n",
    "\n",
    "_tensor_control = Utils.convert_to_tensor_DCN(_np_control_matched_X, _np_ps_score_list_control_matched,\n",
    "                                            _np_control_matched_f, _np_control_matched_cf)\n",
    "\n",
    "_np_original_X = tuple_treated[0]\n",
    "_np_original_ps_score = tuple_treated[1]\n",
    "_np_original_Y_f = tuple_treated[2]\n",
    "_np_original_Y_cf = tuple_treated[3]\n",
    "\n",
    "print(\"Actual Sizes\")\n",
    "print(_np_control_matched_X.shape)\n",
    "print(_np_original_X.shape)\n",
    "\n",
    "_tensor_treated = Utils.convert_to_tensor_DCN(_np_original_X, _np_original_ps_score,\n",
    "                                            _np_original_Y_cf, _np_original_Y_f)\n",
    "\n",
    "train_mode = Constants.DCN_TRAIN_NO_DROPOUT\n",
    "DCN_train_parameters = {\n",
    "                \"epochs\": epochs,\n",
    "                \"lr\": lr,\n",
    "                \"treated_batch_size\": 1,\n",
    "                \"control_batch_size\": 1,\n",
    "                \"shuffle\": True,\n",
    "                \"treated_set_train\": _tensor_treated,\n",
    "                \"control_set_train\": _tensor_control,\n",
    "                \"input_nodes\": input_nodes\n",
    "    }\n",
    "\n",
    "# train DCN network\n",
    "dcn_ss_no_pd = DCN_network_2(input_nodes, train_mode, device )\n",
    "dcn_ss_no_pd.train(DCN_train_parameters, device, train_mode=train_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_score_list_treated_np = np.array(ps_score_list_treated)\n",
    "eval_set = convert_to_tensor_DCN_PS(treated_generated.detach(), ps_score_list_treated_np)\n",
    "\n",
    "DCN_test_parameters = {\n",
    "            \"eval_set\": eval_set\n",
    "}\n",
    "treated_gen_y = dcn_ss_no_pd.eval_semi_supervised(DCN_test_parameters, device, input_nodes,\n",
    "                                 Constants.DCN_EVALUATION, treated_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434, 1)\n",
      "(112, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(546, 25)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_treated_generated = treated_generated.detach().numpy()\n",
    "np_ps_score_list_gen_treated = ps_score_list_treated_np\n",
    "np_treated_gen_f = Utils.convert_to_col_vector(treated_gen_y[\"y_f_list\"])\n",
    "np_treated_gen_cf = Utils.convert_to_col_vector(treated_gen_y[\"y_cf_list\"])\n",
    "\n",
    "print(np_treated_gen_f.shape)\n",
    "\n",
    "np_original_X = tuple_treated[0]\n",
    "np_original_ps_score = tuple_treated[1]\n",
    "np_original_Y_f = tuple_treated[2]\n",
    "np_original_Y_cf = tuple_treated[3]\n",
    "\n",
    "print(np_original_Y_f.shape)\n",
    "\n",
    "np_treated_x = np.concatenate((np_treated_generated, np_original_X), axis=0)\n",
    "np_treated_ps = np.concatenate((np_ps_score_list_gen_treated, np_original_ps_score), axis=0)\n",
    "np_treated_f = np.concatenate((np_treated_gen_f, np_original_Y_f), axis=0)\n",
    "np_treated_cf = np.concatenate((np_treated_gen_cf, np_original_Y_cf), axis=0)\n",
    "\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_x, np_treated_ps,\n",
    "                                            np_treated_f, np_treated_cf)\n",
    "\n",
    "np_treated_x.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434, 1)\n",
      "(112, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(546, 25)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_control_unmatched_X = tuple_unmatched_control[0]\n",
    "np_ps_score_list_control_unmatched = tuple_unmatched_control[1]\n",
    "np_control_unmatched_f = tuple_unmatched_control[2]\n",
    "np_control_unmatched_cf = tuple_unmatched_control[3]\n",
    "\n",
    "np_control_matched_X = tuple_matched_control[0]\n",
    "np_ps_score_list_control_matched = tuple_matched_control[1]\n",
    "np_control_matched_f = tuple_matched_control[2]\n",
    "np_control_matched_cf = tuple_matched_control[3]\n",
    "\n",
    "print(np_control_unmatched_cf.shape)\n",
    "print(np_control_matched_cf.shape)\n",
    "\n",
    "np_control_x = np.concatenate((np_control_unmatched_X, np_control_matched_X), axis=0)\n",
    "np_control_ps = np.concatenate((np_ps_score_list_control_unmatched, np_ps_score_list_control_matched), axis=0)\n",
    "np_control_f = np.concatenate((np_control_unmatched_f, np_control_matched_f), axis=0)\n",
    "np_control_cf = np.concatenate((np_control_unmatched_cf, np_control_matched_cf), axis=0)\n",
    "\n",
    "tensor_control = Utils.convert_to_tensor_DCN(np_control_x, np_control_ps,\n",
    "                                            np_control_f, np_control_cf)\n",
    "\n",
    "\n",
    "np_control_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DCN training using all dataset no PD ###\n",
      "train_with_no_dropout\n",
      "train_with_no_dropout\n",
      "train_with_no_dropout\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 9, Treated + Control loss: 1923.5984726065035\n",
      "epoch: 19, Treated + Control loss: 1335.7644194515124\n",
      "epoch: 29, Treated + Control loss: 944.7288769938918\n",
      "epoch: 39, Treated + Control loss: 775.0536957502468\n",
      "epoch: 49, Treated + Control loss: 677.5601658655917\n",
      "epoch: 59, Treated + Control loss: 509.5134793673059\n",
      "epoch: 69, Treated + Control loss: 507.101072462347\n",
      "epoch: 79, Treated + Control loss: 386.40602669220834\n",
      "epoch: 89, Treated + Control loss: 380.55538553593397\n",
      "epoch: 99, Treated + Control loss: 358.60722736265075\n",
      "epoch: 109, Treated + Control loss: 335.4339377170128\n",
      "epoch: 119, Treated + Control loss: 305.6068016312066\n",
      "epoch: 129, Treated + Control loss: 270.7890891432603\n",
      "epoch: 139, Treated + Control loss: 239.82527028199843\n",
      "epoch: 149, Treated + Control loss: 220.15096876160476\n",
      "epoch: 159, Treated + Control loss: 211.9422173362501\n",
      "epoch: 169, Treated + Control loss: 201.18315128861505\n",
      "epoch: 179, Treated + Control loss: 184.74914489173003\n",
      "epoch: 189, Treated + Control loss: 176.28683383157423\n",
      "epoch: 199, Treated + Control loss: 164.34387243421145\n",
      "epoch: 209, Treated + Control loss: 155.6054823428773\n",
      "epoch: 219, Treated + Control loss: 149.3253265833522\n",
      "epoch: 229, Treated + Control loss: 156.22089626399975\n",
      "epoch: 239, Treated + Control loss: 138.1918013211747\n",
      "epoch: 249, Treated + Control loss: 138.6613141716348\n",
      "epoch: 259, Treated + Control loss: 144.3462202179228\n",
      "epoch: 269, Treated + Control loss: 128.84422900170014\n",
      "epoch: 279, Treated + Control loss: 143.21100275200712\n",
      "epoch: 289, Treated + Control loss: 107.86001442228996\n",
      "epoch: 299, Treated + Control loss: 111.25920773205064\n",
      "epoch: 309, Treated + Control loss: 101.95761933504832\n",
      "epoch: 319, Treated + Control loss: 105.11088900901203\n",
      "epoch: 329, Treated + Control loss: 101.51714057027151\n",
      "epoch: 339, Treated + Control loss: 81.77453591866711\n",
      "epoch: 349, Treated + Control loss: 100.95226922783047\n",
      "epoch: 359, Treated + Control loss: 91.10779017931145\n",
      "epoch: 369, Treated + Control loss: 92.45143754749317\n",
      "epoch: 379, Treated + Control loss: 91.30219647093736\n",
      "epoch: 389, Treated + Control loss: 112.40879708782188\n",
      "epoch: 399, Treated + Control loss: 67.38864355869919\n"
     ]
    }
   ],
   "source": [
    "print(\"### DCN training using all dataset no PD ###\")\n",
    "\n",
    "DCN_train_parameters = {\n",
    "                \"epochs\": epochs,\n",
    "                \"lr\": lr,\n",
    "                \"treated_batch_size\": 1,\n",
    "                \"control_batch_size\": 1,\n",
    "                \"shuffle\": True,\n",
    "                \"treated_set_train\": tensor_treated,\n",
    "                \"control_set_train\": tensor_control,\n",
    "                \"input_nodes\": input_nodes\n",
    "    }\n",
    "\n",
    "train_mode = Constants.DCN_TRAIN_NO_DROPOUT\n",
    "dcn_pd = DCN_network_2(input_nodes, train_mode, device )\n",
    "dcn_pd.train(DCN_train_parameters, device, train_mode=train_mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Testing phase ------------\n",
      " Treated Statistics ==>\n",
      "(27, 25)\n",
      " Control Statistics ==>\n",
      "(123, 25)\n",
      "PSM\n",
      "2.6879088326703333\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"----------- Testing phase ------------\")\n",
    "dL = DataLoader()\n",
    "ps_test_set = dL.convert_to_tensor(np_covariates_X_test,\n",
    "                                           np_covariates_Y_test)\n",
    "\n",
    "is_synthetic = False\n",
    "input_nodes = 25\n",
    "\n",
    "        # get propensity scores using NN\n",
    "ps_net_NN = Propensity_socre_network()\n",
    "ps_eval_parameters_NN = {\n",
    "            \"eval_set\": ps_test_set,\n",
    "            \"model_path\": Constants.PROP_SCORE_NN_MODEL_PATH\n",
    "                .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR),\n",
    "            \"input_nodes\": input_nodes\n",
    "}\n",
    "ps_score_list_NN = ps_net_NN.eval(ps_eval_parameters_NN, device, phase=\"eval\")\n",
    "\n",
    "data_loader_dict = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                        np_covariates_Y_test,\n",
    "                                                        ps_score_list_NN,\n",
    "                                                        is_synthetic)\n",
    "\n",
    "\n",
    "treated_group = data_loader_dict[\"treated_data\"]\n",
    "np_treated_df_X = treated_group[0]\n",
    "np_treated_ps_score = treated_group[1]\n",
    "np_treated_df_Y_f = treated_group[2]\n",
    "np_treated_df_Y_cf = treated_group[3]\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_df_X, np_treated_ps_score,\n",
    "                                                     np_treated_df_Y_f, np_treated_df_Y_cf)\n",
    "\n",
    "control_group = data_loader_dict[\"control_data\"]\n",
    "np_control_df_X = control_group[0]\n",
    "np_control_ps_score = control_group[1]\n",
    "np_control_df_Y_f = control_group[2]\n",
    "np_control_df_Y_cf = control_group[3]\n",
    "tensor_control = Utils.convert_to_tensor_DCN(np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f, np_control_df_Y_cf)\n",
    "\n",
    "DCN_test_parameters = {\n",
    "            \"treated_set\": tensor_treated,\n",
    "            \"control_set\": tensor_control\n",
    "}\n",
    "\n",
    "response_dict = dcn_pd.eval(DCN_test_parameters, device, input_nodes,\n",
    "                                 Constants.DCN_EVALUATION)\n",
    "err_treated = [ele ** 2 for ele in response_dict[\"treated_err\"]]\n",
    "err_control = [ele ** 2 for ele in response_dict[\"control_err\"]]\n",
    "\n",
    "total_sum = sum(err_treated) + sum(err_control)\n",
    "total_item = len(err_treated) + len(err_control)\n",
    "MSE = total_sum / total_item\n",
    "\n",
    "print(\"PSM\")\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCN_PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 25)\n",
      "(485, 25)\n",
      "train_PD\n",
      "train_PD\n",
      "train_PD\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 9, Treated + Control loss: 1836.2886278360947\n",
      "epoch: 19, Treated + Control loss: 1563.3444581875556\n",
      "epoch: 29, Treated + Control loss: 1270.2965044251723\n",
      "epoch: 39, Treated + Control loss: 1193.3462254907008\n",
      "epoch: 49, Treated + Control loss: 1041.038339991488\n",
      "epoch: 59, Treated + Control loss: 900.1107994784106\n",
      "epoch: 69, Treated + Control loss: 947.7266284664393\n",
      "epoch: 79, Treated + Control loss: 774.2350817413226\n",
      "epoch: 89, Treated + Control loss: 834.2603956277833\n",
      "epoch: 99, Treated + Control loss: 637.0902408858775\n",
      "epoch: 109, Treated + Control loss: 742.3948909238206\n",
      "epoch: 119, Treated + Control loss: 694.3209205018629\n",
      "epoch: 129, Treated + Control loss: 674.6392585598697\n",
      "epoch: 139, Treated + Control loss: 615.0801592739156\n",
      "epoch: 149, Treated + Control loss: 662.7991940983504\n",
      "epoch: 159, Treated + Control loss: 563.0515799035724\n",
      "epoch: 169, Treated + Control loss: 581.4781293988622\n",
      "epoch: 179, Treated + Control loss: 495.2368431785012\n",
      "epoch: 189, Treated + Control loss: 618.0893289787327\n",
      "epoch: 199, Treated + Control loss: 548.1845444625942\n",
      "epoch: 209, Treated + Control loss: 462.2022169757929\n",
      "epoch: 219, Treated + Control loss: 618.4812497215353\n",
      "epoch: 229, Treated + Control loss: 609.9515598401176\n",
      "epoch: 239, Treated + Control loss: 565.0061533546632\n",
      "epoch: 249, Treated + Control loss: 579.8424199452791\n",
      "epoch: 259, Treated + Control loss: 516.8793480076183\n",
      "epoch: 269, Treated + Control loss: 508.25400169263344\n",
      "epoch: 279, Treated + Control loss: 416.99896267207805\n",
      "epoch: 289, Treated + Control loss: 512.967694546996\n",
      "epoch: 299, Treated + Control loss: 474.7438650700069\n",
      "epoch: 309, Treated + Control loss: 508.06620130056035\n",
      "epoch: 319, Treated + Control loss: 428.10512712231866\n",
      "epoch: 329, Treated + Control loss: 437.6325035461298\n",
      "epoch: 339, Treated + Control loss: 565.8264781057715\n",
      "epoch: 349, Treated + Control loss: 377.8547053430167\n",
      "epoch: 359, Treated + Control loss: 413.0978433889375\n",
      "epoch: 369, Treated + Control loss: 380.87137232569887\n",
      "epoch: 379, Treated + Control loss: 466.01111859216576\n",
      "epoch: 389, Treated + Control loss: 406.19356488085987\n",
      "epoch: 399, Treated + Control loss: 465.80652875474993\n"
     ]
    }
   ],
   "source": [
    "t_treated = Utils.create_tensors_to_train_DCN(data_loader_dict_train[\"treated_data\"], dL)\n",
    "t_control =  Utils.create_tensors_to_train_DCN(data_loader_dict_train[\"control_data\"], dL)\n",
    "\n",
    "print(data_loader_dict_train[\"treated_data\"][0].shape)\n",
    "print(data_loader_dict_train[\"control_data\"][0].shape)\n",
    "\n",
    "train_mode = Constants.DCN_TRAIN_PD\n",
    "DCN_train_parameters = {\n",
    "                \"epochs\": epochs,\n",
    "                \"lr\": lr,\n",
    "                \"treated_batch_size\": 1,\n",
    "                \"control_batch_size\": 1,\n",
    "                \"shuffle\": True,\n",
    "                \"treated_set_train\": t_treated,\n",
    "                \"control_set_train\": t_control,\n",
    "                \"input_nodes\": input_nodes\n",
    "    }\n",
    "\n",
    "            # train DCN network\n",
    "dcn_ss_pd = DCN_network_2(input_nodes, train_mode, device )\n",
    "dcn_ss_pd.train(DCN_train_parameters, device, train_mode=train_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Testing phase ------------\n",
      " Treated Statistics ==>\n",
      "(27, 25)\n",
      " Control Statistics ==>\n",
      "(123, 25)\n",
      "PSM\n",
      "2.7972959882390196\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"----------- Testing phase ------------\")\n",
    "dL = DataLoader()\n",
    "ps_test_set = dL.convert_to_tensor(np_covariates_X_test,\n",
    "                                           np_covariates_Y_test)\n",
    "\n",
    "is_synthetic = False\n",
    "input_nodes = 25\n",
    "\n",
    "        # get propensity scores using NN\n",
    "ps_net_NN = Propensity_socre_network()\n",
    "ps_eval_parameters_NN = {\n",
    "            \"eval_set\": ps_test_set,\n",
    "            \"model_path\": Constants.PROP_SCORE_NN_MODEL_PATH\n",
    "                .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR),\n",
    "            \"input_nodes\": input_nodes\n",
    "}\n",
    "ps_score_list_NN = ps_net_NN.eval(ps_eval_parameters_NN, device, phase=\"eval\")\n",
    "\n",
    "data_loader_dict = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                        np_covariates_Y_test,\n",
    "                                                        ps_score_list_NN,\n",
    "                                                        is_synthetic)\n",
    "\n",
    "\n",
    "treated_group = data_loader_dict[\"treated_data\"]\n",
    "np_treated_df_X = treated_group[0]\n",
    "np_treated_ps_score = treated_group[1]\n",
    "np_treated_df_Y_f = treated_group[2]\n",
    "np_treated_df_Y_cf = treated_group[3]\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_df_X, np_treated_ps_score,\n",
    "                                                     np_treated_df_Y_f, np_treated_df_Y_cf)\n",
    "\n",
    "control_group = data_loader_dict[\"control_data\"]\n",
    "np_control_df_X = control_group[0]\n",
    "np_control_ps_score = control_group[1]\n",
    "np_control_df_Y_f = control_group[2]\n",
    "np_control_df_Y_cf = control_group[3]\n",
    "tensor_control = Utils.convert_to_tensor_DCN(np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f, np_control_df_Y_cf)\n",
    "\n",
    "DCN_test_parameters = {\n",
    "            \"treated_set\": tensor_treated,\n",
    "            \"control_set\": tensor_control\n",
    "}\n",
    "\n",
    "response_dict = dcn_ss_pd.eval(DCN_test_parameters, device, input_nodes,\n",
    "                                 Constants.DCN_EVALUATION)\n",
    "err_treated = [ele ** 2 for ele in response_dict[\"treated_err\"]]\n",
    "err_control = [ele ** 2 for ele in response_dict[\"control_err\"]]\n",
    "\n",
    "total_sum = sum(err_treated) + sum(err_control)\n",
    "total_item = len(err_treated) + len(err_control)\n",
    "MSE = total_sum / total_item\n",
    "\n",
    "print(\"PSM\")\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
