{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from Utils import Utils\n",
    "\n",
    "csv_path = \"/Users/shantanughosh/Desktop/Shantanu_MS/Research/Mattia_Prosperi/Propensity_Dropout/Dataset/ihdp_sample.csv\"\n",
    "\n",
    "from dataloader import DataLoader\n",
    "\n",
    "dL = DataLoader()\n",
    "split_size = 0.8\n",
    "# np_covariates_X_train, np_covariates_X_test, \\\n",
    "# np_covariates_Y_train, np_covariates_Y_test = dL.preprocess_data_from_csv(csv_path, split_size=0.8)\n",
    "# norm_X_train = np_covariates_X_train / np.linalg.norm(np_covariates_X_train)\n",
    "# ps_train_set = dL.convert_to_tensor(np_covariates_X_train, np_covariates_Y_train)\n",
    "# device = Utils.get_device()\n",
    "\n",
    "# print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.5],\n",
      "       [1. ]]), array([[2],\n",
      "       [0]]))\n",
      "[[2]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=1)\n",
    "neigh.fit(samples) \n",
    "\n",
    "dist, neigbor = neigh.kneighbors([[1., 1., 1.], [0., 0., 1.]]) \n",
    "print(ret)\n",
    "print(neigbor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  20  30  40  50]\n",
      " [100 200 300 400 500]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filter_indices = [1, 2]\n",
    "axis = 0\n",
    "array = np.array([[1, 2, 3, 4, 5], \n",
    "                  [10, 20, 30, 40, 50], \n",
    "                  [100, 200, 300, 400, 500]])\n",
    "\n",
    "print(np.take(array, filter_indices, axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478]\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 85,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 98,\n",
       " 99,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 132,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 163,\n",
       " 164,\n",
       " 166,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 257,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 324,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 359,\n",
       " 360,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 368,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 396,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478]\n",
    "l2 = [258, 302, 303, 32, 97, 53, 133, 100, 131, 458, 326, 367, 66, 1, 188, 174, 216, 162, 165, 138, 361, 397, 86, 239, 369, 101, 148, 358, 447, 117, 48, 323, 256, 266, 344, 62, 167, 404, 325, 418, 70, 332, 84, 349, 395, 49, 274, 39, 229, 293, 348, 459]\n",
    "l2.sort()\n",
    "l5 = set(l2)\n",
    "l3 = list(filter(lambda x: x not in l5, l1))\n",
    "print(l1)\n",
    "print(l2.sort())\n",
    "l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Data Loading synthetic..\n",
      "std dev\n",
      "ps_np_covariates_X: (747, 77)\n",
      "ps_np_treatment_Y: (747, 1)\n",
      "np_covariates_X_train: (597, 77)\n",
      "np_covariates_Y_train: (597, 1)\n",
      "------------------------------------------------------------\n",
      "np_covariates_X_test: (150, 77)\n",
      "np_covariates_Y_test: (150, 1)\n"
     ]
    }
   ],
   "source": [
    "np_covariates_X_train, np_covariates_X_test, np_covariates_Y_train, np_covariates_Y_test = \\\n",
    "                dL.preprocess_data_from_csv_augmented(csv_path, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5 1.5 6.5 4.5]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1], [2], [3]])\n",
    "y = np.array([[4, 5, 6, 9], [7, 8, 19, 18]])\n",
    "y1 = y\n",
    "np.random.seed(0)\n",
    "std = np.std(y, axis=0)\n",
    "print(std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time:2020-06-28 02:31:07.035978\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Current date time in local system\n",
    "start = datetime.now()\n",
    "print(\"start_time:\" + str(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration : 0:00:10.656428\n"
     ]
    }
   ],
   "source": [
    "print(\"Duration : \" + str( start - now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now:2020-06-28 02:30:31.358836\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"now:\" + str(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1.5\n",
      "1.5\n",
      "6.5\n",
      "4.5\n",
      "[[ 5.1863139   5.1863139  22.47402689 15.55894169]\n",
      " [ 5.1863139   5.1863139  22.47402689 15.55894169]]\n",
      "[[ 9.1863139  10.1863139  28.47402689 24.55894169]\n",
      " [12.1863139  13.1863139  41.47402689 33.55894169]]\n",
      "33.558941691434796\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "print(std.shape[0])\n",
    "y = np.array([[4, 5, 6, 9], [7, 8, 19, 18]])\n",
    "id = -1\n",
    "noise = np.empty([y.shape[0], y.shape[1]])\n",
    "for s in std:\n",
    "    id +=1\n",
    "    print(s)\n",
    "    np.random.seed(0)\n",
    "#     noise[:, id] = s\n",
    "    \n",
    "    noise[:, id] = np.random.normal(0, 1.96 * s)\n",
    "    \n",
    "print(noise)\n",
    "\n",
    "noise_y = noise + y\n",
    "\n",
    "print(noise_y)\n",
    "\n",
    "np.random.seed(0)\n",
    "print(18 + np.random.normal(0, 1.96 * 4.5))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.        ,  5.        ,  6.        ,  9.        ,  9.1863139 ,\n",
       "        10.1863139 , 28.47402689, 24.55894169],\n",
       "       [ 7.        ,  8.        , 19.        , 18.        , 12.1863139 ,\n",
       "        13.1863139 , 41.47402689, 33.55894169]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_covariates_X = np.concatenate((y, noise_y), axis=1)\n",
    "np_covariates_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.46065812, 3.0567042 , 2.11719028, ..., 4.28878054, 3.10172903,\n",
       "        1.54447345],\n",
       "       [9.79916421, 6.72271051, 0.56458668, ..., 6.61807015, 0.84864102,\n",
       "        2.20950882],\n",
       "       [3.08468093, 0.80308587, 8.23004286, ..., 9.35602002, 4.00396147,\n",
       "        3.45538455],\n",
       "       ...,\n",
       "       [7.6659666 , 5.92223383, 5.4396373 , ..., 2.00343179, 3.31540825,\n",
       "        5.78755804],\n",
       "       [7.03123107, 5.19232512, 0.0989919 , ..., 2.46743731, 8.43478922,\n",
       "        1.385397  ],\n",
       "       [6.85562166, 5.98248875, 0.6526608 , ..., 8.12321355, 2.73154856,\n",
       "        0.28318412]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrandom = np.random.random((747,25)) *10\n",
    "nrandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Sparse_Propensity_score import Sparse_Propensity_score\n",
    "train_parameters_SAE = {\n",
    "            \"epochs\": 200,\n",
    "            \"lr\": 0.0001,\n",
    "            \"batch_size\": 32,\n",
    "            \"shuffle\": True,\n",
    "            \"train_set\": ps_train_set,\n",
    "            \"sparsity_probability\": 0.08,\n",
    "            \"weight_decay\": 0.0003,\n",
    "            \"BETA\": 0.4,\n",
    "            \"model_save_path\": \"./Propensity_Model/SAE_PS_model_iter_id_\"\n",
    "                               + str(1) + \"_epoch_{0}_lr_{1}.pth\"\n",
    "        }\n",
    "\n",
    "# train_parameters_SAE = {\n",
    "#     \"epochs\": 20,\n",
    "#     \"lr\": 0.001,\n",
    "#     \"batch_size\": 32,\n",
    "#     \"shuffle\": True,\n",
    "#     \"train_set\": ps_train_set,\n",
    "#     \"sparsity_probability\": 0.05,\n",
    "#     \"weight_decay\": 0.0003,\n",
    "#     \"BETA\": 3,\n",
    "#     \"model_save_path\": \"./Propensity_Model/SAE_PS_model_iter_id_\" + str(1) + \"_epoch_{0}_lr_{1}.pth\"\n",
    "# }\n",
    "\n",
    "# train_parameters_SAE = {\n",
    "#     \"epochs\": 200,\n",
    "#     \"lr\": 0.0001,\n",
    "#     \"batch_size\": 32,\n",
    "#     \"shuffle\": True,\n",
    "#     \"train_set\": ps_train_set,\n",
    "#     \"sparsity_probability\": 0.08,\n",
    "#     \"weight_decay\": 0.0003,\n",
    "#     \"BETA\": 0.4,\n",
    "#     \"model_save_path\": \"./Propensity_Model/SAE_PS_model_iter_id_\" + str(1) + \"_epoch_{0}_lr_{1}.pth\"\n",
    "# }\n",
    "\n",
    "ps_net_SAE = Sparse_Propensity_score()\n",
    "print(\"############### Propensity Score SAE net Training ###############\")\n",
    "sparse_classifier, sae_classifier_stacked_all_layer_active, sae_classifier_stacked_cur_layer_active = ps_net_SAE.train(train_parameters_SAE, device, phase=\"train\")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(ps_train_set, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_score_list_SAE = ps_net_SAE.eval(ps_train_set, device, phase=\"eval\", sparse_classifier=sparse_classifier)\n",
    "ps_score_list_SAE_all_stacked = ps_net_SAE.eval(ps_train_set, device, phase=\"eval\", \n",
    "                                                sparse_classifier=sae_classifier_stacked_all_layer_active)\n",
    "ps_score_list_SAE_cur_stacked = ps_net_SAE.eval(ps_train_set, device, phase=\"eval\", \n",
    "                                                sparse_classifier=sae_classifier_stacked_cur_layer_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Propensity_socre_network import Propensity_socre_network\n",
    "train_parameters_NN = {\n",
    "        \"epochs\": 100,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"shuffle\": True,\n",
    "        \"train_set\": ps_train_set,\n",
    "        \"model_save_path\": \"./Propensity_Model/NN_PS_model_iter_id_\" + str(1) + \"_epoch_{0}_lr_{1}.pth\"\n",
    "}\n",
    "ps_net_NN = Propensity_socre_network()\n",
    "print(\"############### Propensity Score neural net Training ###############\")\n",
    "ps_net_NN.train(train_parameters_NN, device, phase=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval propensity network using NN\n",
    "eval_parameters_NN = {\n",
    "        \"eval_set\": ps_train_set,\n",
    "        \"model_path\": \"./Propensity_Model/NN_PS_model_iter_id_{0}_epoch_100_lr_0.001.pth\".format(1)\n",
    "}\n",
    "\n",
    "ps_score_list_NN = ps_net_NN.eval(eval_parameters_NN, device, phase=\"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps_score_list_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DCN_network import DCN_network\n",
    "\n",
    "def train_DCN(data_loader_dict, model_path, dL, device):\n",
    "    treated_group = data_loader_dict[\"treated_data\"]\n",
    "    np_treated_df_X = treated_group[0]\n",
    "    np_treated_ps_score = treated_group[1]\n",
    "    np_treated_df_Y_f = treated_group[2]\n",
    "    np_treated_df_Y_cf = treated_group[3]\n",
    "    tensor_treated = dL.convert_to_tensor_DCN(np_treated_df_X, np_treated_ps_score,\n",
    "                                              np_treated_df_Y_f, np_treated_df_Y_cf)\n",
    "\n",
    "    control_group = data_loader_dict[\"control_data\"]\n",
    "    np_control_df_X = control_group[0]\n",
    "    np_control_ps_score = control_group[1]\n",
    "    np_control_df_Y_f = control_group[2]\n",
    "    np_control_df_Y_cf = control_group[3]\n",
    "    tensor_control = dL.convert_to_tensor_DCN(np_control_df_X, np_control_ps_score,\n",
    "                                              np_control_df_Y_f, np_control_df_Y_cf)\n",
    "\n",
    "    DCN_train_parameters = {\n",
    "        \"epochs\": 100,\n",
    "        \"lr\": 0.001,\n",
    "        \"treated_batch_size\": 1,\n",
    "        \"control_batch_size\": 1,\n",
    "        \"shuffle\": True,\n",
    "        \"treated_set\": tensor_treated,\n",
    "        \"control_set\": tensor_control,\n",
    "        \"model_save_path\": model_path\n",
    "    }\n",
    "\n",
    "    # train DCN network\n",
    "    dcn = DCN_network()\n",
    "    dcn.train(DCN_train_parameters, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############### DCN Training using NN ###############\")\n",
    "data_loader_dict_NN = dL.prepare_tensor_for_DCN(np_covariates_X_train,\n",
    "                                                    np_covariates_Y_train,\n",
    "                                                    ps_score_list_NN)\n",
    "model_path = \"./DCNModel/NN_DCN_model_iter_id_\" + str(1) + \"_epoch_{0}_lr_{1}.pth\"\n",
    "train_DCN(data_loader_dict_NN, model_path, dL, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############### DCN Training using Sparse ###############\")\n",
    "data_loader_dict_SAE = dL.prepare_tensor_for_DCN(np_covariates_X_train,\n",
    "                                                     np_covariates_Y_train,\n",
    "                                                     ps_score_list_SAE)\n",
    "\n",
    "data_loader_dict_SAE_all_stacked = dL.prepare_tensor_for_DCN(np_covariates_X_train,\n",
    "                                                     np_covariates_Y_train,\n",
    "                                                     ps_score_list_SAE_all_stacked)\n",
    "\n",
    "data_loader_dict_SAE_cur_stacked = dL.prepare_tensor_for_DCN(np_covariates_X_train,\n",
    "                                                     np_covariates_Y_train,\n",
    "                                                     ps_score_list_SAE_cur_stacked)\n",
    "print(\"e2e\")\n",
    "model_path = \"./DCNModel/SAE_DCN_model_iter_id_\" + str(1) + \"_epoch_{0}_lr_{1}.pth\"\n",
    "train_DCN(data_loader_dict_SAE, model_path, dL, device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all stacked\")\n",
    "model_path = \"./DCNModel/SAE_all_stacked_DCN_model_iter_id_\" + str(1) + \"_epoch_{0}_lr_{1}.pth\"\n",
    "train_DCN(data_loader_dict_SAE_all_stacked, model_path, dL, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cur stacked\")\n",
    "model_path = \"./DCNModel/SAE_cur_stacked_DCN_model_iter_id_\" + str(1) + \"_epoch_{0}_lr_{1}.pth\"\n",
    "train_DCN(data_loader_dict_SAE_cur_stacked, model_path, dL, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_test_set = dL.convert_to_tensor(np_covariates_X_test, np_covariates_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_parameters_sparse_test = {\n",
    "        \"eval_set\": ps_test_set\n",
    "}\n",
    "\n",
    "ps_score_list_sparse = ps_net_SAE.eval(ps_test_set, device, phase=\"eval\", sparse_classifier=sparse_classifier)\n",
    "ps_score_list_sparse_all_stacked = ps_net_SAE.eval(ps_test_set, device, phase=\"eval\", \n",
    "                                       sparse_classifier=sae_classifier_stacked_all_layer_active)\n",
    "ps_score_list_sparse_cur_stacked = ps_net_SAE.eval(ps_test_set, device, phase=\"eval\", \n",
    "                                       sparse_classifier=sae_classifier_stacked_cur_layer_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval propensity network using NN\n",
    "eval_parameters_NN = {\n",
    "        \"eval_set\": ps_test_set,\n",
    "        \"model_path\": \"./Propensity_Model/NN_PS_model_iter_id_{0}_epoch_100_lr_0.001.pth\".format(1)\n",
    "}\n",
    "\n",
    "ps_score_list_NN = ps_net_NN.eval(eval_parameters_NN, device, phase=\"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_test_DCN(data_loader_dict, dL, device, model_path):\n",
    "    treated_group = data_loader_dict[\"treated_data\"]\n",
    "    np_treated_df_X = treated_group[0]\n",
    "    np_treated_ps_score = treated_group[1]\n",
    "    np_treated_df_Y_f = treated_group[2]\n",
    "    np_treated_df_Y_cf = treated_group[3]\n",
    "    tensor_treated = dL.convert_to_tensor_DCN(np_treated_df_X, np_treated_ps_score,\n",
    "                                              np_treated_df_Y_f, np_treated_df_Y_cf)\n",
    "\n",
    "    control_group = data_loader_dict[\"control_data\"]\n",
    "    np_control_df_X = control_group[0]\n",
    "    np_control_ps_score = control_group[1]\n",
    "    np_control_df_Y_f = control_group[2]\n",
    "    np_control_df_Y_cf = control_group[3]\n",
    "    tensor_control = dL.convert_to_tensor_DCN(np_control_df_X, np_control_ps_score,\n",
    "                                              np_control_df_Y_f, np_control_df_Y_cf)\n",
    "\n",
    "    DCN_test_parameters = {\n",
    "        \"treated_set\": tensor_treated,\n",
    "        \"control_set\": tensor_control,\n",
    "        \"model_save_path\": model_path\n",
    "    }\n",
    "\n",
    "    dcn = DCN_network()\n",
    "    err_dict = dcn.eval(DCN_test_parameters, device)\n",
    "    err_treated = [ele ** 2 for ele in err_dict[\"treated_err\"]]\n",
    "    err_control = [ele ** 2 for ele in err_dict[\"control_err\"]]\n",
    "\n",
    "    total_sum = sum(err_treated) + sum(err_control)\n",
    "    total_item = len(err_treated) + len(err_control)\n",
    "    MSE = total_sum / total_item\n",
    "    print(\"MSE: {0}\".format(MSE))\n",
    "    max_treated = max(err_treated)\n",
    "    max_control = max(err_control)\n",
    "    max_total = max(max_treated, max_control)\n",
    "\n",
    "    min_treated = min(err_treated)\n",
    "    min_control = min(err_control)\n",
    "    min_total = min(min_treated, min_control)\n",
    "\n",
    "    print(\"Max: {0}, Min: {1}\".format(max_total, min_total))\n",
    "    return MSE\n",
    "    # np.save(\"treated_err.npy\", err_treated)\n",
    "    # np.save(\"control_err.npy\", err_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############### DCN Testing using NN ###############\")\n",
    "data_loader_dict_NN = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                    np_covariates_Y_test,\n",
    "                                                    ps_score_list_NN)\n",
    "model_path = \"./DCNModel/NN_DCN_model_iter_id_{0}_epoch_100_lr_0.001.pth\".format(1)\n",
    "MSE_NN = do_test_DCN(data_loader_dict_NN, dL, device, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############### DCN Testing using SAE e2e ###############\")\n",
    "data_loader_dict_SAE = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                     np_covariates_Y_test,\n",
    "                                                     ps_score_list_sparse)\n",
    "model_path = \"./DCNModel/SAE_DCN_model_iter_id_{0}_epoch_100_lr_0.001.pth\".format(1)\n",
    "MSE_SAE = do_test_DCN(data_loader_dict_SAE, dL, device, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############### DCN Testing using SAE all stacked ###############\")\n",
    "data_loader_dict_SAE = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                     np_covariates_Y_test,\n",
    "                                                     ps_score_list_sparse_all_stacked)\n",
    "model_path = \"./DCNModel/SAE_all_stacked_DCN_model_iter_id_{0}_epoch_100_lr_0.001.pth\".format(1)\n",
    "MSE_SAE = do_test_DCN(data_loader_dict_SAE, dL, device, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############### DCN Testing using SAE cur stacked ###############\")\n",
    "data_loader_dict_SAE = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                     np_covariates_Y_test,\n",
    "                                                     ps_score_list_sparse_cur_stacked)\n",
    "model_path = \"./DCNModel/SAE_cur_stacked_DCN_model_iter_id_{0}_epoch_100_lr_0.001.pth\".format(1)\n",
    "MSE_SAE = do_test_DCN(data_loader_dict_SAE, dL, device, model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
