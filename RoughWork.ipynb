{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from Utils import Utils\n",
    "from Constants import Constants\n",
    "from PS_Matching import PS_Matching\n",
    "from Propensity_socre_network import Propensity_socre_network\n",
    "from Utils import Utils\n",
    "\n",
    "from GAN import Generator, Discriminator\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from GAN_Manager import GAN_Manager\n",
    "from Utils import Utils\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from torch.autograd.variable import Variable\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from DCN import DCN\n",
    "\n",
    "class DCN_network_1:    \n",
    "    def train(self, train_parameters, device, train_mode=Constants.DCN_TRAIN_PD):\n",
    "        epochs = train_parameters[\"epochs\"]\n",
    "        treated_batch_size = train_parameters[\"treated_batch_size\"]\n",
    "        control_batch_size = train_parameters[\"control_batch_size\"]\n",
    "        lr = train_parameters[\"lr\"]\n",
    "        shuffle = train_parameters[\"shuffle\"]\n",
    "        model_save_path = train_parameters[\"model_save_path\"].format(epochs, lr)\n",
    "        treated_set_train = train_parameters[\"treated_set_train\"]\n",
    "        control_set_train = train_parameters[\"control_set_train\"]\n",
    "\n",
    "        input_nodes = train_parameters[\"input_nodes\"]\n",
    "\n",
    "        print(\"Saved model path: {0}\".format(model_save_path))\n",
    "\n",
    "        treated_data_loader_train = torch.utils.data.DataLoader(treated_set_train,\n",
    "                                                                batch_size=treated_batch_size,\n",
    "                                                                shuffle=shuffle,\n",
    "                                                                num_workers=1)\n",
    "\n",
    "        control_data_loader_train = torch.utils.data.DataLoader(control_set_train,\n",
    "                                                                batch_size=control_batch_size,\n",
    "                                                                shuffle=shuffle,\n",
    "                                                                num_workers=1)\n",
    "\n",
    "        network = DCN(training_mode=train_mode,\n",
    "                      input_nodes=input_nodes).to(device)\n",
    "        optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "        lossF = nn.MSELoss()\n",
    "        min_loss = 100000.0\n",
    "        dataset_loss = 0.0\n",
    "        print(\".. Training started ..\")\n",
    "        print(device)\n",
    "        for epoch in range(epochs):\n",
    "            network.train()\n",
    "            total_loss = 0\n",
    "            train_set_size = 0\n",
    "\n",
    "            if epoch % 2 == 0:\n",
    "                dataset_loss = 0\n",
    "                # train treated\n",
    "                network.hidden1_Y1.weight.requires_grad = True\n",
    "                network.hidden1_Y1.bias.requires_grad = True\n",
    "                network.hidden2_Y1.weight.requires_grad = True\n",
    "                network.hidden2_Y1.bias.requires_grad = True\n",
    "                network.out_Y1.weight.requires_grad = True\n",
    "                network.out_Y1.bias.requires_grad = True\n",
    "\n",
    "                network.hidden1_Y0.weight.requires_grad = False\n",
    "                network.hidden1_Y0.bias.requires_grad = False\n",
    "                network.hidden2_Y0.weight.requires_grad = False\n",
    "                network.hidden2_Y0.bias.requires_grad = False\n",
    "                network.out_Y0.weight.requires_grad = False\n",
    "                network.out_Y0.bias.requires_grad = False\n",
    "\n",
    "                for batch in treated_data_loader_train:\n",
    "                    covariates_X, ps_score, y_f, y_cf = batch\n",
    "                    covariates_X = covariates_X.to(device)\n",
    "                    ps_score = ps_score.squeeze().to(device)\n",
    "\n",
    "                    train_set_size += covariates_X.size(0)\n",
    "                    treatment_pred = network(covariates_X, ps_score)\n",
    "                    # treatment_pred[0] -> y1\n",
    "                    # treatment_pred[1] -> y0\n",
    "                    yf_predicted = treatment_pred[0]\n",
    "                    if torch.cuda.is_available():\n",
    "                        loss = lossF(yf_predicted.float().cuda(),\n",
    "                                     y_f.float().cuda()).to(device)\n",
    "                    else:\n",
    "                        loss = lossF(yf_predicted.float(),\n",
    "                                     y_f.float()).to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                dataset_loss = total_loss\n",
    "\n",
    "            elif epoch % 2 == 1:\n",
    "                # train controlled\n",
    "                network.hidden1_Y1.weight.requires_grad = False\n",
    "                network.hidden1_Y1.bias.requires_grad = False\n",
    "                network.hidden2_Y1.weight.requires_grad = False\n",
    "                network.hidden2_Y1.bias.requires_grad = False\n",
    "                network.out_Y1.weight.requires_grad = False\n",
    "                network.out_Y1.bias.requires_grad = False\n",
    "\n",
    "                network.hidden1_Y0.weight.requires_grad = True\n",
    "                network.hidden1_Y0.bias.requires_grad = True\n",
    "                network.hidden2_Y0.weight.requires_grad = True\n",
    "                network.hidden2_Y0.bias.requires_grad = True\n",
    "                network.out_Y0.weight.requires_grad = True\n",
    "                network.out_Y0.bias.requires_grad = True\n",
    "\n",
    "                for batch in control_data_loader_train:\n",
    "                    covariates_X, ps_score, y_f, y_cf = batch\n",
    "                    covariates_X = covariates_X.to(device)\n",
    "                    ps_score = ps_score.squeeze().to(device)\n",
    "\n",
    "                    train_set_size += covariates_X.size(0)\n",
    "                    treatment_pred = network(covariates_X, ps_score)\n",
    "                    # treatment_pred[0] -> y1\n",
    "                    # treatment_pred[1] -> y0\n",
    "                    yf_predicted = treatment_pred[1]\n",
    "                    if torch.cuda.is_available():\n",
    "                        loss = lossF(yf_predicted.float().cuda(),\n",
    "                                     y_f.float().cuda()).to(device)\n",
    "                    else:\n",
    "                        loss = lossF(yf_predicted.float(),\n",
    "                                     y_f.float()).to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                dataset_loss = dataset_loss + total_loss\n",
    "\n",
    "            if epoch % 10 == 9:\n",
    "                print(\"epoch: {0}, Treated + Control loss: {1}\".format(epoch, dataset_loss))\n",
    "        torch.save(network.state_dict(), model_save_path)\n",
    "\n",
    "    def eval(self, eval_parameters, device, input_nodes, train_mode):\n",
    "        treated_set = eval_parameters[\"treated_set\"]\n",
    "        control_set = eval_parameters[\"control_set\"]\n",
    "        model_path = eval_parameters[\"model_save_path\"]\n",
    "        network = DCN(training_mode=train_mode, input_nodes=input_nodes).to(device)\n",
    "        network.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        network.eval()\n",
    "        treated_data_loader = torch.utils.data.DataLoader(treated_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "        control_data_loader = torch.utils.data.DataLoader(control_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "\n",
    "        err_treated_list = []\n",
    "        err_control_list = []\n",
    "        true_ITE_list = []\n",
    "        predicted_ITE_list = []\n",
    "\n",
    "        ITE_dict_list = []\n",
    "\n",
    "        for batch in treated_data_loader:\n",
    "            covariates_X, ps_score, y_f, y_cf = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "            ps_score = ps_score.squeeze().to(device)\n",
    "            treatment_pred = network(covariates_X, ps_score)\n",
    "\n",
    "            predicted_ITE = treatment_pred[0] - treatment_pred[1]\n",
    "            true_ITE = y_f - y_cf\n",
    "            if torch.cuda.is_available():\n",
    "                diff = true_ITE.float().cuda() - predicted_ITE.float().cuda()\n",
    "            else:\n",
    "                diff = true_ITE.float() - predicted_ITE.float()\n",
    "\n",
    "            ITE_dict_list.append(self.create_ITE_Dict(covariates_X,\n",
    "                                                      ps_score.item(), y_f.item(),\n",
    "                                                      y_cf.item(),\n",
    "                                                      true_ITE.item(),\n",
    "                                                      predicted_ITE.item(),\n",
    "                                                      diff.item()))\n",
    "            err_treated_list.append(diff.item())\n",
    "            true_ITE_list.append(true_ITE.item())\n",
    "            predicted_ITE_list.append(predicted_ITE.item())\n",
    "\n",
    "        for batch in control_data_loader:\n",
    "            covariates_X, ps_score, y_f, y_cf = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "            ps_score = ps_score.squeeze().to(device)\n",
    "            treatment_pred = network(covariates_X, ps_score)\n",
    "\n",
    "            predicted_ITE = treatment_pred[0] - treatment_pred[1]\n",
    "            true_ITE = y_cf - y_f\n",
    "            if torch.cuda.is_available():\n",
    "                diff = true_ITE.float().cuda() - predicted_ITE.float().cuda()\n",
    "            else:\n",
    "                diff = true_ITE.float() - predicted_ITE.float()\n",
    "\n",
    "            ITE_dict_list.append(self.create_ITE_Dict(covariates_X,\n",
    "                                                      ps_score.item(), y_f.item(),\n",
    "                                                      y_cf.item(),\n",
    "                                                      true_ITE.item(),\n",
    "                                                      predicted_ITE.item(),\n",
    "                                                      diff.item()))\n",
    "            err_control_list.append(diff.item())\n",
    "            true_ITE_list.append(true_ITE.item())\n",
    "            predicted_ITE_list.append(predicted_ITE.item())\n",
    "\n",
    "        # print(err_treated_list)\n",
    "        # print(err_control_list)\n",
    "        return {\n",
    "            \"treated_err\": err_treated_list,\n",
    "            \"control_err\": err_control_list,\n",
    "            \"true_ITE\": true_ITE_list,\n",
    "            \"predicted_ITE\": predicted_ITE_list,\n",
    "            \"ITE_dict_list\": ITE_dict_list\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def eval_semi_supervised(eval_parameters, device, input_nodes, train_mode, treated_flag):\n",
    "        eval_set = eval_parameters[\"eval_set\"]\n",
    "        model_path = eval_parameters[\"model_save_path\"]\n",
    "        network = DCN(training_mode=train_mode, input_nodes=input_nodes).to(device)\n",
    "        network.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        network.eval()\n",
    "        treated_data_loader = torch.utils.data.DataLoader(eval_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "\n",
    "        y_f_list = []\n",
    "        y_cf_list = []\n",
    "\n",
    "        for batch in treated_data_loader:\n",
    "            covariates_X, ps_score = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "            ps_score = ps_score.squeeze().to(device)\n",
    "            treatment_pred = network(covariates_X, ps_score)\n",
    "            if treated_flag:\n",
    "                y_f_list.append(treatment_pred[0].item())\n",
    "                y_cf_list.append(treatment_pred[1].item())\n",
    "            else:\n",
    "                y_f_list.append(treatment_pred[1].item())\n",
    "                y_cf_list.append(treatment_pred[0].item())\n",
    "\n",
    "        return {\n",
    "            \"y_f_list\": np.array(y_f_list),\n",
    "            \"y_cf_list\": np.array(y_cf_list)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def create_ITE_Dict(covariates_X, ps_score, y_f, y_cf, true_ITE,\n",
    "                        predicted_ITE, diff):\n",
    "        result_dict = OrderedDict()\n",
    "        covariate_list = [element.item() for element in covariates_X.flatten()]\n",
    "        idx = 0\n",
    "        for item in covariate_list:\n",
    "            idx += 1\n",
    "            result_dict[\"X\" + str(idx)] = item\n",
    "\n",
    "        result_dict[\"ps_score\"] = ps_score\n",
    "        result_dict[\"factual\"] = y_f\n",
    "        result_dict[\"counter_factual\"] = y_cf\n",
    "        result_dict[\"true_ITE\"] = true_ITE\n",
    "        result_dict[\"predicted_ITE\"] = predicted_ITE\n",
    "        result_dict[\"diff\"] = diff\n",
    "\n",
    "        return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propensity_scores(ps_train_set, iter_id, input_nodes, device):\n",
    "    prop_score_NN_model_path = Constants.PROP_SCORE_NN_MODEL_PATH \\\n",
    "            .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR)\n",
    "\n",
    "    train_parameters_NN = {\n",
    "            \"epochs\": Constants.PROP_SCORE_NN_EPOCHS,\n",
    "            \"lr\": Constants.PROP_SCORE_NN_LR,\n",
    "            \"batch_size\": Constants.PROP_SCORE_NN_BATCH_SIZE,\n",
    "            \"shuffle\": True,\n",
    "            \"train_set\": ps_train_set,\n",
    "            \"model_save_path\": prop_score_NN_model_path,\n",
    "            \"input_nodes\": input_nodes\n",
    "    }\n",
    "\n",
    "        # ps using NN\n",
    "    ps_net_NN = Propensity_socre_network()\n",
    "    print(\"############### Propensity Score neural net Training ###############\")\n",
    "    ps_net_NN.train(train_parameters_NN, device, phase=\"train\")\n",
    "\n",
    "        # eval\n",
    "    eval_parameters_train_NN = {\n",
    "            \"eval_set\": ps_train_set,\n",
    "            \"model_path\": prop_score_NN_model_path,\n",
    "            \"input_nodes\": input_nodes\n",
    "    }\n",
    "\n",
    "    ps_score_list_train_NN = ps_net_NN.eval(eval_parameters_train_NN, device, phase=\"eval\")\n",
    "\n",
    "    return ps_score_list_train_NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from GAN_Manager import GAN_Manager\n",
    "from Utils import Utils\n",
    "\n",
    "def get_matched_and_unmatched_control_indices(ps_treated, ps_control):\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(ps_control)\n",
    "    distance, matched_control = nn.kneighbors(ps_treated)\n",
    "    matched_control_indices = np.array(matched_control).ravel()\n",
    "\n",
    "        # # remove duplicates\n",
    "    #matched_control_indices = list(dict.fromkeys(matched_control_indices))\n",
    "    set_matched_control_indices = set(matched_control_indices)\n",
    "    total_indices = list(range(len(ps_control)))\n",
    "    unmatched_control_indices = list(filter(lambda x: x not in set_matched_control_indices,\n",
    "                                                total_indices))\n",
    "\n",
    "    return matched_control_indices, unmatched_control_indices\n",
    "\n",
    "def filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                              np_control_df_Y_f,\n",
    "                              np_control_df_Y_cf, indices):\n",
    "    np_filter_control_df_X = np.take(np_control_df_X, indices, axis=0)\n",
    "    np_filter_control_ps_score = np.take(np_control_ps_score, indices, axis=0)\n",
    "    np_filter_control_df_Y_f = np.take(np_control_df_Y_f, indices, axis=0)\n",
    "    np_filter_control_df_Y_cf = np.take(np_control_df_Y_cf, indices, axis=0)\n",
    "    tuple_matched_control = (np_filter_control_df_X, np_filter_control_ps_score,\n",
    "                                 np_filter_control_df_Y_f, np_filter_control_df_Y_cf)\n",
    "\n",
    "    return tuple_matched_control\n",
    "\n",
    "def filter_matched_and_unmatched_control_samples(np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f,\n",
    "                                                     np_control_df_Y_cf, matched_control_indices,\n",
    "                                                     unmatched_control_indices):\n",
    "    tuple_matched_control = filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                                                           np_control_df_Y_f,\n",
    "                                                           np_control_df_Y_cf,\n",
    "                                                           matched_control_indices)\n",
    "\n",
    "    tuple_unmatched_control = filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                                                             np_control_df_Y_f,\n",
    "                                                             np_control_df_Y_cf,\n",
    "                                                             unmatched_control_indices)\n",
    "\n",
    "    return tuple_matched_control, tuple_unmatched_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_id = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Propensity Score neural net Training ###############\n",
      ".. Training started ..\n",
      "Saved model path: ./Propensity_Model/NN_PS_model_iter_id_10_epoch_50_lr_0.001.pth\n",
      "Epoch: 25, loss: 6.840774089097977, correct: 492/597, accuracy: 0.8241206030150754\n",
      "Epoch: 50, loss: 4.242011800408363, correct: 538/597, accuracy: 0.9011725293132329\n",
      "Saved model..\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"Dataset/ihdp_sample.csv\"\n",
    "from dataloader import DataLoader\n",
    "split_size = 0.8\n",
    "dL = DataLoader()\n",
    "\n",
    "input_nodes = 25\n",
    "device = Utils.get_device()\n",
    "\n",
    "np_covariates_X_train, np_covariates_X_test, np_covariates_Y_train, np_covariates_Y_test \\\n",
    "                = dL.preprocess_data_from_csv(csv_path, split_size)\n",
    "\n",
    "ps_train_set = dL.convert_to_tensor(np_covariates_X_train, np_covariates_Y_train)\n",
    "ps_score_list_train_NN = get_propensity_scores(ps_train_set, iter_id, input_nodes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Treated Statistics ==>\n",
      "(112, 25)\n",
      " Control Statistics ==>\n",
      "(485, 25)\n"
     ]
    }
   ],
   "source": [
    "data_loader_dict_train = dL.prepare_tensor_for_DCN(np_covariates_X_train,\n",
    "                                                              np_covariates_Y_train,\n",
    "                                                              ps_score_list_train_NN,\n",
    "                                                              False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Control: 112\n",
      "Unmatched Control: 447\n",
      "Matched Control: (112, 25)\n",
      "Matched Treated: (112, 25)\n"
     ]
    }
   ],
   "source": [
    "tuple_treated = data_loader_dict_train[\"treated_data\"]\n",
    "tuple_control = data_loader_dict_train[\"control_data\"]\n",
    "np_treated_df_X, np_treated_ps_score, np_treated_df_Y_f, np_treated_df_Y_cf = tuple_treated\n",
    "np_control_df_X, np_control_ps_score, np_control_df_Y_f, np_control_df_Y_cf = tuple_control\n",
    "\n",
    "        # get unmatched controls\n",
    "matched_control_indices, unmatched_control_indices = get_matched_and_unmatched_control_indices(\n",
    "Utils.convert_to_col_vector(np_treated_ps_score),\n",
    "Utils.convert_to_col_vector(np_control_ps_score))\n",
    "\n",
    "print(\"Matched Control: {0}\".format(len(matched_control_indices)))\n",
    "print(\"Unmatched Control: {0}\".format(len(unmatched_control_indices)))\n",
    "\n",
    "tuple_matched_control, tuple_unmatched_control = filter_matched_and_unmatched_control_samples(\n",
    "            np_control_df_X, np_control_ps_score,\n",
    "            np_control_df_Y_f,\n",
    "            np_control_df_Y_cf, matched_control_indices,\n",
    "            unmatched_control_indices)\n",
    "\n",
    "        # generate matched treated for unmatched controls using variable\n",
    "        # tuple_unmatched_control\n",
    "        # create GAN code here\n",
    "print(\"Matched Control: {0}\".format(tuple_matched_control[0].shape))\n",
    "print(\"Matched Treated: {0}\".format(tuple_treated[0].shape))\n",
    "tensor_treated = \\\n",
    "            Utils.create_tensors_to_train_DCN(tuple_treated, dL)\n",
    "\n",
    "        # need to change for unmatched\n",
    "tensor_matched_control = \\\n",
    "            Utils.create_tensors_to_train_DCN(tuple_matched_control, dL)\n",
    "\n",
    "tensor_unmatched_control = \\\n",
    "            Utils.create_tensors_to_train_DCN(tuple_unmatched_control, dL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw(treated_ps_list, control_ps_list, bins1):\n",
    "    pyplot.hist(treated_ps_list, bins1, alpha=0.5, label='treated')\n",
    "    pyplot.hist(control_ps_list, bins1, alpha=0.5, label='control')\n",
    "\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 112\n"
     ]
    }
   ],
   "source": [
    "ps_matched_control_list = tuple_matched_control[1].tolist()\n",
    "ps_un_matched_control_list = tuple_unmatched_control[1].tolist()\n",
    "ps_treated_list = tuple_treated[1].tolist()\n",
    "\n",
    "print(len(ps_matched_control_list), len(ps_treated_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins1 = np.linspace(0, 1, 100)\n",
    "bins2 = np.linspace(0, 0.2, 100)\n",
    "bins3 = np.linspace(0.2, 0.5, 100)\n",
    "bins4 = np.linspace(0.5, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matched control and treated\n",
    "draw(ps_treated_list, ps_matched_control_list, bins1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unmatched control and treated\n",
    "draw(ps_treated_list, ps_un_matched_control_list, bins1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_Module:\n",
    "    def __init__(self, discriminator_in_nodes, generator_out_nodes, device):\n",
    "        self.discriminator = Discriminator(in_nodes=discriminator_in_nodes).to(device)\n",
    "        self.discriminator.apply(self.weights_init)\n",
    "\n",
    "        self.generator = Generator(out_nodes=generator_out_nodes).to(device)\n",
    "        self.generator.apply(self.weights_init)\n",
    "\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def get_generator(self):\n",
    "        return self.generator\n",
    "\n",
    "    def __cal_propensity_loss(self, ps_score_control, prop_score_NN_model_path, gen_treated, device):\n",
    "        # Assign treated\n",
    "        Y = np.ones(gen_treated.size(0))\n",
    "        eval_set = Utils.convert_to_tensor(gen_treated.detach().numpy(), Y)\n",
    "\n",
    "        eval_parameters_ps_net = {\n",
    "            \"eval_set\": eval_set,\n",
    "            \"model_path\": prop_score_NN_model_path,\n",
    "            \"input_nodes\": 25\n",
    "        }\n",
    "        ps_net_NN = Propensity_socre_network()\n",
    "        ps_score_list_treated = ps_net_NN.eval(eval_parameters_ps_net, device,\n",
    "                                               phase=\"eval\",\n",
    "                                               eval_from_GAN=True)\n",
    "\n",
    "        Tensor = torch.cuda.DoubleTensor if torch.cuda.is_available() else torch.DoubleTensor\n",
    "        ps_score_treated = Tensor(ps_score_list_treated)\n",
    "\n",
    "        prop_loss = torch.sum((torch.sub(ps_score_treated, ps_score_control)) ** 2)\n",
    "        return prop_loss\n",
    "\n",
    "    def noise(self, _size):\n",
    "        n = Variable(torch.normal(mean=0, std=1, size=(_size, 25)))\n",
    "        # print(n.size())\n",
    "        if torch.cuda.is_available(): return n.cuda()\n",
    "        return n\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def real_data_target(self, size):\n",
    "        '''\n",
    "        Tensor containing ones, with shape = size\n",
    "        '''\n",
    "        data = Variable(torch.ones(size, 1))\n",
    "        if torch.cuda.is_available(): return data.cuda()\n",
    "        return data\n",
    "\n",
    "    def fake_data_target(self, size):\n",
    "        '''\n",
    "        Tensor containing zeros, with shape = size\n",
    "        '''\n",
    "        data = Variable(torch.zeros(size, 1))\n",
    "        if torch.cuda.is_available(): return data.cuda()\n",
    "        return data\n",
    "\n",
    "    def train_discriminator(self, optimizer, real_data, fake_data):\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1.1 Train on Real Data\n",
    "        prediction_real = self.discriminator(real_data)\n",
    "        real_score = torch.mean(prediction_real).item()\n",
    "        \n",
    "        # Calculate error and backpropagate\n",
    "        error_real = self.loss(prediction_real, self.real_data_target(real_data.size(0)))\n",
    "        error_real.backward()\n",
    "\n",
    "        # 1.2 Train on Fake Data\n",
    "        prediction_fake = self.discriminator(fake_data)\n",
    "        fake_score = torch.mean(prediction_fake).item()\n",
    "        # Calculate error and backpropagate\n",
    "        error_fake = self.loss(prediction_fake, self.fake_data_target(real_data.size(0)))\n",
    "        error_fake.backward()\n",
    "\n",
    "        # 1.3 Update weights with gradients\n",
    "        optimizer.step()\n",
    "        loss_D = error_real + error_fake\n",
    "        # Return error\n",
    "        return loss_D.item(), real_score, fake_score\n",
    "\n",
    "    def train_generator(self, optimizer, fake_data, ALPHA, ps_score_control,\n",
    "                        prop_score_NN_model_path, device):\n",
    "        # 2. Train Generator\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Sample noise and generate fake data\n",
    "        predicted_D = self.discriminator(fake_data)\n",
    "        # Calculate error and back propagate\n",
    "        error_g = self.loss(predicted_D, self.real_data_target(predicted_D.size(0)))\n",
    "        prop_loss = self.__cal_propensity_loss(ps_score_control, prop_score_NN_model_path,\n",
    "                                               fake_data, device)\n",
    "        error = error_g + (ALPHA * prop_loss)\n",
    "        error.backward()\n",
    "        # Update weights with gradients\n",
    "        optimizer.step()\n",
    "        # Return error\n",
    "        return error_g.item(), prop_loss.item()\n",
    "\n",
    "    def train_GAN(self, train_parameters, device):\n",
    "        epochs = train_parameters[\"epochs\"]\n",
    "        train_set = train_parameters[\"train_set\"]\n",
    "        lr = train_parameters[\"lr\"]\n",
    "        shuffle = train_parameters[\"shuffle\"]\n",
    "        batch_size = train_parameters[\"batch_size\"]\n",
    "        prop_score_NN_model_path = train_parameters[\"prop_score_NN_model_path\"]\n",
    "        ALPHA = train_parameters[\"ALPHA\"]\n",
    "\n",
    "        data_loader_train = torch.utils.data.DataLoader(train_set,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=shuffle,\n",
    "                                                        num_workers=1)\n",
    "\n",
    "        #         generator = Generator(out_nodes=generator_out_nodes).to(device)\n",
    "        #         discriminator = Discriminator(in_nodes=discriminator_in_nodes).to(device)\n",
    "\n",
    "        g_optimizer = optim.Adam(self.generator.parameters(), lr=lr)\n",
    "        d_optimizer = optim.Adam(self.discriminator.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch += 1\n",
    "#             self.generator.train()\n",
    "#             self.discriminator.train()\n",
    "            \n",
    "            total_G_loss = 0\n",
    "            total_D_loss = 0\n",
    "            total_prop_loss = 0\n",
    "            total_d_pred_real = 0\n",
    "            total_d_pred_fake = 0\n",
    "\n",
    "            for batch in data_loader_train:\n",
    "                covariates_X_control, ps_score_control, y_f, y_cf = batch\n",
    "                covariates_X_control = covariates_X_control.to(device)\n",
    "                covariates_X_control_size = covariates_X_control.size(0)\n",
    "                ps_score_control = ps_score_control.squeeze().to(device)\n",
    "\n",
    "                # 1. Train Discriminator\n",
    "                real_data = covariates_X_control\n",
    "                # Generate fake data\n",
    "                fake_data = self.generator(self.noise(covariates_X_control_size)).detach()\n",
    "                # Train D\n",
    "                d_error, d_pred_real, d_pred_fake = self.train_discriminator(d_optimizer,\n",
    "                                                                             real_data, fake_data)\n",
    "                total_D_loss += d_error\n",
    "                total_d_pred_real += d_pred_real\n",
    "                total_d_pred_fake += d_pred_fake\n",
    "\n",
    "                # 2. Train Generator\n",
    "                # Generate fake data\n",
    "                fake_data = self.generator(self.noise(covariates_X_control_size))\n",
    "                # Train G\n",
    "                error_g, prop_loss = self.train_generator(g_optimizer, fake_data, ALPHA, ps_score_control,\n",
    "                                                          prop_score_NN_model_path, device)\n",
    "                total_G_loss += error_g\n",
    "                total_prop_loss += prop_loss\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(\"Epoch: {0}, D_loss: {1}, D_score_real: {2}, D_score_Fake: {3}, G_loss: {4}, \"\n",
    "                      \"Prop_loss: {5}\"\n",
    "                      .format(epoch,\n",
    "                              total_D_loss, total_d_pred_real, total_d_pred_fake, total_G_loss, total_prop_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000, D_loss: 8.04822313785553, D_score_real: 4.229069471359253, D_score_Fake: 2.4143779575824738, G_loss: 8.230191707611084, Prop_loss: 30.502287797106\n",
      "Epoch: 2000, D_loss: 5.547657370567322, D_score_real: 5.098188638687134, D_score_Fake: 1.7744233310222626, G_loss: 10.193827986717224, Prop_loss: 10.755319167336605\n",
      "Epoch: 3000, D_loss: 5.450867235660553, D_score_real: 5.196991264820099, D_score_Fake: 1.6319672614336014, G_loss: 10.946046233177185, Prop_loss: 13.03755600979926\n",
      "Epoch: 4000, D_loss: 4.557817459106445, D_score_real: 5.577185869216919, D_score_Fake: 1.518505498766899, G_loss: 10.830700993537903, Prop_loss: 5.46445472645753\n",
      "Epoch: 5000, D_loss: 4.430369734764099, D_score_real: 5.595266401767731, D_score_Fake: 1.4090280383825302, G_loss: 11.39314091205597, Prop_loss: 5.396853944667522\n",
      "Epoch: 6000, D_loss: 4.02604877948761, D_score_real: 5.735526740550995, D_score_Fake: 1.3572924137115479, G_loss: 11.903408408164978, Prop_loss: 5.337163867409228\n",
      "Epoch: 7000, D_loss: 4.7468820214271545, D_score_real: 5.625899493694305, D_score_Fake: 1.3329158425331116, G_loss: 12.974462509155273, Prop_loss: 23.165284181604846\n",
      "Epoch: 8000, D_loss: 3.987400323152542, D_score_real: 5.836249887943268, D_score_Fake: 1.3075575828552246, G_loss: 12.454379081726074, Prop_loss: 6.202034992059127\n",
      "Epoch: 9000, D_loss: 3.753626763820648, D_score_real: 5.803851783275604, D_score_Fake: 1.176290437579155, G_loss: 13.074147939682007, Prop_loss: 11.774580670248724\n",
      "Epoch: 10000, D_loss: 3.9814010858535767, D_score_real: 5.749917805194855, D_score_Fake: 1.1951017826795578, G_loss: 13.03286612033844, Prop_loss: 14.417415182516464\n"
     ]
    }
   ],
   "source": [
    "# GAN Part from here\n",
    "prop_score_NN_model_path = Constants.PROP_SCORE_NN_MODEL_PATH \\\n",
    "            .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR)\n",
    "gan = GAN_Module(discriminator_in_nodes=25, generator_out_nodes=25, device=device)\n",
    "GAN_train_parameters = {\n",
    "            \"epochs\": 10000,\n",
    "            \"lr\": 0.0002,\n",
    "            \"shuffle\": True,\n",
    "            \"train_set\": tensor_unmatched_control,\n",
    "            \"batch_size\": 64,\n",
    "            \"prop_score_NN_model_path\": prop_score_NN_model_path,\n",
    "            \"ALPHA\": 1.2\n",
    "}\n",
    "\n",
    "gan.train_GAN(GAN_train_parameters, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(_size):\n",
    "        n = Variable(torch.normal(mean=0, std=1, size=(_size, 25)))\n",
    "        # print(n.size())\n",
    "        if torch.cuda.is_available(): return n.cuda()\n",
    "        return n\n",
    "    \n",
    "def eval_GAN(eval_size, generator, device):\n",
    "    treated_g = generator(noise(eval_size))\n",
    "    return treated_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_size: 447\n",
      "Treated G size\n",
      "torch.Size([447, 25])\n"
     ]
    }
   ],
   "source": [
    "eval_size = len(unmatched_control_indices)\n",
    "gen_net = gan.get_generator()\n",
    "treated_generated = eval_GAN(eval_size, gen_net, device)\n",
    "print(\"eval_size: \" + str(eval_size))\n",
    "print(\"Treated G size\")\n",
    "print(treated_generated.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.ones(treated_generated.size(0))\n",
    "eval_set = Utils.convert_to_tensor(treated_generated.detach().numpy(), Y)\n",
    "eval_parameters_ps_net = {\n",
    "            \"eval_set\": eval_set,\n",
    "            \"model_path\": prop_score_NN_model_path,\n",
    "            \"input_nodes\": 25\n",
    "}\n",
    "ps_net_NN = Propensity_socre_network()\n",
    "ps_score_list_treated = ps_net_NN.eval(eval_parameters_ps_net, device,\n",
    "                                        phase=\"eval\", eval_from_GAN=True)\n",
    "\n",
    "len(ps_score_list_treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# treated by GAN vs unmactched control\n",
    "draw(ps_score_list_treated, ps_un_matched_control_list, bins1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_DCN_train(tensor_treated_train, tensor_control_train, model_path, iter_id,\n",
    "                            input_nodes, device, train_mode):\n",
    "    DCN_train_parameters = {\n",
    "                \"epochs\": epochs,\n",
    "                \"lr\": lr,\n",
    "                \"treated_batch_size\": 1,\n",
    "                \"control_batch_size\": 1,\n",
    "                \"shuffle\": True,\n",
    "                \"treated_set_train\": tensor_treated_train,\n",
    "                \"control_set_train\": tensor_control_train,\n",
    "                \"model_save_path\": model_path.format(iter_id,\n",
    "                                                     epochs,\n",
    "                                                     lr),\n",
    "                \"input_nodes\": input_nodes\n",
    "    }\n",
    "\n",
    "            # train DCN network\n",
    "    dcn = DCN_network_1()\n",
    "    dcn.train(DCN_train_parameters, device, train_mode=train_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DCN semi supervised training using PS Matching No PD###\n",
      "Saved model path: ./DCNModel/NN_DCN_SEMI_SUPERVISED_NO_DROPOUT_PM_MATCH_FALSE_model_iter_id_10_epoch_100_lr_0.001.pth\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 9, Treated + Control loss: 116.94579231196622\n",
      "epoch: 19, Treated + Control loss: 35.21253450181757\n",
      "epoch: 29, Treated + Control loss: 59.59191041842729\n",
      "epoch: 39, Treated + Control loss: 24.2423763605085\n",
      "epoch: 49, Treated + Control loss: 18.771584004244062\n",
      "epoch: 59, Treated + Control loss: 4.259756335868133\n",
      "epoch: 69, Treated + Control loss: 73.15056561124649\n",
      "epoch: 79, Treated + Control loss: 3.4450513626997576\n",
      "epoch: 89, Treated + Control loss: 22.161050639135453\n",
      "epoch: 99, Treated + Control loss: 75.35371143297266\n"
     ]
    }
   ],
   "source": [
    "print(\"### DCN semi supervised training using PS Matching No PD###\")\n",
    "\n",
    "\n",
    "\n",
    "model_path_no_dropout = \"./DCNModel/NN_DCN_SEMI_SUPERVISED_NO_DROPOUT_PM_MATCH_FALSE_model_iter_id_{0}_epoch_{1}_lr_{2}.pth\"\n",
    "train_mode = Constants.DCN_TRAIN_NO_DROPOUT\n",
    "execute_DCN_train(tensor_treated, \n",
    "                  tensor_matched_control, \n",
    "                  model_path_no_dropout, iter_id,\n",
    "                input_nodes, device, train_mode)\n",
    "\n",
    "# t_treated = Utils.create_tensors_to_train_DCN(data_loader_dict_train[\"treated_data\"], dL)\n",
    "# t_control =  Utils.create_tensors_to_train_DCN(data_loader_dict_train[\"control_data\"], dL)\n",
    "\n",
    "\n",
    "# train_mode = Constants.DCN_TRAIN_PD\n",
    "# execute_DCN_train(t_treated, \n",
    "#                   t_control, \n",
    "#                   model_path_no_dropout, \n",
    "#                   iter_id,\n",
    "#                   input_nodes, \n",
    "#                   device, \n",
    "#                   train_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to be added in the code\n",
    "def convert_to_tensor_DCN_PS(tensor_x, ps_score):\n",
    "    tensor_ps_score = torch.from_numpy(ps_score)\n",
    "    # tensor_x = torch.stack([torch.Tensor(i) for i in X])\n",
    "    processed_dataset = torch.utils.data.TensorDataset(tensor_x, tensor_ps_score)\n",
    "    return processed_dataset\n",
    "\n",
    "def eval_semi_supervised(eval_parameters, device, input_nodes, train_mode, treated_flag):\n",
    "    eval_set = eval_parameters[\"eval_set\"]\n",
    "    model_path = eval_parameters[\"model_save_path\"]\n",
    "    network = DCN(training_mode=train_mode, input_nodes=input_nodes).to(device)\n",
    "    network.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    network.eval()\n",
    "    treated_data_loader = torch.utils.data.DataLoader(eval_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "    \n",
    "    y_f_list = []\n",
    "    y_cf_list = []\n",
    "\n",
    "    for batch in treated_data_loader:\n",
    "        covariates_X, ps_score = batch\n",
    "        covariates_X = covariates_X.to(device)\n",
    "        ps_score = ps_score.squeeze().to(device)\n",
    "        treatment_pred = network(covariates_X, ps_score)\n",
    "        if treated_flag:\n",
    "            y_f_list.append(treatment_pred[0].item())\n",
    "            y_cf_list.append(treatment_pred[1].item())\n",
    "        else:\n",
    "            y_f_list.append(treatment_pred[1].item())\n",
    "            y_cf_list.append(treatment_pred[0].item())\n",
    "        # print(err_treated_list)\n",
    "        # print(err_control_list)\n",
    "    return {\n",
    "            \"y_f_list\": np.array(y_f_list),\n",
    "            \"y_cf_list\": np.array(y_cf_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_score_list_treated_np = np.array(ps_score_list_treated)\n",
    "eval_set = convert_to_tensor_DCN_PS(treated_generated.detach(), ps_score_list_treated_np)\n",
    "\n",
    "DCN_test_parameters = {\n",
    "            \"eval_set\": eval_set,\n",
    "            \"model_save_path\": model_path_no_dropout.format(iter_id,\n",
    "                    epochs,\n",
    "                    lr)\n",
    "}\n",
    "treated_gen_y = eval_semi_supervised(DCN_test_parameters, device, input_nodes,\n",
    "                                 Constants.DCN_EVALUATION, treated_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447, 1)\n",
      "(112, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(559, 25)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_treated_generated = treated_generated.detach().numpy()\n",
    "np_ps_score_list_gen_treated = ps_score_list_treated_np\n",
    "np_treated_gen_f = Utils.convert_to_col_vector(treated_gen_y[\"y_f_list\"])\n",
    "np_treated_gen_cf = Utils.convert_to_col_vector(treated_gen_y[\"y_cf_list\"])\n",
    "\n",
    "print(np_treated_gen_f.shape)\n",
    "\n",
    "np_original_X = tuple_treated[0]\n",
    "np_original_ps_score = tuple_treated[1]\n",
    "np_original_Y_f = tuple_treated[2]\n",
    "np_original_Y_cf = tuple_treated[3]\n",
    "\n",
    "print(np_original_Y_f.shape)\n",
    "\n",
    "np_treated_x = np.concatenate((np_treated_generated, np_original_X), axis=0)\n",
    "np_treated_ps = np.concatenate((np_ps_score_list_gen_treated, np_original_ps_score), axis=0)\n",
    "np_treated_f = np.concatenate((np_treated_gen_f, np_original_Y_f), axis=0)\n",
    "np_treated_cf = np.concatenate((np_treated_gen_cf, np_original_Y_cf), axis=0)\n",
    "\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_x, np_treated_ps,\n",
    "                                            np_treated_f, np_treated_cf)\n",
    "\n",
    "np_treated_x.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447, 1)\n",
      "(112, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(559, 25)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_control_unmatched_X = tuple_unmatched_control[0]\n",
    "np_ps_score_list_control_unmatched = tuple_unmatched_control[1]\n",
    "np_control_unmatched_f = tuple_unmatched_control[2]\n",
    "np_control_unmatched_cf = tuple_unmatched_control[3]\n",
    "\n",
    "np_control_matched_X = tuple_matched_control[0]\n",
    "np_ps_score_list_control_matched = tuple_matched_control[1]\n",
    "np_control_matched_f = tuple_matched_control[2]\n",
    "np_control_matched_cf = tuple_matched_control[3]\n",
    "\n",
    "print(np_control_unmatched_cf.shape)\n",
    "print(np_control_matched_cf.shape)\n",
    "\n",
    "np_control_x = np.concatenate((np_control_unmatched_X, np_control_matched_X), axis=0)\n",
    "np_control_ps = np.concatenate((np_ps_score_list_control_unmatched, np_ps_score_list_control_matched), axis=0)\n",
    "np_control_f = np.concatenate((np_control_unmatched_f, np_control_matched_f), axis=0)\n",
    "np_control_cf = np.concatenate((np_control_unmatched_cf, np_control_matched_cf), axis=0)\n",
    "\n",
    "tensor_control = Utils.convert_to_tensor_DCN(np_control_x, np_control_ps,\n",
    "                                            np_control_f, np_control_cf)\n",
    "\n",
    "\n",
    "np_control_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_control_train = (np_control_x, np_control_ps, np_control_f, np_control_cf)\n",
    "tuple_treated_train = (np_treated_x, np_treated_ps, np_treated_f, np_treated_cf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DCN training using all dataset no PD ###\n",
      "Saved model path: ./DCNModel/NN_DCN_NO_DROPOUT_PM_MATCH_FALSE_model_iter_id_10_epoch_100_lr_0.001.pth\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 9, Treated + Control loss: 707.8079475988322\n",
      "epoch: 19, Treated + Control loss: 467.26298138267475\n",
      "epoch: 29, Treated + Control loss: 320.09224322317664\n",
      "epoch: 39, Treated + Control loss: 237.40315885586529\n",
      "epoch: 49, Treated + Control loss: 137.67990073766123\n",
      "epoch: 59, Treated + Control loss: 94.7723870317252\n",
      "epoch: 69, Treated + Control loss: 105.38134943241832\n",
      "epoch: 79, Treated + Control loss: 81.92853258633221\n",
      "epoch: 89, Treated + Control loss: 59.7053548837828\n",
      "epoch: 99, Treated + Control loss: 59.37603156465383\n"
     ]
    }
   ],
   "source": [
    "print(\"### DCN training using all dataset no PD ###\")\n",
    "\n",
    "train_mode = Constants.DCN_TRAIN_NO_DROPOUT\n",
    "\n",
    "model_path = \"./DCNModel/NN_DCN_NO_DROPOUT_PM_MATCH_FALSE_model_iter_id_{0}_epoch_{1}_lr_{2}.pth\"\n",
    "\n",
    "execute_DCN_train(tensor_treated, \n",
    "                  tensor_control, \n",
    "                  model_path, \n",
    "                  iter_id,\n",
    "                  input_nodes, \n",
    "                  device, \n",
    "                  train_mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Testing phase ------------\n",
      " Treated Statistics ==>\n",
      "(27, 25)\n",
      " Control Statistics ==>\n",
      "(123, 25)\n",
      "./DCNModel/NN_DCN_NO_DROPOUT_PM_MATCH_FALSE_model_iter_id_10_epoch_100_lr_0.001.pth\n",
      "PSM\n",
      "2.3053911928931794\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"----------- Testing phase ------------\")\n",
    "dL = DataLoader()\n",
    "ps_test_set = dL.convert_to_tensor(np_covariates_X_test,\n",
    "                                           np_covariates_Y_test)\n",
    "\n",
    "is_synthetic = False\n",
    "input_nodes = 25\n",
    "\n",
    "        # get propensity scores using NN\n",
    "ps_net_NN = Propensity_socre_network()\n",
    "ps_eval_parameters_NN = {\n",
    "            \"eval_set\": ps_test_set,\n",
    "            \"model_path\": Constants.PROP_SCORE_NN_MODEL_PATH\n",
    "                .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR),\n",
    "            \"input_nodes\": input_nodes\n",
    "}\n",
    "ps_score_list_NN = ps_net_NN.eval(ps_eval_parameters_NN, device, phase=\"eval\")\n",
    "\n",
    "data_loader_dict = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                        np_covariates_Y_test,\n",
    "                                                        ps_score_list_NN,\n",
    "                                                        is_synthetic)\n",
    "\n",
    "model_path = model_path.format(iter_id,\n",
    "                    epochs,\n",
    "                    lr)\n",
    "\n",
    "print(model_path)\n",
    "treated_group = data_loader_dict[\"treated_data\"]\n",
    "np_treated_df_X = treated_group[0]\n",
    "np_treated_ps_score = treated_group[1]\n",
    "np_treated_df_Y_f = treated_group[2]\n",
    "np_treated_df_Y_cf = treated_group[3]\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_df_X, np_treated_ps_score,\n",
    "                                                     np_treated_df_Y_f, np_treated_df_Y_cf)\n",
    "\n",
    "control_group = data_loader_dict[\"control_data\"]\n",
    "np_control_df_X = control_group[0]\n",
    "np_control_ps_score = control_group[1]\n",
    "np_control_df_Y_f = control_group[2]\n",
    "np_control_df_Y_cf = control_group[3]\n",
    "tensor_control = Utils.convert_to_tensor_DCN(np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f, np_control_df_Y_cf)\n",
    "\n",
    "DCN_test_parameters = {\n",
    "            \"treated_set\": tensor_treated,\n",
    "            \"control_set\": tensor_control,\n",
    "            \"model_save_path\": model_path\n",
    "}\n",
    "\n",
    "dcn = DCN_network_1()\n",
    "response_dict = dcn.eval(DCN_test_parameters, device, input_nodes,\n",
    "                                 Constants.DCN_EVALUATION)\n",
    "err_treated = [ele ** 2 for ele in response_dict[\"treated_err\"]]\n",
    "err_control = [ele ** 2 for ele in response_dict[\"control_err\"]]\n",
    "\n",
    "total_sum = sum(err_treated) + sum(err_control)\n",
    "total_item = len(err_treated) + len(err_control)\n",
    "MSE = total_sum / total_item\n",
    "\n",
    "print(\"PSM\")\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DCN training using all dataset DCN PD ###\n",
      "Saved model path: ./DCNModel/DCN_PD_model_iter_id_10_epoch_100_lr_0.001.pth\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 9, Treated + Control loss: 1001.0237502176751\n",
      "epoch: 19, Treated + Control loss: 821.8525736710048\n",
      "epoch: 29, Treated + Control loss: 824.4688989928818\n",
      "epoch: 39, Treated + Control loss: 718.0689708249702\n",
      "epoch: 49, Treated + Control loss: 665.4653844915052\n",
      "epoch: 59, Treated + Control loss: 600.6444613171188\n",
      "epoch: 69, Treated + Control loss: 570.2974571891214\n",
      "epoch: 79, Treated + Control loss: 723.323639266355\n",
      "epoch: 89, Treated + Control loss: 516.2083776923175\n",
      "epoch: 99, Treated + Control loss: 454.3476396009885\n"
     ]
    }
   ],
   "source": [
    "print(\"### DCN training using all dataset DCN PD ###\")\n",
    "\n",
    "train_mode = Constants.DCN_TRAIN_PD\n",
    "\n",
    "model_path = \"./DCNModel/DCN_PD_model_iter_id_{0}_epoch_{1}_lr_{2}.pth\"\n",
    "t_treated = Utils.create_tensors_to_train_DCN(data_loader_dict_train[\"treated_data\"], dL)\n",
    "t_control =  Utils.create_tensors_to_train_DCN(data_loader_dict_train[\"control_data\"], dL)\n",
    "\n",
    "execute_DCN_train(t_treated, \n",
    "                  t_control, \n",
    "                  model_path, \n",
    "                  iter_id,\n",
    "                  input_nodes, \n",
    "                  device, \n",
    "                  train_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Testing phase ------------\n",
      " Treated Statistics ==>\n",
      "(27, 25)\n",
      " Control Statistics ==>\n",
      "(123, 25)\n",
      "./DCNModel/DCN_PD_model_iter_id_10_epoch_100_lr_0.001.pth\n",
      "DCN_PD\n",
      "2.6120958867727557\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"----------- Testing phase ------------\")\n",
    "dL = DataLoader()\n",
    "ps_test_set = dL.convert_to_tensor(np_covariates_X_test,\n",
    "                                           np_covariates_Y_test)\n",
    "\n",
    "is_synthetic = False\n",
    "input_nodes = 25\n",
    "\n",
    "        # get propensity scores using NN\n",
    "ps_net_NN = Propensity_socre_network()\n",
    "ps_eval_parameters_NN = {\n",
    "            \"eval_set\": ps_test_set,\n",
    "            \"model_path\": Constants.PROP_SCORE_NN_MODEL_PATH\n",
    "                .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR),\n",
    "            \"input_nodes\": input_nodes\n",
    "}\n",
    "ps_score_list_NN = ps_net_NN.eval(ps_eval_parameters_NN, device, phase=\"eval\")\n",
    "\n",
    "data_loader_dict = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                        np_covariates_Y_test,\n",
    "                                                        ps_score_list_NN,\n",
    "                                                        is_synthetic)\n",
    "\n",
    "model_path = model_path.format(iter_id,\n",
    "                    epochs,\n",
    "                    lr)\n",
    "\n",
    "print(model_path)\n",
    "treated_group = data_loader_dict[\"treated_data\"]\n",
    "np_treated_df_X = treated_group[0]\n",
    "np_treated_ps_score = treated_group[1]\n",
    "np_treated_df_Y_f = treated_group[2]\n",
    "np_treated_df_Y_cf = treated_group[3]\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_df_X, np_treated_ps_score,\n",
    "                                                     np_treated_df_Y_f, np_treated_df_Y_cf)\n",
    "\n",
    "control_group = data_loader_dict[\"control_data\"]\n",
    "np_control_df_X = control_group[0]\n",
    "np_control_ps_score = control_group[1]\n",
    "np_control_df_Y_f = control_group[2]\n",
    "np_control_df_Y_cf = control_group[3]\n",
    "tensor_control = Utils.convert_to_tensor_DCN(np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f, np_control_df_Y_cf)\n",
    "\n",
    "DCN_test_parameters = {\n",
    "            \"treated_set\": tensor_treated,\n",
    "            \"control_set\": tensor_control,\n",
    "            \"model_save_path\": model_path\n",
    "}\n",
    "\n",
    "dcn = DCN_network_1()\n",
    "response_dict = dcn.eval(DCN_test_parameters, device, input_nodes,\n",
    "                                 Constants.DCN_EVALUATION)\n",
    "err_treated = [ele ** 2 for ele in response_dict[\"treated_err\"]]\n",
    "err_control = [ele ** 2 for ele in response_dict[\"control_err\"]]\n",
    "\n",
    "total_sum = sum(err_treated) + sum(err_control)\n",
    "total_item = len(err_treated) + len(err_control)\n",
    "MSE = total_sum / total_item\n",
    "print(\"DCN_PD\")\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TARNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TARNetPhi(nn.Module):\n",
    "    def __init__(self, input_nodes, shared_nodes=200):\n",
    "        super(TARNetPhi, self).__init__()\n",
    "\n",
    "        # shared layer\n",
    "        self.shared1 = nn.Linear(in_features=input_nodes, out_features=shared_nodes)\n",
    "        nn.init.xavier_uniform_(self.shared1.weight)\n",
    "        nn.init.zeros_(self.shared1.bias)\n",
    "\n",
    "        self.shared2 = nn.Linear(in_features=shared_nodes, out_features=shared_nodes)\n",
    "        nn.init.xavier_uniform_(self.shared2.weight)\n",
    "        nn.init.zeros_(self.shared2.bias)\n",
    "\n",
    "        self.shared3 = nn.Linear(in_features=shared_nodes, out_features=shared_nodes)\n",
    "        nn.init.xavier_uniform_(self.shared3.weight)\n",
    "        nn.init.zeros_(self.shared3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.float().cuda()\n",
    "        else:\n",
    "            x = x.float()\n",
    "        # shared layers\n",
    "        x = F.relu(self.shared1(x))\n",
    "        x = F.relu(self.shared2(x))\n",
    "        x = F.relu(self.shared3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TARNetH_Y1(nn.Module):\n",
    "    def __init__(self, input_nodes=200, outcome_nodes=100):\n",
    "        super(TARNetH_Y1, self).__init__()\n",
    "\n",
    "        # potential outcome1 Y(1)\n",
    "        self.hidden1_Y1 = nn.Linear(in_features=input_nodes, out_features=outcome_nodes)\n",
    "        nn.init.xavier_uniform_(self.hidden1_Y1.weight)\n",
    "        nn.init.zeros_(self.hidden1_Y1.bias)\n",
    "\n",
    "        self.hidden2_Y1 = nn.Linear(in_features=outcome_nodes, out_features=outcome_nodes)\n",
    "        nn.init.xavier_uniform_(self.hidden2_Y1.weight)\n",
    "        nn.init.zeros_(self.hidden2_Y1.bias)\n",
    "\n",
    "        self.out_Y1 = nn.Linear(in_features=outcome_nodes, out_features=1)\n",
    "        nn.init.xavier_uniform_(self.out_Y1.weight)\n",
    "        nn.init.zeros_(self.out_Y1.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.float().cuda()\n",
    "        else:\n",
    "            x = x.float()\n",
    "\n",
    "        # potential outcome1 Y(1)\n",
    "        y1 = F.relu(self.hidden1_Y1(x))\n",
    "        y1 = F.relu(self.hidden2_Y1(y1))\n",
    "        y1 = self.out_Y1(y1)\n",
    "\n",
    "        return y1\n",
    "\n",
    "\n",
    "class TARNetH_Y0(nn.Module):\n",
    "    def __init__(self, input_nodes=200, outcome_nodes=100):\n",
    "        super(TARNetH_Y0, self).__init__()\n",
    "\n",
    "        # potential outcome1 Y(0)\n",
    "        self.hidden1_Y0 = nn.Linear(in_features=input_nodes, out_features=outcome_nodes)\n",
    "        nn.init.xavier_uniform_(self.hidden1_Y0.weight)\n",
    "        nn.init.zeros_(self.hidden1_Y0.bias)\n",
    "\n",
    "        self.hidden2_Y0 = nn.Linear(in_features=outcome_nodes, out_features=outcome_nodes)\n",
    "        nn.init.xavier_uniform_(self.hidden2_Y0.weight)\n",
    "        nn.init.zeros_(self.hidden2_Y0.bias)\n",
    "\n",
    "        self.out_Y0 = nn.Linear(in_features=outcome_nodes, out_features=1)\n",
    "        nn.init.xavier_uniform_(self.out_Y0.weight)\n",
    "        nn.init.zeros_(self.out_Y0.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.float().cuda()\n",
    "        else:\n",
    "            x = x.float()\n",
    "\n",
    "        # potential outcome1 Y(0)\n",
    "        y0 = F.relu(self.hidden1_Y0(x))\n",
    "        y0 = F.relu(self.hidden2_Y0(y0))\n",
    "        y0 = self.out_Y0(y0)\n",
    "\n",
    "        return y0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PS_Matching:\n",
    "    def match_using_prop_score(self, tuple_treated, tuple_control):\n",
    "        matched_controls = []\n",
    "\n",
    "        # do ps match\n",
    "        np_treated_df_X, np_treated_ps_score, np_treated_df_Y_f, np_treated_df_Y_cf = tuple_treated\n",
    "        np_control_df_X, np_control_ps_score, np_control_df_Y_f, np_control_df_Y_cf = tuple_control\n",
    "\n",
    "        # get unmatched controls\n",
    "        matched_control_indices, unmatched_control_indices = self.get_matched_and_unmatched_control_indices(\n",
    "            Utils.convert_to_col_vector(np_treated_ps_score),\n",
    "            Utils.convert_to_col_vector(np_control_ps_score))\n",
    "\n",
    "        tuple_matched_control, tuple_unmatched_control = self.filter_matched_and_unmatched_control_samples(\n",
    "            np_control_df_X, np_control_ps_score,\n",
    "            np_control_df_Y_f,\n",
    "            np_control_df_Y_cf, matched_control_indices,\n",
    "            unmatched_control_indices)\n",
    "\n",
    "        return tuple_matched_control\n",
    "\n",
    "    def filter_matched_and_unmatched_control_samples(self, np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f,\n",
    "                                                     np_control_df_Y_cf, matched_control_indices,\n",
    "                                                     unmatched_control_indices):\n",
    "        tuple_matched_control = self.filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                                                           np_control_df_Y_f,\n",
    "                                                           np_control_df_Y_cf,\n",
    "                                                           matched_control_indices)\n",
    "\n",
    "        tuple_unmatched_control = self.filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                                                             np_control_df_Y_f,\n",
    "                                                             np_control_df_Y_cf,\n",
    "                                                             unmatched_control_indices)\n",
    "\n",
    "        return tuple_matched_control, tuple_unmatched_control\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_control_groups(np_control_df_X, np_control_ps_score,\n",
    "                              np_control_df_Y_f,\n",
    "                              np_control_df_Y_cf, indices):\n",
    "        np_filter_control_df_X = np.take(np_control_df_X, indices, axis=0)\n",
    "        np_filter_control_ps_score = np.take(np_control_ps_score, indices, axis=0)\n",
    "        np_filter_control_df_Y_f = np.take(np_control_df_Y_f, indices, axis=0)\n",
    "        np_filter_control_df_Y_cf = np.take(np_control_df_Y_cf, indices, axis=0)\n",
    "        tuple_matched_control = (np_filter_control_df_X, np_filter_control_ps_score,\n",
    "                                 np_filter_control_df_Y_f, np_filter_control_df_Y_cf)\n",
    "\n",
    "        return tuple_matched_control\n",
    "\n",
    "    @staticmethod\n",
    "    def get_matched_and_unmatched_control_indices(ps_treated, ps_control):\n",
    "        nn = NearestNeighbors(n_neighbors=1)\n",
    "        nn.fit(ps_control)\n",
    "        distance, matched_control = nn.kneighbors(ps_treated)\n",
    "        matched_control_indices = np.array(matched_control).ravel()\n",
    "\n",
    "        # remove duplicates\n",
    "        # matched_control_indices = list(dict.fromkeys(matched_control_indices))\n",
    "        set_matched_control_indices = set(matched_control_indices)\n",
    "        total_indices = list(range(len(ps_control)))\n",
    "        unmatched_control_indices = list(filter(lambda x: x not in set_matched_control_indices,\n",
    "                                                total_indices))\n",
    "\n",
    "        return matched_control_indices, unmatched_control_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def get_unmatched_prop_list(tensor_unmatched_control):\n",
    "        control_data_loader_train = torch.utils.data.DataLoader(tensor_unmatched_control,\n",
    "                                                                batch_size=1,\n",
    "                                                                shuffle=False,\n",
    "                                                                num_workers=1)\n",
    "        ps_unmatched_control_list = []\n",
    "        for batch in control_data_loader_train:\n",
    "            covariates_X, ps_score, y_f, y_cf = batch\n",
    "            ps_unmatched_control_list.append(ps_score.item())\n",
    "\n",
    "        return ps_unmatched_control_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceNet:\n",
    "    def __init__(self, input_nodes, shared_nodes, outcome_nodes, device):\n",
    "        self.tarnet_phi = TARNetPhi(input_nodes, shared_nodes=shared_nodes).to(device)\n",
    "\n",
    "        self.tarnet_h_y1 = TARNetH_Y1(input_nodes=shared_nodes,\n",
    "                                      outcome_nodes=outcome_nodes).to(device)\n",
    "\n",
    "        self.tarnet_h_y0 = TARNetH_Y0(input_nodes=shared_nodes,\n",
    "                                      outcome_nodes=outcome_nodes).to(device)\n",
    "\n",
    "    def get_tarnet_phi(self):\n",
    "        return self.tarnet_phi\n",
    "\n",
    "    def get_tarnet_h_y1(self):\n",
    "        return self.tarnet_h_y1\n",
    "\n",
    "    def get_tarnet_h_y0_model(self):\n",
    "        return self.tarnet_h_y0\n",
    "\n",
    "    def train(self, train_parameters, device):\n",
    "        epochs = train_parameters[\"epochs\"]\n",
    "        batch_size = train_parameters[\"batch_size\"]\n",
    "        lr = train_parameters[\"lr\"]\n",
    "        weight_decay = train_parameters[\"lambda\"]\n",
    "        shuffle = train_parameters[\"shuffle\"]\n",
    "        treated_tensor_dataset = train_parameters[\"treated_tensor_dataset\"]\n",
    "        tuple_control = train_parameters[\"tuple_control_train\"]\n",
    "\n",
    "        treated_data_loader_train = torch.utils.data.DataLoader(treated_tensor_dataset,\n",
    "                                                                batch_size=batch_size,\n",
    "                                                                shuffle=shuffle,\n",
    "                                                                num_workers=1)\n",
    "\n",
    "        optimizer_W = optim.Adam(self.tarnet_phi.parameters(), lr=lr)\n",
    "        optimizer_V1 = optim.Adam(self.tarnet_h_y1.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        optimizer_V2 = optim.Adam(self.tarnet_h_y0.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        lossF = nn.MSELoss()\n",
    "        print(\".. Training started ..\")\n",
    "        print(device)\n",
    "        for epoch in range(epochs):\n",
    "            epoch += 1\n",
    "            total_loss_T = 0\n",
    "            total_loss_C = 0\n",
    "            for batch in treated_data_loader_train:\n",
    "                covariates_X_treated, ps_score_treated, y_f_treated, y_cf_treated = batch\n",
    "                covariates_X_treated = covariates_X_treated.to(device)\n",
    "                ps_score_treated = ps_score_treated.squeeze().to(device)\n",
    "\n",
    "                _tuple_treated = self.get_np_tuple_from_tensor(covariates_X_treated, ps_score_treated,\n",
    "                                                               y_f_treated, y_cf_treated)\n",
    "                psm = PS_Matching()\n",
    "                tuple_matched_control = psm.match_using_prop_score(_tuple_treated, tuple_control)\n",
    "\n",
    "                covariates_X_control, ps_score_control, y_f_control, y_cf_control = \\\n",
    "                    self.get_tensor_from_np_tuple(tuple_matched_control)\n",
    "\n",
    "                y1_hat = self.tarnet_h_y1(self.tarnet_phi(covariates_X_treated))\n",
    "                y0_hat = self.tarnet_h_y0(self.tarnet_phi(covariates_X_treated))\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    loss_T = lossF(y1_hat.float().cuda(),\n",
    "                                   y_f_treated.float().cuda()).to(device)\n",
    "                    loss_C = lossF(y0_hat.float().cuda(),\n",
    "                                   y_f_control.float().cuda()).to(device)\n",
    "                else:\n",
    "                    loss_T = lossF(y1_hat.float(),\n",
    "                                   y_f_treated.float()).to(device)\n",
    "                    loss_C = lossF(y0_hat.float(),\n",
    "                                   y_f_control.float()).to(device)\n",
    "\n",
    "                optimizer_W.zero_grad()\n",
    "                optimizer_V1.zero_grad()\n",
    "                optimizer_V2.zero_grad()\n",
    "                loss_T.backward()\n",
    "                loss_C.backward()\n",
    "                optimizer_W.step()\n",
    "                optimizer_V1.step()\n",
    "                optimizer_V2.step()\n",
    "                total_loss_T += loss_T.item()\n",
    "                total_loss_C += loss_C.item()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"epoch: {0}, Treated + Control loss: {1}\".format(epoch, total_loss_T + total_loss_C))\n",
    "\n",
    "    def eval(self, eval_parameters, device):\n",
    "        treated_set = eval_parameters[\"treated_set\"]\n",
    "        control_set = eval_parameters[\"control_set\"]\n",
    "        treated_data_loader = torch.utils.data.DataLoader(treated_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "        control_data_loader = torch.utils.data.DataLoader(control_set,\n",
    "                                                          shuffle=False, num_workers=1)\n",
    "\n",
    "        err_treated_list = []\n",
    "        err_control_list = []\n",
    "        true_ITE_list = []\n",
    "        predicted_ITE_list = []\n",
    "\n",
    "        ITE_dict_list = []\n",
    "\n",
    "        for batch in treated_data_loader:\n",
    "            covariates_X, ps_score, y_f, y_cf = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "            y1_hat = self.tarnet_h_y1(self.tarnet_phi(covariates_X))\n",
    "            y0_hat = self.tarnet_h_y0(self.tarnet_phi(covariates_X))\n",
    "\n",
    "            predicted_ITE = y1_hat - y0_hat\n",
    "            true_ITE = y_f - y_cf\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                diff = true_ITE.float().cuda() - predicted_ITE.float().cuda()\n",
    "            else:\n",
    "                diff = true_ITE.float() - predicted_ITE.float()\n",
    "\n",
    "            # ITE_dict_list.append(self.create_ITE_Dict(covariates_X,\n",
    "            #                                           ps_score.item(), y_f.item(),\n",
    "            #                                           y_cf.item(),\n",
    "            #                                           true_ITE.item(),\n",
    "            #                                           predicted_ITE.item(),\n",
    "            #                                           diff.item()))\n",
    "            err_treated_list.append(diff.item())\n",
    "            true_ITE_list.append(true_ITE.item())\n",
    "            predicted_ITE_list.append(predicted_ITE.item())\n",
    "\n",
    "        for batch in control_data_loader:\n",
    "            covariates_X, ps_score, y_f, y_cf = batch\n",
    "            covariates_X = covariates_X.to(device)\n",
    "\n",
    "            y1_hat = self.tarnet_h_y1(self.tarnet_phi(covariates_X))\n",
    "            y0_hat = self.tarnet_h_y0(self.tarnet_phi(covariates_X))\n",
    "\n",
    "            predicted_ITE = y1_hat - y0_hat\n",
    "            true_ITE = y_cf - y_f\n",
    "            if torch.cuda.is_available():\n",
    "                diff = true_ITE.float().cuda() - predicted_ITE.float().cuda()\n",
    "            else:\n",
    "                diff = true_ITE.float() - predicted_ITE.float()\n",
    "\n",
    "            # ITE_dict_list.append(self.create_ITE_Dict(covariates_X,\n",
    "            #                                           ps_score.item(), y_f.item(),\n",
    "            #                                           y_cf.item(),\n",
    "            #                                           true_ITE.item(),\n",
    "            #                                           predicted_ITE.item(),\n",
    "            #                                           diff.item()))\n",
    "            err_control_list.append(diff.item())\n",
    "            true_ITE_list.append(true_ITE.item())\n",
    "            predicted_ITE_list.append(predicted_ITE.item())\n",
    "\n",
    "        # print(err_treated_list)\n",
    "        # print(err_control_list)\n",
    "        return {\n",
    "            \"treated_err\": err_treated_list,\n",
    "            \"control_err\": err_control_list,\n",
    "            \"true_ITE\": true_ITE_list,\n",
    "            \"predicted_ITE\": predicted_ITE_list,\n",
    "            \"ITE_dict_list\": ITE_dict_list\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_np_tuple_from_tensor(covariates_X, ps_score, y_f, y_cf):\n",
    "        np_covariates_X = covariates_X.numpy()\n",
    "        ps_score = ps_score.numpy()\n",
    "        y_f = y_f.numpy()\n",
    "        y_cf = y_cf.numpy()\n",
    "        _tuple = (np_covariates_X, ps_score, y_f, y_cf)\n",
    "\n",
    "        return _tuple\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tensor_from_np_tuple(_tuple):\n",
    "        np_df_X, np_ps_score, np_df_Y_f, np_df_Y_cf = _tuple\n",
    "        return torch.from_numpy(np_df_X), torch.from_numpy(np_ps_score), \\\n",
    "               torch.from_numpy(np_df_Y_f), torch.from_numpy(np_df_Y_cf),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Sizes\n",
      "(559, 25)\n",
      "(559, 25)\n",
      ".. Training started ..\n",
      "cpu\n",
      "epoch: 100, Treated + Control loss: 11.158835638314486\n"
     ]
    }
   ],
   "source": [
    "tuple_control_train = (np_control_x, np_control_ps, np_control_f, np_control_cf)\n",
    "tuple_treated_train = (np_treated_x, np_treated_ps, np_treated_f, np_treated_cf)\n",
    "\n",
    "print(\"Actual Sizes\")\n",
    "print(np_treated_x.shape)\n",
    "print(np_control_x.shape)\n",
    "\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_x, np_treated_ps,\n",
    "                                            np_treated_f, np_treated_cf)\n",
    "\n",
    "train_parameters = {\n",
    "                \"epochs\": 100,\n",
    "                \"lr\": 1e-3,\n",
    "                \"lambda\":1e-4,\n",
    "                \"batch_size\": 32,\n",
    "                \"shuffle\": True,\n",
    "                \"treated_tensor_dataset\": tensor_treated,\n",
    "                \"tuple_control_train\": tuple_control_train\n",
    "    }\n",
    "\n",
    "inference = InferenceNet(input_nodes=25, shared_nodes=200, \n",
    "                         outcome_nodes=100,\n",
    "                         device=device)\n",
    "\n",
    "inference.train(train_parameters, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Testing phase ------------\n",
      " Treated Statistics ==>\n",
      "(27, 25)\n",
      " Control Statistics ==>\n",
      "(123, 25)\n",
      "./DCNModel/NN_DCN_NO_DROPOUT_PM_MATCH_FALSE_model_iter_id_10_epoch_100_lr_0.001.pth\n",
      "(27, 25)\n",
      "(123, 25)\n",
      "Testing using Inference\n",
      "6.906225153709756\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"----------- Testing phase ------------\")\n",
    "dL = DataLoader()\n",
    "ps_test_set = dL.convert_to_tensor(np_covariates_X_test,\n",
    "                                           np_covariates_Y_test)\n",
    "\n",
    "is_synthetic = False\n",
    "input_nodes = 25\n",
    "\n",
    "        # get propensity scores using NN\n",
    "ps_net_NN = Propensity_socre_network()\n",
    "ps_eval_parameters_NN = {\n",
    "            \"eval_set\": ps_test_set,\n",
    "            \"model_path\": Constants.PROP_SCORE_NN_MODEL_PATH\n",
    "                .format(iter_id, Constants.PROP_SCORE_NN_EPOCHS, Constants.PROP_SCORE_NN_LR),\n",
    "            \"input_nodes\": input_nodes\n",
    "}\n",
    "ps_score_list_NN = ps_net_NN.eval(ps_eval_parameters_NN, device, phase=\"eval\")\n",
    "\n",
    "data_loader_dict = dL.prepare_tensor_for_DCN(np_covariates_X_test,\n",
    "                                                        np_covariates_Y_test,\n",
    "                                                        ps_score_list_NN,\n",
    "                                                        is_synthetic)\n",
    "\n",
    "model_path = model_path.format(iter_id,\n",
    "                    epochs,\n",
    "                    lr)\n",
    "\n",
    "print(model_path)\n",
    "treated_group = data_loader_dict[\"treated_data\"]\n",
    "np_treated_df_X = treated_group[0]\n",
    "np_treated_ps_score = treated_group[1]\n",
    "np_treated_df_Y_f = treated_group[2]\n",
    "np_treated_df_Y_cf = treated_group[3]\n",
    "tensor_treated = Utils.convert_to_tensor_DCN(np_treated_df_X, np_treated_ps_score,\n",
    "                                                     np_treated_df_Y_f, np_treated_df_Y_cf)\n",
    "\n",
    "control_group = data_loader_dict[\"control_data\"]\n",
    "np_control_df_X = control_group[0]\n",
    "np_control_ps_score = control_group[1]\n",
    "np_control_df_Y_f = control_group[2]\n",
    "np_control_df_Y_cf = control_group[3]\n",
    "tensor_control = Utils.convert_to_tensor_DCN(np_control_df_X, np_control_ps_score,\n",
    "                                                     np_control_df_Y_f, np_control_df_Y_cf)\n",
    "\n",
    "print(np_treated_df_X.shape)\n",
    "print(np_control_df_X.shape)\n",
    "\n",
    "test_parameters = {\n",
    "            \"treated_set\": tensor_treated,\n",
    "            \"control_set\": tensor_control\n",
    "}\n",
    "\n",
    "response_dict = inference.eval(test_parameters, device)\n",
    "err_treated = [ele ** 2 for ele in response_dict[\"treated_err\"]]\n",
    "err_control = [ele ** 2 for ele in response_dict[\"control_err\"]]\n",
    "\n",
    "total_sum = sum(err_treated) + sum(err_control)\n",
    "total_item = len(err_treated) + len(err_control)\n",
    "MSE = total_sum / total_item\n",
    "\n",
    "print(\"Testing using Inference\")\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
